{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Gaussian:\n",
    "    def __init__(self, eta, Lam):\n",
    "        self.eta = eta\n",
    "        self.Lam = Lam\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.eta, self.Lam), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "    def mu(self):\n",
    "        return jnp.where(\n",
    "            jnp.allclose(self.Lam, 0), self.eta, jnp.linalg.solve(self.Lam, self.eta)\n",
    "        )\n",
    "    \n",
    "    def sigma(self):\n",
    "        return jnp.linalg.inv(self.Lam)\n",
    "\n",
    "    def zero_like(self):\n",
    "        return Gaussian(jnp.zeros_like(self.eta), jnp.zeros_like(self.Lam))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Gaussian(eta={self.eta}, lam={self.Lam})\"\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Gaussian(self.eta + other.eta, self.Lam + other.Lam)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Gaussian(self.eta - other.eta, self.Lam - other.Lam)\n",
    "\n",
    "    def copy(self):\n",
    "        return Gaussian(self.eta.copy(), self.Lam.copy())\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Variable:\n",
    "    var_id: int\n",
    "    belief: Gaussian\n",
    "    msgs: Gaussian\n",
    "    adj_factor_idx: jnp.array\n",
    "\n",
    "    def __init__(self, var_id, belief, msgs, adj_factor_idx):\n",
    "        self.var_id = var_id\n",
    "        self.belief = belief\n",
    "        self.msgs = msgs\n",
    "        self.adj_factor_idx = adj_factor_idx\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.var_id, self.belief, self.msgs, self.adj_factor_idx), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Factor:\n",
    "    factor_id: jnp.array\n",
    "    z: jnp.ndarray\n",
    "    z_Lam: jnp.ndarray\n",
    "    threshold: jnp.ndarray\n",
    "    potential: Gaussian\n",
    "    adj_var_id: jnp.array\n",
    "    adj_var_idx: jnp.array\n",
    "\n",
    "    def __init__(\n",
    "        self, factor_id, z, z_Lam, threshold, potential, adj_var_id, adj_var_idx\n",
    "    ):\n",
    "        self.factor_id = factor_id\n",
    "        self.z = z\n",
    "        self.z_Lam = z_Lam\n",
    "        self.threshold = threshold\n",
    "        self.potential = potential\n",
    "        self.adj_var_id = adj_var_id\n",
    "        self.adj_var_idx = adj_var_idx\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (\n",
    "            self.factor_id,\n",
    "            self.z,\n",
    "            self.z_Lam,\n",
    "            self.threshold,\n",
    "            self.potential,\n",
    "            self.adj_var_id,\n",
    "            self.adj_var_idx,\n",
    "        ), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"i\", \"j\"])\n",
    "def marginalize(gaussians: Gaussian, i, j): # Equ. (46), (47); Compute msg to i:j Variables from connected factors\n",
    "    eta = gaussians.eta\n",
    "    Lam = gaussians.Lam\n",
    "    k = eta.size\n",
    "    idx = jnp.arange(0, k)\n",
    "    aa = idx[i:j] # index from i to j-1\n",
    "    bb = jnp.concat([idx[:i], idx[j:]]) # rest\n",
    "    aa_eta = eta[aa]\n",
    "    bb_eta = eta[bb]\n",
    "    aa_Lam = Lam[aa[:, None], aa]\n",
    "    ab_Lam = Lam[aa[:, None], bb]\n",
    "    bb_Lam = Lam[bb][:, bb]\n",
    "    if bb_Lam.size == 0:\n",
    "        return Gaussian(aa_eta, aa_Lam)\n",
    "    # print(\"How large? \", bb_Lam.shape)\n",
    "\n",
    "    bb_Cov = jnp.linalg.inv(bb_Lam)\n",
    "    eta = aa_eta - ab_Lam @ bb_Cov @ bb_eta\n",
    "    Lam = aa_Lam - ab_Lam @ bb_Cov @ ab_Lam.T\n",
    "    return Gaussian(eta, Lam)\n",
    "\n",
    "\n",
    "def tree_stack(tree, axis=0, use_np=True):\n",
    "    if use_np:\n",
    "        return jax.tree.map(lambda *v: jnp.array(np.stack(v, axis=axis)), *tree)\n",
    "    return jax.tree.map(lambda *v: jnp.stack(v, axis=axis), *tree)\n",
    "\n",
    "\n",
    "def h_fn(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def h2_fn(x):\n",
    "    \n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return x2 - x1\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def update_belief(var: Variable, ftov_msgs): # Calculate Eq. (7)\n",
    "    belief = var.belief.zero_like()\n",
    "    for i in range(ftov_msgs.eta.shape[0]):\n",
    "        belief = belief * Gaussian(ftov_msgs.eta[i], ftov_msgs.Lam[i])\n",
    "    return belief\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def compute_vtof_msgs(var: Variable, ftov_msgs): # Eq.(19); do for each variable (x_m)\n",
    "    vtof_msgs = []\n",
    "    for i, idx in enumerate(var.adj_factor_idx): # for each f_si connected to x_m...\n",
    "        msg = var.belief / Gaussian(ftov_msgs.eta[i], ftov_msgs.Lam[i]) # Eq.(19) LHS subscript of SUM\n",
    "        eta = jnp.where(idx < 0, msg.zero_like().eta, msg.eta) # Those not connected should not affect the calculation (idx < 0)\n",
    "        Lam = jnp.where(idx < 0, msg.zero_like().Lam, msg.Lam) # The reason to not using \"if\" (while it's per-element) is to optimize better\n",
    "        vtof_msgs.append(Gaussian(eta, Lam)) # append (x_m -> f_si)\n",
    "    return tree_stack(vtof_msgs, use_np=False) # [(x_m -> f_s1), (x_m -> f_s2), ... ] # The length is Ni_v\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"h_fn\"])\n",
    "def factor_energy(factor, xs, h_fn):\n",
    "    h = h_fn(xs)\n",
    "    z = factor.z\n",
    "    z_Lam = factor.z_Lam\n",
    "    r = z - h\n",
    "    return 0.5 * r @ z_Lam @ r.T\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnames=[\"h_fn\", \"w\"])\n",
    "def factor_update(factor, xs, h_fn, w):\n",
    "    h = h_fn(xs)\n",
    "    J = jax.jacrev(h_fn)(xs).reshape(h.size, xs.size) # Jacobian auto-diff (J_s)\n",
    "    z = factor.z # I think this is a vector\n",
    "    z_Lam = factor.z_Lam\n",
    "    r = z - h.reshape(-1) # TODO: reshape can be problematic\n",
    "    s = w(r.T @ z_Lam @ r, factor.threshold) # Scale to consider Robust Loss\n",
    "    Lam = s * J.T @ z_Lam @ J # Eq. (36)\n",
    "    eta = s * J.T @ z_Lam @ (J @ xs.reshape(-1) + r) # TODO: reshape can be problematic; Eq. (36); xs should be a vector\n",
    "    return Gaussian(eta, Lam) # Factor; represented w.r.t. neighboring variables xs\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def compute_ftov_msg(factor, vtof_msgs): # Ch 3.5 Message Passing at a Factor Node\n",
    "    N_adj, dim = vtof_msgs.eta.shape # #Ni_v * 3 (#Vars is vmapped)\n",
    "    pot = factor.potential.copy() # log(f_s), but for only a specific variable a factor is connected to.\n",
    "    i = 0\n",
    "    for n in range(N_adj): # Add all! (Produce all)\n",
    "        j = i + dim\n",
    "        pot.eta = pot.eta.at[i:j].add(vtof_msgs.eta[n])\n",
    "        pot.Lam = pot.Lam.at[i:j, i:j].add(vtof_msgs.Lam[n])\n",
    "        i = j\n",
    "\n",
    "\n",
    "    ftov_msgs = []\n",
    "    i = 0\n",
    "    for n in range(N_adj):\n",
    "        j = i + dim\n",
    "        pot_m_1 = pot.copy()\n",
    "        pot_m_1.eta = pot_m_1.eta.at[i:j].add(-vtof_msgs.eta[n]) # Subtract direction of going out! (42)\n",
    "        pot_m_1.Lam = pot_m_1.Lam.at[i:j, i:j].add(-vtof_msgs.Lam[n]) # (43)\n",
    "        msg = marginalize(pot_m_1, i, j) # (46), (47)\n",
    "        ftov_msgs.append(msg)\n",
    "        i = j\n",
    "    return tree_stack(ftov_msgs, use_np=False)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_variable(vars): # Update belief with receiving msgs and calculate msg to factors; vars.msgs are up-to-date and vars.belief are not\n",
    "    vars.belief = jax.vmap(update_belief)(vars, vars.msgs) # Eq. (7); vars.msgs is receiving msgs (ftov)\n",
    "    vtof_msgs = jax.vmap(compute_vtof_msgs)(vars, vars.msgs) # Variable -> Factor Msg; Eq. (19)\n",
    "    linpoints = jax.vmap(lambda x: x.mu())(vars.belief) # Current avg of belief! Belief is posterior\n",
    "    return vars, vtof_msgs, linpoints # vtof msgs: # Var * # Var-direction (factor, Ni_v) msgs\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"f\", \"w\"])\n",
    "def update_factor(facs, vars, vtof_msgs, linpoints, f, w): # f is factor function, w is robustifier\n",
    "    vtof_msgs_reordered = jax.tree_util.tree_map( # Variable to factor messages to specific (variable, factor; or variable-direction) pair\n",
    "        lambda x: x[facs.adj_var_id, facs.adj_var_idx], vtof_msgs # id: Variable id (one end), idx: direction (another end)\n",
    "    )\n",
    "    linpoints_reordered = jax.tree_util.tree_map(\n",
    "        lambda x: x[facs.adj_var_id], linpoints # Reorder linpoints by adj_var_id: variables' mean for factors' one ends\n",
    "    )\n",
    "    facs.potential = jax.vmap(factor_update, in_axes=(0, 0, None, None))( # Calculate each factor potential (f_s(x, x_1, ..., x_M) of Eq. (15))\n",
    "        facs, linpoints_reordered, f, w # Each factor contribution of variable-direction pair (factor: variable-direction pair)\n",
    "    ) # 1 or 2-dimensional!! (gradient / prior factor or smoothness factor)\n",
    "    ftov_msgs = jax.vmap(compute_ftov_msg)(facs, vtof_msgs_reordered) # ftov calculation by Eq. (15), with potential f_s, and msg vtof\n",
    "    vars.msgs.eta = vars.msgs.eta.at[facs.adj_var_id, facs.adj_var_idx].set( # Setting vars' receiving messages\n",
    "        ftov_msgs.eta\n",
    "    )\n",
    "    vars.msgs.Lam = vars.msgs.Lam.at[facs.adj_var_id, facs.adj_var_idx].set(\n",
    "        ftov_msgs.Lam\n",
    "    )\n",
    "    vars.msgs.eta = vars.msgs.eta.at[:, -1].set(0) # Receiving messages via last port (invalid port) is zero.\n",
    "    vars.msgs.Lam = vars.msgs.Lam.at[:, -1].set(0)\n",
    "\n",
    "    return facs, vars\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def huber(e, t):\n",
    "    x = jnp.sqrt(e)\n",
    "    return jnp.where(x <= t, 1.0, t / x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def l2(e, _):\n",
    "    return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87bb7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax version: 0.6.2\n",
      "jaxlib version: 0.6.2\n",
      "backend: cpu\n",
      "devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jaxlib\n",
    "\n",
    "print(\"jax version:\", jax.__version__)\n",
    "print(\"jaxlib version:\", jaxlib.__version__)\n",
    "print(\"backend:\", jax.default_backend())\n",
    "print(\"devices:\", jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fff27d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# SLAM-like base graph\n",
    "# -----------------------\n",
    "def make_slam_like_graph(N=100, step_size=25, loop_prob=0.05, loop_radius=50, prior_prop=0.0, seed=None):\n",
    "    if seed is None :\n",
    "        rng = np.random.default_rng()  # ✅ Ensure we have an RNG\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    nodes, edges = [], []\n",
    "    positions = []\n",
    "    x, y = 0.0, 0.0\n",
    "    positions.append((x, y))\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: trajectory generation\n",
    "    for _ in range(1, int(N)):\n",
    "        dx, dy = rng.standard_normal(2)  # replace np.random.randn\n",
    "        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "        dx, dy = dx / norm * float(step_size), dy / norm * float(step_size)\n",
    "        x, y = x + dx, y + dy\n",
    "        positions.append((x, y))\n",
    "\n",
    "    # Sequential edges along the path\n",
    "    for i, (px, py) in enumerate(positions):\n",
    "        nodes.append({\n",
    "            \"data\": {\"id\": f\"{i}\", \"layer\": 0, \"dim\": 2, \"num_base\": 1},\n",
    "            \"position\": {\"x\": float(px), \"y\": float(py)}\n",
    "        })\n",
    "\n",
    "    for i in range(int(N) - 1):\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{i+1}\"}})\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: loop-closure edges\n",
    "    for i in range(int(N)):\n",
    "        for j in range(i + 5, int(N)):\n",
    "            if rng.random() < float(loop_prob):  # replace np.random.rand\n",
    "                xi, yi = positions[i]\n",
    "                xj, yj = positions[j]\n",
    "                if np.hypot(xi - xj, yi - yj) < float(loop_radius):\n",
    "                    edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{j}\"}})\n",
    "\n",
    "    # ✅ Sample priors using the same RNG\n",
    "    if prior_prop <= 0.0:\n",
    "        strong_ids = {0}\n",
    "    elif prior_prop >= 1.0:\n",
    "        strong_ids = set(range(N))\n",
    "    else:\n",
    "        k = max(1, int(np.floor(prior_prop * N)))\n",
    "        strong_ids = set(rng.choice(N, size=k, replace=False).tolist())\n",
    "\n",
    "    # Add edges for nodes with strong priors\n",
    "    for i in strong_ids:\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": \"prior\"}})\n",
    "\n",
    "    edges.append({\"data\": {\"source\": f\"{0}\", \"target\": \"anchor\"}}) \n",
    "    return nodes, edges\n",
    "\n",
    "def build_posegraph_jax_from_nodes_edges(\n",
    "    nodes,\n",
    "    edges,\n",
    "    prior_sigma: float = 10.0,\n",
    "    odom_sigma: float = 10.0,\n",
    "    tiny_prior: float = 1e-12,\n",
    "    anchor_sigma: float = 1e-4,\n",
    "    seed=None,\n",
    "    device=\"cpu\",\n",
    "    robust_threshold: float = 1.0,  # 你的 grid 里 threshold=ones(...,1)，这里保持可调\n",
    "):\n",
    "    \"\"\"\n",
    "    JAX version matching build_noisy_pose_graph():\n",
    "      - variables: 2D\n",
    "      - between measurement: (xj - xi) + N(0, odom_sigma^2 I)\n",
    "      - strong prior: z = GT + N(0, prior_sigma^2 I)\n",
    "      - anchor: node 0 with sigma=anchor_sigma, z=GT (no noise)\n",
    "      - tiny prior for all vars: Lam=tiny_prior*I, eta=0 (like your old code)\n",
    "    Returns:\n",
    "      vars, prior_facs, between_facs\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) parse GT positions\n",
    "    # -------------------------\n",
    "    N = len(nodes)\n",
    "    D = 2\n",
    "    GT = np.zeros((N, D), dtype=np.float32)\n",
    "    for i, n in enumerate(nodes):\n",
    "        GT[i, 0] = float(n[\"position\"][\"x\"])\n",
    "        GT[i, 1] = float(n[\"position\"][\"y\"])\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) split edges\n",
    "    # -------------------------\n",
    "    # old code: binary if dst not in {\"prior\",\"anchor\"}\n",
    "    binary_pairs = []\n",
    "    prior_vars = []   # strong priors (\"prior\")\n",
    "    has_anchor_edge = False\n",
    "\n",
    "    for e in edges:\n",
    "        src = e[\"data\"][\"source\"]\n",
    "        dst = e[\"data\"][\"target\"]\n",
    "        if (dst != \"prior\") and (dst != \"anchor\"):\n",
    "            binary_pairs.append((int(src), int(dst)))\n",
    "        elif dst == \"prior\":\n",
    "            prior_vars.append(int(src))\n",
    "        elif dst == \"anchor\":\n",
    "            has_anchor_edge = True\n",
    "\n",
    "    # old code always adds an anchor factor for v0 regardless; edges include \"anchor\" but we enforce anyway\n",
    "    # anchor applies to node 0\n",
    "    if not has_anchor_edge:\n",
    "        # still apply anchor factor; we don't need to modify edges list\n",
    "        pass\n",
    "\n",
    "    E = len(binary_pairs)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) RNG + noises (match old: pre-generate per-edge and per-prior)\n",
    "    # -------------------------\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    odom_noise = np.zeros((E, D), dtype=np.float32)\n",
    "    for k, (i, j) in enumerate(binary_pairs):\n",
    "        odom_noise[k] = rng.normal(0.0, odom_sigma, size=D).astype(np.float32)\n",
    "\n",
    "    # strong prior noises\n",
    "    prior_vars = list(dict.fromkeys(prior_vars))  # unique, stable\n",
    "    P_strong = len(prior_vars)\n",
    "    prior_noise = np.zeros((P_strong, D), dtype=np.float32)\n",
    "    for k, v in enumerate(prior_vars):\n",
    "        prior_noise[k] = rng.normal(0.0, prior_sigma, size=D).astype(np.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) build factor arrays (unary priors = strong priors + anchor)\n",
    "    # -------------------------\n",
    "    # unary factors count\n",
    "    P_anchor = 1\n",
    "    P = P_strong + P_anchor\n",
    "\n",
    "    # between factors\n",
    "    z_b = np.zeros((E, D), dtype=np.float32)\n",
    "    zLam_b = np.zeros((E, D, D), dtype=np.float32)\n",
    "\n",
    "    inv_odom_var = 1.0 / (odom_sigma * odom_sigma)\n",
    "    for k, (i, j) in enumerate(binary_pairs):\n",
    "        z_b[k] = (GT[j] - GT[i]) + odom_noise[k]\n",
    "        zLam_b[k] = np.eye(D, dtype=np.float32) * inv_odom_var\n",
    "\n",
    "    # unary priors\n",
    "    z_p = np.zeros((P, D), dtype=np.float32)\n",
    "    zLam_p = np.zeros((P, D, D), dtype=np.float32)\n",
    "\n",
    "    inv_prior_var = 1.0 / (prior_sigma * prior_sigma)\n",
    "    for k, v in enumerate(prior_vars):\n",
    "        z_p[k] = GT[v] + prior_noise[k]\n",
    "        zLam_p[k] = np.eye(D, dtype=np.float32) * inv_prior_var\n",
    "\n",
    "    # anchor factor at last\n",
    "    z_p[P_strong] = GT[0]\n",
    "    zLam_p[P_strong] = np.eye(D, dtype=np.float32) * (1.0 / (anchor_sigma * anchor_sigma))\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) factor ids (must be global unique)\n",
    "    #    We'll do: priors: [0..P-1], betweens: [P..P+E-1]\n",
    "    # -------------------------\n",
    "    prior_factor_id = np.arange(0, P, dtype=np.int32)\n",
    "    between_factor_id = np.arange(P, P + E, dtype=np.int32)\n",
    "\n",
    "    # -------------------------\n",
    "    # 6) adjacency + ports\n",
    "    # -------------------------\n",
    "    # collect per-variable factor list in deterministic order:\n",
    "    #  - if variable has strong prior: include its unary factor id\n",
    "    #  - if variable is 0: include anchor unary factor id\n",
    "    #  - then include all binary factor ids in edges order\n",
    "    var_adj = [[] for _ in range(N)]\n",
    "\n",
    "    # map from unary factor index -> variable\n",
    "    # strong priors: factor k corresponds to prior_vars[k]\n",
    "    for k, v in enumerate(prior_vars):\n",
    "        fid = int(prior_factor_id[k])\n",
    "        var_adj[v].append(fid)\n",
    "\n",
    "    # anchor unary factor is prior_factor_id[P_strong], attached to v=0\n",
    "    anchor_fid = int(prior_factor_id[P_strong])\n",
    "    var_adj[0].append(anchor_fid)\n",
    "\n",
    "    # binary factors: fid = between_factor_id[k], attached to i and j\n",
    "    for k, (i, j) in enumerate(binary_pairs):\n",
    "        fid = int(between_factor_id[k])\n",
    "        var_adj[i].append(fid)\n",
    "        var_adj[j].append(fid)\n",
    "\n",
    "    max_deg = max(len(a) for a in var_adj) if N > 0 else 0\n",
    "    Ni_v = max_deg + 1  # +dummy like your grid (last port invalid)\n",
    "    adj_factor_idx = -np.ones((N, Ni_v), dtype=np.int32)\n",
    "\n",
    "    # map (v, fid) -> port index\n",
    "    port_of = {}\n",
    "    for v in range(N):\n",
    "        for p, fid in enumerate(var_adj[v]):\n",
    "            adj_factor_idx[v, p] = fid\n",
    "            port_of[(v, fid)] = p\n",
    "        # last port stays -1\n",
    "\n",
    "    # -------------------------\n",
    "    # 7) build Factor.adj_var_id / adj_var_idx\n",
    "    # -------------------------\n",
    "    # priors: unary => shape (P,1)\n",
    "    adj_var_id_p = np.zeros((P, 1), dtype=np.int32)\n",
    "    adj_var_idx_p = np.zeros((P, 1), dtype=np.int32)\n",
    "\n",
    "    for k, v in enumerate(prior_vars):\n",
    "        fid = int(prior_factor_id[k])\n",
    "        adj_var_id_p[k, 0] = v\n",
    "        adj_var_idx_p[k, 0] = port_of[(v, fid)]\n",
    "\n",
    "    # anchor\n",
    "    adj_var_id_p[P_strong, 0] = 0\n",
    "    adj_var_idx_p[P_strong, 0] = port_of[(0, anchor_fid)]\n",
    "\n",
    "    # betweens: binary => shape (E,2)\n",
    "    adj_var_id_b = np.zeros((E, 2), dtype=np.int32)\n",
    "    adj_var_idx_b = np.zeros((E, 2), dtype=np.int32)\n",
    "\n",
    "    for k, (i, j) in enumerate(binary_pairs):\n",
    "        fid = int(between_factor_id[k])\n",
    "        adj_var_id_b[k, 0] = i\n",
    "        adj_var_id_b[k, 1] = j\n",
    "        adj_var_idx_b[k, 0] = port_of[(i, fid)]\n",
    "        adj_var_idx_b[k, 1] = port_of[(j, fid)]\n",
    "\n",
    "    # -------------------------\n",
    "    # 8) variables: belief, msgs, tiny prior\n",
    "    # -------------------------\n",
    "    belief = Gaussian(\n",
    "        eta=jnp.zeros((N, D), dtype=jnp.float32),\n",
    "        Lam=jnp.tile(jnp.eye(D, dtype=jnp.float32)[None, :, :], (N, 1, 1)),\n",
    "    )\n",
    "\n",
    "    # override Lam to tiny_prior * I, eta=0 (matches your old C++/python)\n",
    "    belief = Gaussian(\n",
    "        eta=jnp.zeros((N, D), dtype=jnp.float32),\n",
    "        Lam=(tiny_prior * jnp.eye(D, dtype=jnp.float32))[None, :, :].repeat(N, axis=0),\n",
    "    )\n",
    "\n",
    "    msgs = Gaussian(\n",
    "        eta=jnp.zeros((N, Ni_v, D), dtype=jnp.float32),\n",
    "        Lam=jnp.zeros((N, Ni_v, D, D), dtype=jnp.float32),\n",
    "    )\n",
    "\n",
    "    vars = Variable(\n",
    "        var_id=jnp.arange(N, dtype=jnp.int32),\n",
    "        belief=belief,\n",
    "        msgs=msgs,\n",
    "        adj_factor_idx=jnp.asarray(adj_factor_idx),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 9) factors\n",
    "    # -------------------------\n",
    "    thr_p = np.ones((P, 1), dtype=np.float32) * float(robust_threshold)\n",
    "    thr_b = np.ones((E, 1), dtype=np.float32) * float(robust_threshold)\n",
    "\n",
    "    # potential: your update_factor overwrites facs.potential, but Factor class requires it.\n",
    "    # set zeros with correct shape:\n",
    "    pot_p = Gaussian(\n",
    "        eta=jnp.zeros((P, D), dtype=jnp.float32),\n",
    "        Lam=jnp.zeros((P, D, D), dtype=jnp.float32),\n",
    "    )\n",
    "    pot_b = Gaussian(\n",
    "        eta=jnp.zeros((E, 2 * D), dtype=jnp.float32),\n",
    "        Lam=jnp.zeros((E, 2 * D, 2 * D), dtype=jnp.float32),\n",
    "    )\n",
    "\n",
    "    prior_facs = Factor(\n",
    "        factor_id=jnp.asarray(prior_factor_id),\n",
    "        z=jnp.asarray(z_p),\n",
    "        z_Lam=jnp.asarray(zLam_p),\n",
    "        threshold=jnp.asarray(thr_p),\n",
    "        potential=pot_p,\n",
    "        adj_var_id=jnp.asarray(adj_var_id_p),\n",
    "        adj_var_idx=jnp.asarray(adj_var_idx_p),\n",
    "    )\n",
    "\n",
    "    between_facs = Factor(\n",
    "        factor_id=jnp.asarray(between_factor_id),\n",
    "        z=jnp.asarray(z_b),\n",
    "        z_Lam=jnp.asarray(zLam_b),\n",
    "        threshold=jnp.asarray(thr_b),\n",
    "        potential=pot_b,\n",
    "        adj_var_id=jnp.asarray(adj_var_id_b),\n",
    "        adj_var_idx=jnp.asarray(adj_var_idx_b),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 10) device put\n",
    "    # -------------------------\n",
    "    dev = jax.devices(device)[0]\n",
    "    vars = jax.device_put(vars, dev)\n",
    "    prior_facs = jax.device_put(prior_facs, dev)\n",
    "    between_facs = jax.device_put(between_facs, dev)\n",
    "\n",
    "    return vars, prior_facs, between_facs\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "def one_step(state, _):\n",
    "    vars, prior_facs, between_facs = state\n",
    "    vars, vtof_msgs, linpoints = update_variable(vars)\n",
    "    prior_facs, vars = update_factor(prior_facs, vars, vtof_msgs, linpoints, h_fn, l2)\n",
    "    between_facs, vars = update_factor(between_facs, vars, vtof_msgs, linpoints, h2_fn, l2)\n",
    "    return (vars, prior_facs, between_facs), None\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"num_iters\",))\n",
    "def run_iters(vars, prior_facs, between_facs, num_iters: int):\n",
    "    (vars, prior_facs, between_facs), _ = jax.lax.scan(\n",
    "        one_step,\n",
    "        (vars, prior_facs, between_facs),\n",
    "        xs=None,\n",
    "        length=num_iters\n",
    "    )\n",
    "    return vars, prior_facs, between_facs\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def energy_map_jax(vars: Variable, GT: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Equivalent to old energy_map():\n",
    "    sum_i 0.5 * ||mu_i - GT_i||^2\n",
    "    \"\"\"\n",
    "    mu = jax.vmap(lambda b: b.mu())(vars.belief)   # (N, D)\n",
    "    r = mu - GT                                    # (N, D)\n",
    "    return 0.5 * jnp.sum(r * r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe836c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "step = 25\n",
    "prob = 0.05\n",
    "radius = 50\n",
    "prior_prop = 0.02\n",
    "prior_sigma = 1.0\n",
    "odom_sigma = 1.0\n",
    "seed = 2001\n",
    "\n",
    "# -----------------------\n",
    "# 1) nodes, edges\n",
    "# -----------------------\n",
    "nodes, edges = make_slam_like_graph(\n",
    "    N=N,\n",
    "    step_size=step,\n",
    "    loop_prob=prob,\n",
    "    loop_radius=radius,\n",
    "    prior_prop=prior_prop,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "vars, prior_facs, between_facs = build_posegraph_jax_from_nodes_edges(\n",
    "    nodes, edges,\n",
    "    prior_sigma=prior_sigma,\n",
    "    odom_sigma=odom_sigma,\n",
    "    tiny_prior=1e-12,      # 跟你旧版一致\n",
    "    anchor_sigma=1e-4,     # 跟你旧版一致\n",
    "    seed=seed,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae8281df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub got incompatible shapes for broadcasting: (0,), (2,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m GT \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39marray([[n[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], n[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_iters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_facs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetween_facs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m energy \u001b[38;5;241m=\u001b[39m energy_map_jax(\u001b[38;5;28mvars\u001b[39m, GT)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(energy))\n",
      "    \u001b[1;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[75], line 316\u001b[0m, in \u001b[0;36mrun_iters\u001b[1;34m(vars, prior_facs, between_facs, num_iters)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jax\u001b[38;5;241m.\u001b[39mjit, static_argnames\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_iters\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_iters\u001b[39m(\u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs, num_iters: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 316\u001b[0m     (\u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs), _ \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mone_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_facs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetween_facs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iters\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs\n",
      "    \u001b[1;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[75], line 309\u001b[0m, in \u001b[0;36mone_step\u001b[1;34m(state, _)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mvars\u001b[39m, vtof_msgs, linpoints \u001b[38;5;241m=\u001b[39m update_variable(\u001b[38;5;28mvars\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m prior_facs, \u001b[38;5;28mvars\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprior_facs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvtof_msgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m between_facs, \u001b[38;5;28mvars\u001b[39m \u001b[38;5;241m=\u001b[39m update_factor(between_facs, \u001b[38;5;28mvars\u001b[39m, vtof_msgs, linpoints, h2_fn, l2)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mvars\u001b[39m, prior_facs, between_facs), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[74], line 301\u001b[0m, in \u001b[0;36mupdate_factor\u001b[1;34m(facs, vars, vtof_msgs, linpoints, f, w)\u001b[0m\n\u001b[0;32m    293\u001b[0m linpoints_reordered \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x[facs\u001b[38;5;241m.\u001b[39madj_var_id], linpoints\n\u001b[0;32m    295\u001b[0m )\n\u001b[0;32m    297\u001b[0m facs\u001b[38;5;241m.\u001b[39mpotential \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(factor_update, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))(\n\u001b[0;32m    298\u001b[0m     facs, linpoints_reordered, f, w\n\u001b[0;32m    299\u001b[0m )\n\u001b[1;32m--> 301\u001b[0m ftov_msgs \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_ftov_msg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfacs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvtof_msgs_reordered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# ---- Robust scatter: flatten indices and updates to avoid shape/broadcast ambiguity ----\u001b[39;00m\n\u001b[0;32m    304\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m\u001b[38;5;241m.\u001b[39mmsgs\u001b[38;5;241m.\u001b[39meta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[1;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[74], line 276\u001b[0m, in \u001b[0;36mcompute_ftov_msg\u001b[1;34m(factor, vtof_msgs)\u001b[0m\n\u001b[0;32m    273\u001b[0m     out_Lam \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mstack([msg1_Lam, msg2_Lam], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gaussian(out_eta, out_Lam)\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_adj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munary_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[74], line 260\u001b[0m, in \u001b[0;36mcompute_ftov_msg.<locals>.binary_case\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    257\u001b[0m Lam_m2 \u001b[38;5;241m=\u001b[39m Lam\u001b[38;5;241m.\u001b[39mat[D:, D:]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m-\u001b[39mLam2_in)\n\u001b[0;32m    259\u001b[0m eta1b \u001b[38;5;241m=\u001b[39m eta[:D]\n\u001b[1;32m--> 260\u001b[0m eta2b \u001b[38;5;241m=\u001b[39m \u001b[43meta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mD\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meta2_in\u001b[49m\n\u001b[0;32m    261\u001b[0m Lam11b \u001b[38;5;241m=\u001b[39m Lam[:D, :D]\n\u001b[0;32m    262\u001b[0m Lam12b \u001b[38;5;241m=\u001b[39m Lam[:D, D:]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\jax-gpu\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:1083\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1083\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\jax-gpu\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:583\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    581\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[1;32m--> 583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\jax-gpu\\lib\\site-packages\\jax\\_src\\numpy\\ufunc_api.py:182\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[1;34m(self, out, where, *args)\u001b[0m\n\u001b[0;32m    180\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    181\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\jax-gpu\\lib\\site-packages\\jax\\_src\\numpy\\ufuncs.py:1568\u001b[0m, in \u001b[0;36msubtract\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;129m@binary_ufunc\u001b[39m(identity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, at\u001b[38;5;241m=\u001b[39m_subtract_at)\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msubtract\u001b[39m(x: ArrayLike, y: ArrayLike, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Subtract two arrays element-wise.\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \n\u001b[0;32m   1545\u001b[0m \u001b[38;5;124;03m  JAX implementation of :obj:`numpy.subtract`. This is a universal function,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;124;03m    Array([-10,  -9,  -8,  -7], dtype=int32)\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1568\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubtract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1569\u001b[0m   jnp_error\u001b[38;5;241m.\u001b[39m_set_error_if_nan(out)\n\u001b[0;32m   1570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[1;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\jax-gpu\\lib\\site-packages\\jax\\_src\\lax\\lax.py:136\u001b[0m, in \u001b[0;36m_try_broadcast_shapes\u001b[1;34m(name, *shapes)\u001b[0m\n\u001b[0;32m    134\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    137\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[1;31mTypeError\u001b[0m: sub got incompatible shapes for broadcasting: (0,), (2,)."
     ]
    }
   ],
   "source": [
    "GT = jnp.asarray(\n",
    "    np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in nodes], dtype=np.float32)\n",
    ")\n",
    "vars, prior_facs, between_facs = run_iters(vars, prior_facs, between_facs, num_iters=1000)\n",
    "energy = energy_map_jax(vars, GT)\n",
    "print(float(energy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba9b77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 2)\n",
      "(9436, 4)\n"
     ]
    }
   ],
   "source": [
    "print(prior_facs.potential.eta.shape)\n",
    "print(between_facs.potential.eta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fed1eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ni_v = 4\n",
      "max_deg = 3 mean_deg = 2.0198 p95_deg = 2.0\n"
     ]
    }
   ],
   "source": [
    "adj = np.array(vars.adj_factor_idx)      # (N, Ni_v)\n",
    "deg = (adj >= 0).sum(axis=1)             # 每个变量的真实度数\n",
    "print(\"Ni_v =\", adj.shape[1])\n",
    "print(\"max_deg =\", deg.max(), \"mean_deg =\", deg.mean(), \"p95_deg =\", np.percentile(deg, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bcddf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_deg = 3\n",
      "nodes with max_deg: [  22   74   88  114  126  210  354  417  432  435  481  486  612  616\n",
      "  628  694  699  712  795  817  824  840  871  873  899  943  991 1005\n",
      " 1022 1064 1069 1125 1168 1207 1236 1249 1261 1266 1277 1311 1388 1477\n",
      " 1480 1497 1529 1581 1740 1741 1848 1878 1884 1933 2030 2054 2167 2321\n",
      " 2406 2732 2785 2793 2838 2900 2919 2924 2972 2991 2994 3031 3128 3210\n",
      " 3251 3292 3333 3392 3404 3436 3441 3466 3513 3542 3596 3628 3677 3887\n",
      " 3900 3907 4041 4069 4184 4227 4255 4388 4449 4459 4695 4809 4895 4897\n",
      " 4928 4997]\n",
      "one max-deg node: 22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "adj = np.array(vars.adj_factor_idx)   # (N, Ni_v)\n",
    "deg = (adj >= 0).sum(axis=1)          # (N,)\n",
    "\n",
    "max_deg = deg.max()\n",
    "nodes_max = np.where(deg == max_deg)[0]\n",
    "\n",
    "print(\"max_deg =\", max_deg)\n",
    "print(\"nodes with max_deg:\", nodes_max)\n",
    "v_max = int(nodes_max[0])\n",
    "print(\"one max-deg node:\", v_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23665826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge-count from edges list: 3\n"
     ]
    }
   ],
   "source": [
    "v = v_max\n",
    "cnt = 0\n",
    "for e in edges:\n",
    "    s = e[\"data\"][\"source\"]\n",
    "    t = e[\"data\"][\"target\"]\n",
    "    if t not in (\"prior\", \"anchor\"):\n",
    "        i, j = int(s), int(t)\n",
    "        if i == v or j == v:\n",
    "            cnt += 1\n",
    "    elif t == \"prior\" and int(s) == v:\n",
    "        cnt += 1\n",
    "    elif t == \"anchor\" and v == 0:\n",
    "        cnt += 1\n",
    "\n",
    "print(\"edge-count from edges list:\", cnt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
