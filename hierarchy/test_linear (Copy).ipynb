{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db67ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dash\n",
    "from dash import html, dcc, Input, Output, State, no_update\n",
    "import dash_cytoscape as cyto\n",
    "import numpy as np\n",
    "from scipy.linalg import block_diag\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# ==== GBP import ====\n",
    "from gbp.gbp import *\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"Factor Graph SVD Abs&Recovery\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# SLAM-like base graph\n",
    "# -----------------------\n",
    "def make_slam_like_graph(N=100, step_size=25, loop_prob=0.05, loop_radius=50, prior_prop=0.0, seed=None):\n",
    "    if seed is None :\n",
    "        rng = np.random.default_rng()  # ✅ Ensure we have an RNG\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    nodes, edges = [], []\n",
    "    positions = []\n",
    "    x, y = 0.0, 0.0\n",
    "    positions.append((x, y))\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: trajectory generation\n",
    "    for _ in range(1, int(N)):\n",
    "        dx, dy = rng.standard_normal(2)  # replace np.random.randn\n",
    "        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "        dx, dy = dx / norm * float(step_size), dy / norm * float(step_size)\n",
    "        x, y = x + dx, y + dy\n",
    "        positions.append((x, y))\n",
    "\n",
    "    # Sequential edges along the path\n",
    "    for i, (px, py) in enumerate(positions):\n",
    "        nodes.append({\n",
    "            \"data\": {\"id\": f\"{i}\", \"layer\": 0, \"dim\": 2, \"num_base\": 1},\n",
    "            \"position\": {\"x\": float(px), \"y\": float(py)}\n",
    "        })\n",
    "\n",
    "    for i in range(int(N) - 1):\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{i+1}\"}})\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: loop-closure edges\n",
    "    for i in range(int(N)):\n",
    "        for j in range(i + 5, int(N)):\n",
    "            if rng.random() < float(loop_prob):  # replace np.random.rand\n",
    "                xi, yi = positions[i]\n",
    "                xj, yj = positions[j]\n",
    "                if np.hypot(xi - xj, yi - yj) < float(loop_radius):\n",
    "                    edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{j}\"}})\n",
    "\n",
    "    # ✅ Sample priors using the same RNG\n",
    "    if prior_prop <= 0.0:\n",
    "        strong_ids = {0}\n",
    "    elif prior_prop >= 1.0:\n",
    "        strong_ids = set(range(N))\n",
    "    else:\n",
    "        k = max(1, int(np.floor(prior_prop * N)))\n",
    "        strong_ids = set(rng.choice(N, size=k, replace=False).tolist())\n",
    "\n",
    "    # Add edges for nodes with strong priors\n",
    "    for i in strong_ids:\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": \"prior\"}})\n",
    "\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Grid aggregation\n",
    "# -----------------------\n",
    "def fuse_to_super_grid(prev_nodes, prev_edges, gx, gy, layer_idx):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    xmin, ymin = positions.min(axis=0); xmax, ymax = positions.max(axis=0)\n",
    "    cell_w = (xmax - xmin) / gx if gx > 0 else 1.0\n",
    "    cell_h = (ymax - ymin) / gy if gy > 0 else 1.0\n",
    "    if cell_w == 0: cell_w = 1.0\n",
    "    if cell_h == 0: cell_h = 1.0\n",
    "    cell_map = {}\n",
    "    for idx, n in enumerate(prev_nodes):\n",
    "        x, y = n[\"position\"][\"x\"], n[\"position\"][\"y\"]\n",
    "        cx = min(int((x - xmin) / cell_w), gx - 1)\n",
    "        cy = min(int((y - ymin) / cell_h), gy - 1)\n",
    "        cid = cx + cy * gx\n",
    "        cell_map.setdefault(cid, []).append(idx)\n",
    "    super_nodes, node_map = [], {}\n",
    "    for cid, indices in cell_map.items():\n",
    "        pts = positions[indices]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in indices]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in indices]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums))\n",
    "        nid = str(len(super_nodes))\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val   # Inherit the sum\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in indices:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            elif su == sv:\n",
    "                eid = tuple(sorted((su, \"prior\")))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = tuple(sorted((su, \"prior\")))\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "# -----------------------\n",
    "# K-Means aggregation\n",
    "# -----------------------\n",
    "def fuse_to_super_kmeans(prev_nodes, prev_edges, k, layer_idx, max_iters=20, tol=1e-6, seed=0):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    n = positions.shape[0]\n",
    "    if k <= 0: \n",
    "        k = 1\n",
    "    k = min(k, n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # -------- Improved initialization --------\n",
    "    # Randomly sample k points without replacement to ensure each cluster starts with a distinct point\n",
    "    init_idx = rng.choice(n, size=k, replace=False)\n",
    "    centers = positions[init_idx]\n",
    "\n",
    "    # Lloyd iterations\n",
    "    for _ in range(max_iters):\n",
    "        d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "        assign = np.argmin(d2, axis=1)\n",
    "\n",
    "        # -------- Empty-cluster fix --------\n",
    "        counts = np.bincount(assign, minlength=k)\n",
    "        empty_clusters = np.where(counts == 0)[0]\n",
    "        for ci in empty_clusters:\n",
    "            # Find the largest cluster\n",
    "            big_cluster = np.argmax(counts)\n",
    "            big_idxs = np.where(assign == big_cluster)[0]\n",
    "            # Steal one point over\n",
    "            steal_idx = big_idxs[0]\n",
    "            assign[steal_idx] = ci\n",
    "            counts[big_cluster] -= 1\n",
    "            counts[ci] += 1\n",
    "\n",
    "        moved = 0.0\n",
    "        for ci in range(k):\n",
    "            idxs = np.where(assign == ci)[0]\n",
    "            new_c = positions[idxs].mean(axis=0)\n",
    "            moved = max(moved, float(np.linalg.norm(new_c - centers[ci])))\n",
    "            centers[ci] = new_c\n",
    "        if moved < tol:\n",
    "            break\n",
    "\n",
    "    # Final assign (redo once to be safe)\n",
    "    d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "    assign = np.argmin(d2, axis=1)\n",
    "\n",
    "    counts = np.bincount(assign, minlength=k)\n",
    "    empty_clusters = np.where(counts == 0)[0]\n",
    "    for ci in empty_clusters:\n",
    "        big_cluster = np.argmax(counts)\n",
    "        big_idxs = np.where(assign == big_cluster)[0]\n",
    "        steal_idx = big_idxs[0]\n",
    "        assign[steal_idx] = ci\n",
    "        counts[big_cluster] -= 1\n",
    "        counts[ci] += 1\n",
    "\n",
    "    # ---------- Build the super graph ----------\n",
    "    super_nodes, node_map = [], {}\n",
    "    for ci in range(k):\n",
    "        idxs = np.where(assign == ci)[0]\n",
    "        pts = positions[idxs]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in idxs]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in idxs]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums)) \n",
    "        nid = f\"{ci}\"\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val   # Inherit the sum\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in idxs:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            else:\n",
    "                eid = (su, \"prior\")\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = (su, \"prior\")\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "\n",
    "def copy_to_abs(super_nodes, super_edges, layer_idx):\n",
    "    abs_nodes = []\n",
    "    for n in super_nodes:\n",
    "        nid = n[\"data\"][\"id\"].replace(\"s\", \"a\", 1)\n",
    "        abs_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": n[\"data\"][\"dim\"],\n",
    "                \"num_base\": n[\"data\"].get(\"num_base\", 1)  # Inherit\n",
    "            },\n",
    "            \"position\": {\"x\": n[\"position\"][\"x\"], \"y\": n[\"position\"][\"y\"]}\n",
    "        })\n",
    "    abs_edges = []\n",
    "    for e in super_edges:\n",
    "        abs_edges.append({\"data\": {\n",
    "            \"source\": e[\"data\"][\"source\"].replace(\"s\", \"a\", 1),\n",
    "            \"target\": e[\"data\"][\"target\"].replace(\"s\", \"a\", 1)\n",
    "        }})\n",
    "    return abs_nodes, abs_edges\n",
    "\n",
    "# -----------------------\n",
    "# Sequential merge (tail group absorbs remainder)\n",
    "# -----------------------\n",
    "def fuse_to_super_order(prev_nodes, prev_edges, k, layer_idx, tail_heavy=True):\n",
    "    \"\"\"\n",
    "    Sequentially split prev_nodes in current order into k groups; the last group absorbs the remainder (tail_heavy=True).\n",
    "    Reuse existing rules for aggregating dim/num_base, deduplicating edges, and propagating prior.\n",
    "    \"\"\"\n",
    "    n = len(prev_nodes)\n",
    "    if k <= 0: k = 1\n",
    "    k = min(k, n)\n",
    "\n",
    "    # Group sizes\n",
    "    base = n // k\n",
    "    rem  = n %  k\n",
    "    if rem > 0:\n",
    "        sizes = [k]*(base) + [rem]     # Tail absorbs remainder: ..., last += rem\n",
    "    else:\n",
    "        sizes = [k]*(base)\n",
    "\n",
    "    # Build groups: record indices per group\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for s in sizes:\n",
    "        groups.append(list(range(start, start+s)))\n",
    "        start += s\n",
    "\n",
    "    # ---- Build super_nodes & node_map ----\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "\n",
    "    super_nodes, node_map = [], {}\n",
    "    for gi, idxs in enumerate(groups):\n",
    "        pts = positions[idxs]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in idxs]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in idxs]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums))\n",
    "\n",
    "        nid = f\"{gi}\"  # Same as kmeans: use group index as id (string)\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        # Build base-id -> super-id mapping (note: ids are strings throughout)\n",
    "        for i in idxs:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "\n",
    "    # ---- Super edges: keep and deduplicate inter-group edges; intra-group edges collapse to prior; prior edges roll up to their owning super ----\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            else:\n",
    "                # Intra-group pairwise edge → group prior (consistent with grid/kmeans handling)\n",
    "                eid = tuple(sorted((su, \"prior\")))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = tuple(sorted((su, \"prior\")))\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Tools\n",
    "# -----------------------\n",
    "def parse_layer_name(name):\n",
    "    if name == \"base\": return (\"base\", 0)\n",
    "    m = re.match(r\"(super|abs)(\\d+)$\", name)\n",
    "    return (m.group(1), int(m.group(2))) if m else (\"base\", 0)\n",
    "\n",
    "def highest_pair_idx(names):\n",
    "    hi = 0\n",
    "    for nm in names:\n",
    "        kind, k = parse_layer_name(nm)\n",
    "        if kind in (\"super\",\"abs\"): hi = max(hi, k)\n",
    "    return hi\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Initialization & Boundary\n",
    "# -----------------------\n",
    "def init_layers(N=100, step_size=25, loop_prob=0.05, loop_radius=50, prior_prop=0.0, seed=None):\n",
    "    base_nodes, base_edges = make_slam_like_graph(N, step_size, loop_prob, loop_radius, prior_prop, seed)\n",
    "    return [{\"name\": \"base\", \"nodes\": base_nodes, \"edges\": base_edges}]\n",
    "\n",
    "VIEW_W, VIEW_H = 960, 600\n",
    "ASPECT = VIEW_W / VIEW_H\n",
    "AXIS_PAD=20.0\n",
    "# ==== Blobal Status ====\n",
    "layers = init_layers()\n",
    "\n",
    "\n",
    "def adjust_bounds_to_aspect(xmin, xmax, ymin, ymax, aspect):\n",
    "    cx=(xmin+xmax)/2; cy=(ymin+ymax)/2\n",
    "    dx=xmax-xmin; dy=ymax-ymin\n",
    "    if dx<=0: dx=1\n",
    "    if dy<=0: dy=1\n",
    "    if dx/dy > aspect:\n",
    "        dy_new=dx/aspect\n",
    "        return xmin,xmax,cy-dy_new/2,cy+dy_new/2\n",
    "    else:\n",
    "        dx_new=dy*aspect\n",
    "        return cx-dx_new/2,cx+dx_new/2,ymin,ymax\n",
    "\n",
    "def reset_global_bounds(base_nodes):\n",
    "    global GLOBAL_XMIN, GLOBAL_XMAX, GLOBAL_YMIN, GLOBAL_YMAX\n",
    "    global GLOBAL_XMIN_ADJ, GLOBAL_XMAX_ADJ, GLOBAL_YMIN_ADJ, GLOBAL_YMAX_ADJ\n",
    "    xs=[n[\"position\"][\"x\"] for n in base_nodes] or [0.0]\n",
    "    ys=[n[\"position\"][\"y\"] for n in base_nodes] or [0.0]\n",
    "    GLOBAL_XMIN,GLOBAL_XMAX=min(xs),max(xs)\n",
    "    GLOBAL_YMIN,GLOBAL_YMAX=min(ys),max(ys)\n",
    "    GLOBAL_XMIN_ADJ,GLOBAL_XMAX_ADJ,GLOBAL_YMIN_ADJ,GLOBAL_YMAX_ADJ=adjust_bounds_to_aspect(\n",
    "        GLOBAL_XMIN,GLOBAL_XMAX,GLOBAL_YMIN,GLOBAL_YMAX,ASPECT)\n",
    "\n",
    "# ==== Blobal Status ====\n",
    "layers = init_layers()\n",
    "pair_idx = 0\n",
    "reset_global_bounds(layers[0][\"nodes\"])\n",
    "gbp_graph = None\n",
    "\n",
    "# -----------------------\n",
    "# GBP Graph Construction\n",
    "# -----------------------\n",
    "def build_noisy_pose_graph(\n",
    "    nodes,\n",
    "    edges,\n",
    "    prior_sigma: float = 10,\n",
    "    odom_sigma: float = 10,\n",
    "    tiny_prior: float = 1e-12,\n",
    "    seed=None,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Construct a 2D pose-only factor graph (linear, Gaussian) and inject noise.\n",
    "    Parameters:\n",
    "      prior_sigma : standard deviation of the strong prior (smaller = stronger)\n",
    "      odom_sigma  : standard deviation of odometry measurement noise\n",
    "      prior_prop  : 0.0 = anchor only; (0,1) = randomly select by proportion; >=1.0 = all\n",
    "      tiny_prior  : a tiny prior added to all nodes to prevent singularity\n",
    "      seed        : random seed (for reproducibility)\n",
    "    \"\"\"\n",
    "\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=0)\n",
    "\n",
    "    var_nodes = []\n",
    "    I2 = np.eye(2, dtype=float)\n",
    "    N = len(nodes)\n",
    "\n",
    "    # ---- Pre-generate noise ----\n",
    "    prior_noises = {}\n",
    "    odom_noises = {}\n",
    "\n",
    "    if seed is None:\n",
    "        rng = np.random.default_rng()\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Generate noise for all edges\n",
    "    for e in edges:\n",
    "        src = e[\"data\"][\"source\"]; dst = e[\"data\"][\"target\"]\n",
    "        # Binary edge\n",
    "        if dst != \"prior\":\n",
    "            odom_noises[(int(src[:]), int(dst[:]))] = rng.normal(0.0, odom_sigma, size=2)\n",
    "        # Unary edge (strong prior)\n",
    "        elif dst == \"prior\":\n",
    "            prior_noises[int(src[:])] = rng.normal(0.0, prior_sigma, size=2)\n",
    "\n",
    "\n",
    "    # ---- variable nodes ----\n",
    "    for i, n in enumerate(nodes):\n",
    "        v = VariableNode(i, dofs=2)\n",
    "        v.GT = np.array([n[\"position\"][\"x\"], n[\"position\"][\"y\"]], dtype=float)\n",
    "\n",
    "        # Tiny prior\n",
    "        v.prior.lam = tiny_prior * I2\n",
    "        v.prior.eta = np.zeros(2, dtype=float)\n",
    "\n",
    "        var_nodes.append(v)\n",
    "\n",
    "    fg.var_nodes = var_nodes\n",
    "    fg.n_var_nodes = len(var_nodes)\n",
    "\n",
    "\n",
    "    # ---- prior factors ----\n",
    "    def meas_fn_unary(x, *args):\n",
    "        return x\n",
    "    def jac_fn_unary(x, *args):\n",
    "        return np.eye(2)\n",
    "    # ---- odometry factors ----\n",
    "    def meas_fn(xy, *args):\n",
    "        return xy[2:] - xy[:2]\n",
    "    def jac_fn(xy, *args):\n",
    "        return np.array([[-1, 0, 1, 0],\n",
    "                         [ 0,-1, 0, 1]], dtype=float)\n",
    "    \n",
    "    factors = []\n",
    "    fid = 0\n",
    "\n",
    "    for e in edges:\n",
    "        src = e[\"data\"][\"source\"]; dst = e[\"data\"][\"target\"]\n",
    "        if dst != \"prior\":\n",
    "            i, j = int(src[:]), int(dst[:])\n",
    "            vi, vj = var_nodes[i], var_nodes[j]\n",
    "\n",
    "            meas = (vj.GT - vi.GT) + odom_noises[(i, j)]\n",
    "\n",
    "            meas_lambda = np.eye(len(meas))/ (odom_sigma**2)\n",
    "            f = Factor(fid, [vi, vj], meas, meas_lambda, meas_fn, jac_fn)\n",
    "            f.type = \"base\"\n",
    "            linpoint = np.r_[vi.GT, vj.GT]\n",
    "            f.compute_factor(linpoint=linpoint, update_self=True)\n",
    "\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            vj.adj_factors.append(f)\n",
    "            fid += 1\n",
    "\n",
    "        else:\n",
    "            i = int(src[:])\n",
    "            vi = var_nodes[i]\n",
    "            z = vi.GT + prior_noises[i]\n",
    "\n",
    "            z_lambda = np.eye(len(meas))/ (prior_sigma**2)\n",
    "            f = Factor(fid, [vi], z, z_lambda, meas_fn_unary, jac_fn_unary)\n",
    "            f.type = \"prior\"\n",
    "            f.compute_factor(linpoint=z, update_self=True)\n",
    "\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            fid += 1\n",
    "\n",
    "        # anchor for initial position\n",
    "        v0 = var_nodes[0]\n",
    "        z = v0.GT\n",
    "\n",
    "        z_lambda = np.eye(len(meas))/ ((1e-3)**2)\n",
    "        f = Factor(fid, [v0], z, z_lambda, meas_fn_unary, jac_fn_unary)\n",
    "        f.type = \"prior\"\n",
    "        f.compute_factor(linpoint=z, update_self=True)\n",
    "\n",
    "        factors.append(f)\n",
    "        v0.adj_factors.append(f)\n",
    "        fid += 1\n",
    "\n",
    "    fg.factors = factors\n",
    "    fg.n_factor_nodes = len(factors)\n",
    "    return fg\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "def build_super_graph(layers, eta_damping=0.4):\n",
    "    \"\"\"\n",
    "    Construct the super graph based on the base graph in layers[-2] and the super-grouping in layers[-1].\n",
    "    Requirement: layers[-2][\"graph\"] is an already-built base graph (with unary/binary factors).\n",
    "    layers[-1][\"node_map\"]: { base_node_id (str, e.g., 'b12') -> super_node_id (str) }\n",
    "    \"\"\"\n",
    "    # ---------- Extract base & super ----------\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_nodes = layers[-1][\"nodes\"]\n",
    "    super_edges = layers[-1][\"edges\"]\n",
    "    node_map    = layers[-1][\"node_map\"]   # 'bN' -> 'sX_...'\n",
    "\n",
    "    # base: id(int) -> VariableNode, handy to query dofs and mu\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    # ---------- super_id -> [base_id(int)] ----------\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "\n",
    "    # ---------- For each super group, build a (start, dofs) table ----------\n",
    "    # local_idx[sid][bid] = (start, dofs), total_dofs[sid] = sum(dofs)\n",
    "    local_idx   = {}\n",
    "    total_dofs  = {}\n",
    "    for sid, group in super_groups.items():\n",
    "        off = 0\n",
    "        local_idx[sid] = {}\n",
    "        for bid in group:\n",
    "            d = id2var[bid].dofs\n",
    "            local_idx[sid][bid] = (off, d)\n",
    "            off += d\n",
    "        total_dofs[sid] = off\n",
    "\n",
    "\n",
    "    # ---------- Create super VariableNodes ----------\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "\n",
    "    super_var_nodes = {}\n",
    "    for i, sn in enumerate(super_nodes):\n",
    "        sid = sn[\"data\"][\"id\"]\n",
    "        dofs = total_dofs.get(sid, 0)\n",
    "\n",
    "        v = VariableNode(i, dofs=dofs)\n",
    "        gt_vec = np.zeros(dofs)\n",
    "        mu_blocks = []\n",
    "        Sigma_blocks = []\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            # === Stack base GT ===\n",
    "            gt_base = getattr(id2var[bid], \"GT\", None)\n",
    "            if gt_base is None or len(gt_base) != d:\n",
    "                gt_base = np.zeros(d)\n",
    "            gt_vec[st:st+d] = gt_base\n",
    "\n",
    "            # === Stack base belief ===\n",
    "            vb = id2var[bid]\n",
    "            mu_blocks.append(vb.mu)\n",
    "            Sigma_blocks.append(vb.Sigma)\n",
    "\n",
    "        super_var_nodes[sid] = v\n",
    "        v.GT = gt_vec\n",
    "\n",
    "        mu_super = np.concatenate(mu_blocks) if mu_blocks else np.zeros(dofs)\n",
    "        Sigma_super = block_diag(*Sigma_blocks) if Sigma_blocks else np.eye(dofs)\n",
    "        lam = np.linalg.inv(Sigma_super)\n",
    "        eta = lam @ mu_super\n",
    "        v.mu = mu_super\n",
    "        v.Sigma = Sigma_super\n",
    "        v.belief = NdimGaussian(dofs, eta, lam)\n",
    "        v.prior.lam = 1e-12 * lam\n",
    "        v.prior.eta = 1e-12 * eta\n",
    "\n",
    "        fg.var_nodes.append(v)\n",
    "\n",
    "    fg.n_var_nodes = len(fg.var_nodes)\n",
    "\n",
    "    # ---------- Utility: assemble a group's linpoint (using base belief means) ----------\n",
    "    def make_linpoint_for_group(sid):\n",
    "        x = np.zeros(total_dofs[sid])\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            mu = getattr(id2var[bid], \"mu\", None)\n",
    "            if mu is None or len(mu) != d:\n",
    "                mu = np.zeros(d)\n",
    "            x[st:st+d] = mu\n",
    "        return x\n",
    "\n",
    "    # ---------- 3) super prior (in-group unary + in-group binary) ----------\n",
    "    def make_super_prior_factor(sid, base_factors):\n",
    "        group = super_groups[sid]\n",
    "        idx_map = local_idx[sid]\n",
    "        ncols = total_dofs[sid]\n",
    "\n",
    "        # Select factors whose variables are all within the group (unary or binary)\n",
    "        in_group = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if all(vid in group for vid in vids):\n",
    "                in_group.append(f)\n",
    "\n",
    "        def meas_fn_super_prior(x_super, *args):\n",
    "            meas_fn = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # Assemble this factor's local x\n",
    "                x_loc_list = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) if meas_fn else np.zeros(0)\n",
    "\n",
    "        def jac_fn_super_prior(x_super, *args):\n",
    "            Jrows = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # Build this factor's local x for (potentially) nonlinear Jacobian\n",
    "                x_loc_list = []\n",
    "                dims = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    dims.append(d)\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "                \n",
    "                # Map Jloc column blocks back to the super variable columns\n",
    "                row = np.zeros((Jloc.shape[0], ncols))\n",
    "                c0 = 0\n",
    "                for vid, d in zip(vids, dims):\n",
    "                    st, _ = idx_map[vid]\n",
    "                    row[:, st:st+d] = Jloc[:, c0:c0+d]\n",
    "                    c0 += d\n",
    "\n",
    "                Jrows.append(row)\n",
    "            return np.vstack(Jrows) if Jrows else np.zeros((0, ncols))\n",
    "\n",
    "        # z_super: concatenate each base factor's z\n",
    "        z_list = [f.measurement for f in in_group]\n",
    "        z_lambda_list = [f.measurement_lambda for f in in_group]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_prior, jac_fn_super_prior, z_super, z_super_lambda \n",
    "\n",
    "    # ---------- 4) super between (cross-group binary) ----------\n",
    "    def make_super_between_factor(sidA, sidB, base_factors):\n",
    "        groupA, groupB = super_groups[sidA], super_groups[sidB]\n",
    "        idxA, idxB = local_idx[sidA], local_idx[sidB]\n",
    "        nA, nB = total_dofs[sidA], total_dofs[sidB]\n",
    "\n",
    "        cross = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if len(vids) != 2:\n",
    "                continue\n",
    "            i, j = vids\n",
    "            # One side in A, the other side in B\n",
    "            if (i in groupA and j in groupB) or (i in groupB and j in groupA):\n",
    "                cross.append(f)\n",
    "\n",
    "\n",
    "        def meas_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            meas_fn = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) \n",
    "\n",
    "        def jac_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            Jrows = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                    left_start, right_start = si, nA + sj\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                    left_start, right_start = nA + si, sj\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "\n",
    "                row = np.zeros((Jloc.shape[0], nA + nB))\n",
    "                row[:, left_start:left_start+di]   = Jloc[:, :di] \n",
    "                row[:, right_start:right_start+dj] = Jloc[:, di:di+dj] \n",
    "\n",
    "                Jrows.append(row)\n",
    "            \n",
    "            return np.vstack(Jrows) \n",
    "\n",
    "        z_list = [f.measurement for f in cross]\n",
    "        z_lambda_list = [f.measurement_lambda for f in cross]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_between, jac_fn_super_between, z_super, z_super_lambda\n",
    "\n",
    "\n",
    "    for e in super_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if v == \"prior\":\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_prior_factor(u, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_prior\"\n",
    "            lin0 = make_linpoint_for_group(u)\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            \n",
    "        else:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_between_factor(u, v, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u], super_var_nodes[v]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_between\"\n",
    "            lin0 = np.concatenate([make_linpoint_for_group(u), make_linpoint_for_group(v)])\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            super_var_nodes[v].adj_factors.append(f)\n",
    "\n",
    "\n",
    "    fg.n_factor_nodes = len(fg.factors)\n",
    "    return fg\n",
    "\n",
    "\n",
    "def build_abs_graph(\n",
    "    layers,\n",
    "    r_reduced = 2,\n",
    "    eta_damping=0.4):\n",
    "\n",
    "    abs_var_nodes = {}\n",
    "    Bs = {}\n",
    "    ks = {}\n",
    "    k2s = {}\n",
    "\n",
    "    # === 1. Build Abstraction Variables ===\n",
    "    abs_fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "    sup_fg = layers[-2][\"graph\"]\n",
    "\n",
    "    for sn in sup_fg.var_nodes:\n",
    "        if sn.dofs <= r_reduced:\n",
    "            r = sn.dofs  # No reduction if dofs already <= r\n",
    "        else:\n",
    "            r = r_reduced\n",
    "\n",
    "        sid = sn.variableID\n",
    "        varis_sup_mu = sn.mu\n",
    "        varis_sup_sigma = sn.Sigma\n",
    "        \n",
    "        # Step 1: Eigen decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(varis_sup_sigma)\n",
    "\n",
    "        # Step 2: Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
    "        idx = np.argsort(eigvals)[::-1]      # Get indices of sorted eigenvalues (largest first)\n",
    "        eigvals = eigvals[idx]               # Reorder eigenvalues\n",
    "        eigvecs = eigvecs[:, idx]            # Reorder corresponding eigenvectors\n",
    "\n",
    "        # Step 3: Select the top-k eigenvectors to form the projection matrix (principal subspace)\n",
    "        B_reduced = eigvecs[:, :r]                 # B_reduced: shape (sup_dof, r), projects r to sup_dof\n",
    "        Bs[sid] = B_reduced                        # Store the projection matrix for this variable\n",
    "\n",
    "        # Step 4: Project eta and Lam onto the reduced 2D subspace\n",
    "        varis_abs_mu = B_reduced.T @ varis_sup_mu          # Projected natural mean: shape (2,)\n",
    "        varis_abs_sigma = B_reduced.T @ varis_sup_sigma @ B_reduced  # Projected covariance: shape (2, 2)\n",
    "        ks[sid] = varis_sup_mu - B_reduced @ varis_abs_mu  # Store the mean offset for this variable\n",
    "        #k2s[sid] = varis_sup_sigma - B_reduced @ varis_abs_sigma @ B_reduced.T  # Residual covariance\n",
    "\n",
    "        varis_abs_lam = np.linalg.inv(varis_abs_sigma)  # Inverse covariance (precision matrix): shape (2, 2)\n",
    "        varis_abs_eta = varis_abs_lam @ varis_abs_mu  # Natural parameters: shape (2,)\n",
    "\n",
    "        v = VariableNode(sid, dofs=r)\n",
    "        v.GT = sn.GT\n",
    "        v.prior.lam = 1e-10 * varis_abs_lam\n",
    "        v.prior.eta = 1e-10 * varis_abs_eta\n",
    "        v.mu = varis_abs_mu\n",
    "        v.Sigma = varis_abs_sigma\n",
    "        v.belief = NdimGaussian(r, varis_abs_eta, varis_abs_lam)\n",
    "\n",
    "        abs_var_nodes[sid] = v\n",
    "        abs_fg.var_nodes.append(v)\n",
    "    abs_fg.n_var_nodes = len(abs_fg.var_nodes)\n",
    "\n",
    "\n",
    "    # === 2. Abstract Prior ===\n",
    "    def make_abs_prior_factor(sup_factor):\n",
    "        abs_id = sup_factor.adj_var_nodes[0].variableID\n",
    "        B = Bs[abs_id]\n",
    "        k = ks[abs_id]\n",
    "\n",
    "        def meas_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.meas_fn(B @ x_abs + k)\n",
    "        \n",
    "        def jac_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.jac_fn(B @ x_abs + k) @ B\n",
    "\n",
    "        return meas_fn_abs_prior, jac_fn_abs_prior, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "\n",
    "    # === 3. Abstract Between ===\n",
    "    def make_abs_between_factor(sup_factor):\n",
    "        vids = [v.variableID for v in sup_factor.adj_var_nodes]\n",
    "        i, j = vids # two variable IDs\n",
    "        ni = abs_var_nodes[i].dofs\n",
    "        Bi, Bj = Bs[i], Bs[j]\n",
    "        ki, kj = ks[i], ks[j]                       \n",
    "    \n",
    "\n",
    "        def meas_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            return sup_factor.meas_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "\n",
    "        def jac_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            J_sup = sup_factor.jac_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "            J_abs = np.zeros((J_sup.shape[0], ni + xj.shape[0]))\n",
    "            J_abs[:, :ni] = J_sup[:, :Bi.shape[0]] @ Bi\n",
    "            J_abs[:, ni:] = J_sup[:, Bi.shape[0]:] @ Bj\n",
    "            return J_abs\n",
    "        \n",
    "        return meas_fn_super_between, jac_fn_super_between, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "    for f in sup_fg.factors:\n",
    "        if len(f.adj_var_nodes) == 1:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_prior_factor(f)\n",
    "            v = abs_var_nodes[f.adj_var_nodes[0].variableID]\n",
    "            abs_f = Factor(f.factorID, [v], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_prior\"\n",
    "            abs_f.adj_beliefs = [v.belief]\n",
    "\n",
    "            lin0 = v.mu\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            v.adj_factors.append(abs_f)\n",
    "\n",
    "        elif len(f.adj_var_nodes) == 2:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_between_factor(f)\n",
    "            i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "            vi, vj = abs_var_nodes[i], abs_var_nodes[j]\n",
    "            abs_f = Factor(f.factorID, [vi, vj], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_between\"\n",
    "            abs_f.adj_beliefs = [vi.belief, vj.belief]\n",
    "\n",
    "            lin0 = np.concatenate([vi.mu, vj.mu])\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            vi.adj_factors.append(abs_f)\n",
    "            vj.adj_factors.append(abs_f)\n",
    "\n",
    "    abs_fg.n_factor_nodes = len(abs_fg.factors)\n",
    "\n",
    "\n",
    "    return abs_fg, Bs, ks, k2s\n",
    "\n",
    "\n",
    "def bottom_up_modify_super_graph(layers):\n",
    "    \"\"\"\n",
    "    Update super-node means (mu) from base nodes,\n",
    "    and simultaneously adjust variable beliefs and adjacent messages.\n",
    "    \"\"\"\n",
    "\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_graph = layers[-1][\"graph\"]\n",
    "    node_map = layers[-1][\"node_map\"]\n",
    "\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "    sid2idx = {sn[\"data\"][\"id\"]: i for i, sn in enumerate(layers[-1][\"nodes\"])}\n",
    "\n",
    "    for sid, group in super_groups.items():\n",
    "        mu_blocks = [id2var[bid].mu for bid in group]\n",
    "        mu_super = np.concatenate(mu_blocks) if mu_blocks else np.zeros(0)\n",
    "\n",
    "        if sid in sid2idx:\n",
    "            idx = sid2idx[sid]\n",
    "            v = super_graph.var_nodes[idx]\n",
    "\n",
    "            # Old belief\n",
    "            old_belief = v.belief\n",
    "\n",
    "            # 1. Update mu\n",
    "            #v.mu = mu_super\n",
    "\n",
    "            # 2. New belief (use old Sigma + new mu)\n",
    "            #lam = np.linalg.inv(v.Sigma)\n",
    "            #eta = lam @ v.mu\n",
    "            #new_belief = NdimGaussian(v.dofs, eta, lam)\n",
    "            #v.belief = new_belief\n",
    "            #v.prior = NdimGaussian(v.dofs)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            # 3. update adj_beliefs and messages\n",
    "            if v.adj_factors:\n",
    "                n_adj = len(v.adj_factors)\n",
    "                d_eta = new_belief.eta - old_belief.eta\n",
    "                d_lam = new_belief.lam - old_belief.lam\n",
    "                for f in v.adj_factors:\n",
    "                    if v in f.adj_var_nodes:\n",
    "                        idx_in_factor = f.adj_var_nodes.index(v)\n",
    "                        # update factor's adj_belief\n",
    "                        f.adj_beliefs[idx_in_factor] = new_belief\n",
    "                        # update corresponding messages\n",
    "                        msg = f.messages[idx_in_factor]\n",
    "                        msg.eta += d_eta / n_adj\n",
    "                        msg.lam += d_lam / n_adj\n",
    "                        f.messages[idx_in_factor] = msg\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def top_down_modify_base_and_abs_graph(layers):\n",
    "    \"\"\"\n",
    "    From the super graph downward, split μ to the base graph,\n",
    "    and simultaneously update the base variables' beliefs and the adjacent factors'\n",
    "    adj_beliefs / messages.\n",
    "\n",
    "    Assume layers[-1] is the super layer and layers[-2] is the base layer.\n",
    "    \"\"\"\n",
    "    super_graph = layers[-1][\"graph\"]\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    node_map   = layers[-1][\"node_map\"]  # { base_id(str) -> super_id(str) }\n",
    "\n",
    "\n",
    "    # super_id -> [base_id(int)]\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "    # child lookup\n",
    "    id2var_base = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    a = 0\n",
    "    for s_var in super_graph.var_nodes:\n",
    "        sid = str(s_var.variableID)\n",
    "        if sid not in super_groups:\n",
    "            continue\n",
    "        base_ids = super_groups[sid]\n",
    "\n",
    "        # === split super.mu to base ===\n",
    "        mu_super = s_var.mu\n",
    "        off = 0\n",
    "        for bid in base_ids:\n",
    "            v = id2var_base[bid]\n",
    "            d = v.dofs\n",
    "            mu_child = mu_super[off:off+d]\n",
    "            off += d\n",
    "\n",
    "            old_belief = v.belief\n",
    "\n",
    "            # 1. update mu\n",
    "            v.mu = mu_child\n",
    "\n",
    "            # 2. new belief（keep Σ unchanged，use new mu）\n",
    "            eta = v.belief.lam @ v.mu\n",
    "            new_belief = NdimGaussian(v.dofs, eta, v.belief.lam)\n",
    "            v.belief = new_belief\n",
    "            v.prior = NdimGaussian(v.dofs, 1e-10*eta, 1e-10*v.belief.lam)\n",
    "\n",
    "            # 3. Sync to adjacent factors (this step is important)\n",
    "            if v.adj_factors:\n",
    "                n_adj = len(v.adj_factors)\n",
    "                d_eta = new_belief.eta - old_belief.eta\n",
    "                d_lam = new_belief.lam - old_belief.lam\n",
    "\n",
    "                for f in v.adj_factors:\n",
    "                    if v in f.adj_var_nodes:\n",
    "                        idx = f.adj_var_nodes.index(v)\n",
    "                        # update adj_beliefs\n",
    "                        f.adj_beliefs[idx] = new_belief\n",
    "                        # correct coresponding message\n",
    "                        msg = f.messages[idx]\n",
    "                        msg.eta += d_eta / n_adj\n",
    "                        msg.lam += d_lam / n_adj\n",
    "                        f.messages[idx] = msg\n",
    "\n",
    "    return base_graph\n",
    "\n",
    "\n",
    "def top_down_modify_super_graph(layers):\n",
    "    \"\"\"\n",
    "    From the abs graph downward, project mu / Sigma back to the super graph,\n",
    "    and simultaneously update the super variables' beliefs and the adjacent\n",
    "    factors' adj_beliefs / messages.\n",
    "\n",
    "    Requirements:\n",
    "      - layers[-1] is abs, layers[-2] is super\n",
    "      - Factors at the abs level and the super level share the same factorID (one-to-one)\n",
    "      - The columns of B are orthonormal (from covariance eigenvectors; eigenvectors from np.linalg.eigh are orthogonal)\n",
    "    \"\"\"\n",
    "\n",
    "    abs_graph   = layers[-1][\"graph\"]\n",
    "    super_graph = layers[-2][\"graph\"]\n",
    "    Bs  = layers[-1][\"Bs\"]   # { super_id(int) -> B (d_super × r) }\n",
    "    ks  = layers[-1][\"ks\"]   # { super_id(int) -> k (d_super,) }\n",
    "    #k2s = layers[-1][\"k2s\"]  # { super_id(int) -> residual covariance (d_super × d_super) }\n",
    "\n",
    "    # Prebuild abs factor index: factorID -> Factor\n",
    "    #abs_f_by_id = {f.factorID: f for f in getattr(abs_graph, \"factors\", [])}\n",
    "\n",
    "    # ---- First project variables' mu / Sigma and update beliefs ----\n",
    "    for sn in super_graph.var_nodes:\n",
    "        old_belief = sn.belief\n",
    "        n_adj = len(sn.adj_factors)\n",
    "\n",
    "        sid = sn.variableID\n",
    "        if sid not in Bs or sid not in ks:\n",
    "            continue\n",
    "        B  = Bs[sid]    # (d_s × r)\n",
    "        k  = ks[sid]    # (d_s,)\n",
    "        #k2 = k2s[sid]   # (d_s × d_s)\n",
    "\n",
    "        # x_s = B x_a + k; Σ_s = B Σ_a Bᵀ + k2\n",
    "        mu_a    = abs_graph.var_nodes[sid].mu\n",
    "        mu_s    = B @ mu_a + k\n",
    "        sn.mu   = mu_s\n",
    "\n",
    "        # Refresh super belief (natural parameters) with the new μ and Σ\n",
    "        eta = sn.belief.lam @ sn.mu\n",
    "        new_belief = NdimGaussian(sn.dofs, eta, sn.belief.lam)\n",
    "        sn.belief  = new_belief\n",
    "        sn.prior = NdimGaussian(sn.dofs, 1e-10*eta, 1e-10*sn.belief.lam)\n",
    "\n",
    "    # ---- Then push abs messages back to super, preserving the original super messages on the orthogonal complement ----\n",
    "    # Idea: for the side of the super factor f_sup connected to variable sid:\n",
    "    #   η_s_new = B η_a + (I - B Bᵀ) η_s_old\n",
    "    #   Λ_s_new = B Λ_a Bᵀ + (I - B Bᵀ) Λ_s_old (I - B Bᵀ)\n",
    "    # This ensures the subspace is governed by the abs message, while the orthogonal complement keeps the old super message.\n",
    "\n",
    "\n",
    "        d_eta = new_belief.eta - old_belief.eta\n",
    "        d_lam = new_belief.lam - old_belief.lam\n",
    "\n",
    "        # Iterate over super factors adjacent to this super variable\n",
    "        for f_sup in sn.adj_factors:\n",
    "            idx_side = f_sup.adj_var_nodes.index(sn)\n",
    "            # update the factor's recorded adjacent belief on that side (optional; usually refreshed in the next iteration)\n",
    "            f_sup.adj_beliefs[idx_side] = sn.belief\n",
    "\n",
    "            idx = f_sup.adj_var_nodes.index(sn)\n",
    "            # update adj_beliefs\n",
    "            f_sup.adj_beliefs[idx] = new_belief\n",
    "            # correct coresponding message\n",
    "            msg = f_sup.messages[idx]\n",
    "            msg.eta += d_eta / n_adj\n",
    "            msg.lam += d_lam / n_adj\n",
    "            f_sup.messages[idx] = msg\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def refresh_gbp_results(layers):\n",
    "    \"\"\"\n",
    "    Precompute an affine map to the base plane for each layer:\n",
    "      base:   A_i = I2, b_i = 0\n",
    "      super:  A_s = (1/m) [A_c1, A_c2, ..., A_cm], b_s = (1/m) Σ b_cj\n",
    "      abs:    A_a = A_super(s) @ B_s,             b_a = A_super(s) @ k_s + b_super(s)\n",
    "    Then refresh gbp_result via pos = A @ mu + b.\n",
    "    Convention: use string keys everywhere (aligned with Cytoscape ids).\n",
    "    \"\"\"\n",
    "    if not layers:\n",
    "        return\n",
    "\n",
    "    # ---------- 1) Bottom-up: compute A, b for each layer ----------\n",
    "    for li, L in enumerate(layers):\n",
    "        g = L.get(\"graph\")\n",
    "        if g is None:\n",
    "            L.pop(\"A\", None); L.pop(\"b\", None); L.pop(\"gbp_result\", None)\n",
    "            continue\n",
    "\n",
    "        name = L[\"name\"]\n",
    "        # ---- base ----\n",
    "        if name.startswith(\"base\"):\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for v in g.var_nodes:\n",
    "                key = str(v.variableID)\n",
    "                L[\"A\"][key] = np.eye(2)\n",
    "                L[\"b\"][key] = np.zeros(2, dtype=float)\n",
    "\n",
    "        # ---- super ----\n",
    "        elif name.startswith(\"super\"):\n",
    "            parent = layers[li - 1]\n",
    "            node_map = L[\"node_map\"]  # { prev_id(str) -> super_id(str) }\n",
    "\n",
    "            # Grouping (preserve insertion order to match the concatenation order in build_super_graph)\n",
    "            groups = {}\n",
    "            for prev_id, s_id in node_map.items():\n",
    "                prev_id = str(prev_id); s_id = str(s_id)\n",
    "                groups.setdefault(s_id, []).append(prev_id)\n",
    "\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for s_id, children in groups.items():\n",
    "                m = len(children)\n",
    "                # Horizontal concatenation [A_c1, A_c2, ...]\n",
    "                A_blocks = [parent[\"A\"][cid] for cid in children]  # each block has shape 2×d_c\n",
    "                A_concat = np.hstack(A_blocks) if A_blocks else np.zeros((2, 0))\n",
    "                b_sum = sum((parent[\"b\"][cid] for cid in children), start=np.zeros(2, dtype=float))\n",
    "                L[\"A\"][s_id] = (1.0 / m) * A_concat\n",
    "                L[\"b\"][s_id] = (1.0 / m) * b_sum\n",
    "\n",
    "        # ---- abs ----\n",
    "        elif name.startswith(\"abs\"):\n",
    "            parent = layers[li - 1]  # the corresponding super layer\n",
    "            Bs, ks = L[\"Bs\"], L[\"ks\"]  # Note: keys are the super variableIDs (int)\n",
    "\n",
    "            # Build a mapping between super variableID (int) and the super string id (follow node list order)\n",
    "            # The order of nodes in the parent (super) and this (abs) layer is consistent (copy_to_abs preserves order)\n",
    "            int2sid = {i: str(parent[\"nodes\"][i][\"data\"][\"id\"]) for i in range(len(parent[\"nodes\"]))}\n",
    "\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for av in g.var_nodes:\n",
    "                sid_int = av.variableID              # super variableID (int)\n",
    "                s_id = int2sid.get(sid_int, str(sid_int))  # super string id (also the abs node id)\n",
    "                B = Bs[sid_int]                       # (sum d_c) × r\n",
    "                k = ks[sid_int]                       # (sum d_c,)\n",
    "\n",
    "                A_sup = parent[\"A\"][s_id]             # shape 2 × (sum d_c)\n",
    "                b_sup = parent[\"b\"][s_id]             # shape (2,)\n",
    "\n",
    "                L[\"A\"][s_id] = A_sup @ B              # 2 × r\n",
    "                L[\"b\"][s_id] = A_sup @ k + b_sup      # 2,\n",
    "\n",
    "        else:\n",
    "            # Unknown layer type\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "\n",
    "    # ---------- 2) Compute gbp_result ----------\n",
    "    for li, L in enumerate(layers):\n",
    "        g = L.get(\"graph\")\n",
    "        if g is None:\n",
    "            L.pop(\"gbp_result\", None)\n",
    "            continue\n",
    "\n",
    "        name = L[\"name\"]\n",
    "        res = {}\n",
    "\n",
    "        if name.startswith(\"base\"):\n",
    "            for v in g.var_nodes:\n",
    "                vid = str(v.variableID)\n",
    "                res[vid] = v.mu[:2].tolist()\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Directly use A_super, b_super mapping\n",
    "            # nodes order is consistent with var_nodes order\n",
    "            for i, v in enumerate(g.var_nodes):\n",
    "                s_id = str(L[\"nodes\"][i][\"data\"][\"id\"])\n",
    "                A, b = L[\"A\"][s_id], L[\"b\"][s_id]   # A: 2×(sum d_c)\n",
    "                res[s_id] = (A @ v.mu + b).tolist()\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            parent = layers[li - 1]\n",
    "            # Also align via string ids\n",
    "            for i, v in enumerate(g.var_nodes):\n",
    "                a_id = str(L[\"nodes\"][i][\"data\"][\"id\"])  # same text as the super s_id\n",
    "                A, b = L[\"A\"][a_id], L[\"b\"][a_id]        # A: 2×r\n",
    "                res[a_id] = (A @ v.mu + b).tolist()\n",
    "\n",
    "        L[\"gbp_result\"] = res\n",
    "\n",
    "\n",
    "\n",
    "def vloop(layers):\n",
    "    \"\"\"\n",
    "    Simplified V-cycle:\n",
    "    1) bottom-up: rebuild and iterate once for base / super / abs in order\n",
    "    2) top-down: propagate mu from super -> base\n",
    "    3) refresh gbp_result on each layer for UI use\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- bottom-up ----\n",
    "    #if layers and \"graph\" in layers[0]:\n",
    "    #    layers[0][\"graph\"].synchronous_iteration()\n",
    "        \n",
    "    for i in range(1, len(layers)):\n",
    "        name = layers[i][\"name\"]\n",
    "\n",
    "        if name.startswith(\"super1\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            # layers[i][\"graph\"] = build_super_graph(layers[:i+1])\n",
    "            #layers[i][\"graph\"].synchronous_iteration()\n",
    "            #bottom_up_modify_super_graph(layers[:i+1])\n",
    "            #build_super_graph(layers[:i+1])\n",
    "            #update_super_graph_linearized(layers[:i+1])\n",
    "            pass\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            layers[i][\"graph\"] = build_super_graph(layers[:i+1])\n",
    "            #layers[i][\"graph\"] = update_super_graph_linearized(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Rebuild abs using the previous super\n",
    "            abs_graph, Bs, ks, k2s = build_abs_graph(layers[:i+1])\n",
    "            layers[i][\"graph\"] = abs_graph\n",
    "            layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "        # After build, one iteration per layer\n",
    "        if \"graph\" in layers[i]:\n",
    "            #layers[i][\"graph\"].residual_iteration_var_heap()\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "    # ---- top-down (pass mu) ----\n",
    "    for i in range(len(layers) - 1, 0, -1):\n",
    "        # After one iterations per layer, reproject\n",
    "        if \"graph\" in layers[i]:\n",
    "            #layers[i][\"graph\"].residual_iteration_var_heap()\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # this is very important, but dont know why yet\n",
    "        # so abs layer need more iterations\n",
    "        #if name.startswith(\"abs\"):\n",
    "            #layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "        name = layers[i][\"name\"]\n",
    "        if name.startswith(\"super\"):\n",
    "            # Split super.mu back to base/abs\n",
    "            top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Project abs.mu back to super\n",
    "            top_down_modify_super_graph(layers[:i+1])\n",
    "    \n",
    "\n",
    "    # ---- refresh gbp_result for UI ----\n",
    "    refresh_gbp_results(layers)\n",
    "\n",
    "\n",
    "\n",
    "def compute_energy(layers):\n",
    "    \"\"\"\n",
    "    energy = 0.5 * sum_i || mu_i[0:2] - GT_i[0:2] ||^2  over base layer variables\n",
    "    vectorized version (no per-node np calls)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_graph = layers[0].get(\"graph\", None)\n",
    "        var_nodes = getattr(base_graph, \"var_nodes\", None)\n",
    "        if base_graph is None or not var_nodes:\n",
    "            return \"Energy: -\"\n",
    "\n",
    "        # Stack mu[:2] and GT[:2] for all variables into (N, 2)\n",
    "        mus_2 = np.stack([np.asarray(v.mu[:2], dtype=float) for v in var_nodes], axis=0)\n",
    "        gts_2 = np.stack([np.asarray(v.GT[:2], dtype=float) for v in var_nodes], axis=0)\n",
    "\n",
    "        diff = mus_2 - gts_2                      # shape (N, 2)\n",
    "        total = 0.5 * np.sum(diff * diff)         # 0.5 * sum of squared norms\n",
    "\n",
    "        return f\"Energy: {float(total):.4f}\"\n",
    "    except Exception:\n",
    "        return \"Energy: -\"\n",
    "    \n",
    "\n",
    "class VGraph:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 nonlinear_factors=True,\n",
    "                 eta_damping=0.2,\n",
    "                 beta=0.0,\n",
    "                 iters_since_relinear=0,\n",
    "                 num_undamped_iters=0,\n",
    "                 min_linear_iters=100,\n",
    "                 wild_thresh=0):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.iters_since_relinear = iters_since_relinear\n",
    "        self.min_linear_iters = min_linear_iters\n",
    "        self.nonlinear_factors = nonlinear_factors\n",
    "        self.eta_damping = eta_damping\n",
    "        self.wild_thresh = wild_thresh\n",
    "\n",
    "        #self.energy_history = []\n",
    "        #self.error_history = []\n",
    "        #self.nmsgs_history = []\n",
    "        #self.mus = []\n",
    "\n",
    "\n",
    "    def vloop(self):\n",
    "        \"\"\"\n",
    "        Simplified V-cycle:\n",
    "        1) bottom-up: rebuild and iterate once for base / super / abs in order\n",
    "        2) top-down: propagate mu from super -> base\n",
    "        3) refresh gbp_result on each layer for UI use\n",
    "        \"\"\"\n",
    "\n",
    "        layers = self.layers\n",
    "\n",
    "        # ---- bottom-up ----\n",
    "        #if layers and \"graph\" in layers[0]:\n",
    "        #    layers[0][\"graph\"].synchronous_iteration()\n",
    "            \n",
    "        for i in range(1, len(layers)):\n",
    "            name = layers[i][\"name\"]\n",
    "\n",
    "            if name.startswith(\"super1\"):\n",
    "                # Update super using the previous base graph's new linearization points\n",
    "                pass\n",
    "\n",
    "            elif name.startswith(\"super\"):\n",
    "                # Update super using the previous layer's graph\n",
    "                layers[i][\"graph\"] = build_super_graph(layers[:i+1], eta_damping=self.eta_damping)\n",
    "\n",
    "            elif name.startswith(\"abs\"):\n",
    "                # Rebuild abs using the previous super\n",
    "                abs_graph, Bs, ks, k2s = build_abs_graph(layers[:i+1], eta_damping=self.eta_damping)\n",
    "                layers[i][\"graph\"] = abs_graph\n",
    "                layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "            # After build, one iteration per layer\n",
    "            if \"graph\" in layers[i]:\n",
    "                layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # ---- top-down (pass mu) ----\n",
    "        for i in range(len(layers) - 1, 0, -1):\n",
    "            # After one iterations per layer, reproject\n",
    "            if \"graph\" in layers[i]:\n",
    "                layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "            # this is very important, but dont know why yet\n",
    "            # so abs layer need more iterations\n",
    "            #if name.startswith(\"abs\"):\n",
    "                #layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "            name = layers[i][\"name\"]\n",
    "            if name.startswith(\"super\"):\n",
    "                # Split super.mu back to base/abs\n",
    "                top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "            elif name.startswith(\"abs\"):\n",
    "                # Project abs.mu back to super\n",
    "                top_down_modify_super_graph(layers[:i+1])\n",
    "\n",
    "        # ---- refresh gbp_result for UI ----\n",
    "        #refresh_gbp_results(layers)\n",
    "        return layers\n",
    "vg = VGraph(layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def energy_map(graph, include_priors: bool = True, include_factors: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    It is actually the sum of squares of distances.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "\n",
    "    for v in graph.var_nodes[:graph.n_var_nodes]:\n",
    "        gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "        r = np.asarray(v.belief.mu()[0:2], dtype=float) - gt\n",
    "        total += 0.5 * float(r.T @ r)\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c65d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7930430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def graph_diameter(nodes, edges, ignore_prior=True):\n",
    "    \"\"\"\n",
    "    Computes the (unweighted) graph diameter: max shortest-path distance\n",
    "    between all *variable* nodes.\n",
    "\n",
    "    - Treats edges as undirected (typical for pose-graph connectivity).\n",
    "    - By default ignores the special 'prior' node/edges when computing diameter.\n",
    "\n",
    "    Returns:\n",
    "        diameter (int): maximum shortest-path length among variable nodes\n",
    "                        (0 if only one variable node).\n",
    "    Raises:\n",
    "        ValueError if the variable-node subgraph is disconnected.\n",
    "    \"\"\"\n",
    "    # ---- collect variable node ids (strings like \"0\",\"1\",...) ----\n",
    "    var_ids = []\n",
    "    for n in nodes:\n",
    "        nid = n.get(\"data\", {}).get(\"id\", None)\n",
    "        if nid is None:\n",
    "            continue\n",
    "        if ignore_prior and nid == \"prior\":\n",
    "            continue\n",
    "        var_ids.append(str(nid))\n",
    "\n",
    "    var_set = set(var_ids)\n",
    "\n",
    "    # ---- build adjacency (undirected) ----\n",
    "    adj = {vid: set() for vid in var_ids}\n",
    "    for e in edges:\n",
    "        d = e.get(\"data\", {})\n",
    "        u = str(d.get(\"source\"))\n",
    "        v = str(d.get(\"target\"))\n",
    "        if ignore_prior:\n",
    "            if u not in var_set or v not in var_set:\n",
    "                continue\n",
    "        else:\n",
    "            # If not ignoring prior, include any nodes mentioned by edges.\n",
    "            # Ensure they exist in adj.\n",
    "            if u not in adj: adj[u] = set()\n",
    "            if v not in adj: adj[v] = set()\n",
    "\n",
    "        adj[u].add(v)\n",
    "        adj[v].add(u)\n",
    "\n",
    "    # ---- BFS helper ----\n",
    "    def bfs_maxdist(start):\n",
    "        dist = {start: 0}\n",
    "        q = deque([start])\n",
    "        while q:\n",
    "            x = q.popleft()\n",
    "            for y in adj.get(x, ()):\n",
    "                if y not in dist:\n",
    "                    dist[y] = dist[x] + 1\n",
    "                    q.append(y)\n",
    "        return dist\n",
    "\n",
    "    # ---- compute diameter via BFS from each variable node ----\n",
    "    diameter = 0\n",
    "    for s in var_ids:\n",
    "        dist = bfs_maxdist(s)\n",
    "        # If disconnected, some variable nodes are unreachable from s\n",
    "        if len(dist) != len(var_ids):\n",
    "            raise ValueError(\"Variable-node graph is disconnected; diameter is undefined/infinite.\")\n",
    "        diameter = max(diameter, max(dist.values(), default=0))\n",
    "\n",
    "    return diameter\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# nodes, edges = make_slam_like_graph(...)\n",
    "# d = graph_diameter(nodes, edges)   # ignores 'prior' by default\n",
    "# print(\"diameter =\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edefd26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "0 101089344.5794572\n",
      "0 99117762.9777421\n",
      "1 93472676.74641348\n",
      "2 81216946.92221421\n",
      "3 60764519.81885415\n",
      "4 44846122.97234041\n",
      "5 37681013.62228483\n",
      "6 34425457.01014782\n",
      "7 31314304.65222162\n",
      "8 28258940.91830768\n",
      "9 25613438.511047196\n",
      "10 23416615.787896287\n",
      "11 21187442.170679428\n",
      "12 18912041.63080665\n",
      "13 16593571.60966635\n",
      "14 14328361.945581308\n",
      "15 12135673.713625671\n",
      "16 9538801.845862439\n",
      "17 7012843.175017026\n",
      "18 4819820.326769719\n",
      "19 3501050.3206544677\n",
      "20 2616284.895362895\n",
      "21 1738106.951841042\n",
      "22 866153.5700093742\n",
      "23 2337.9247868837806\n"
     ]
    }
   ],
   "source": [
    "#N = 128\n",
    "N = 512\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "\n",
    "\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "\n",
    "\n",
    "# construct GBP graph\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "gbp_graph.num_undamped_iters = 0\n",
    "gbp_graph.min_linear_iters = 2000\n",
    "opts=[{\"label\":\"base\",\"value\":\"base\"}]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "total = 0\n",
    "import time \n",
    "xxxx = time.time()\n",
    "eta, lam = layers[0][\"graph\"].joint_distribution_inf()\n",
    "print(\"time:\", time.time() - xxxx)\n",
    "xxxx = time.time()\n",
    "sigma = np.linalg.inv(lam)\n",
    "mu = sigma @ eta\n",
    "print(\"time:\", time.time() - xxxx)\n",
    "a = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\n",
    "for i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\n",
    "    gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "    r = np.asarray(a[i][0:2], dtype=float) - gt\n",
    "    total += 0.5 * float(r.T @ r)\n",
    "print(total)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(graph_diameter(layers[0][\"nodes\"], layers[0][\"edges\"], ignore_prior=True))\n",
    "\n",
    "print(0, energy_map(gbp_graph, include_priors=True, include_factors=True))\n",
    "for i in range(24):\n",
    "    gbp_graph.synchronous_iteration() \n",
    "    print(i, energy_map(gbp_graph, include_priors=True, include_factors=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 99117762.9777421\n",
      "1 93472676.74641348\n",
      "2 81216946.92221421\n",
      "3 60764519.81885415\n",
      "4 44846122.97234041\n",
      "5 37681013.62228483\n",
      "6 34425457.01014782\n",
      "7 31314304.65222162\n",
      "8 28258940.91830768\n",
      "9 25613438.511047196\n",
      "10 23416615.787896287\n",
      "11 21187442.170679428\n",
      "12 18912041.63080665\n",
      "13 16593571.60966635\n",
      "14 14328361.945581308\n",
      "15 12135673.713625671\n",
      "16 9538801.845862439\n",
      "17 7012843.175017026\n",
      "18 4819820.326769719\n",
      "19 3501050.3206544677\n",
      "20 2616284.895362895\n",
      "21 1738106.951841042\n",
      "22 866153.5700093742\n",
      "23 2337.9247868837806\n",
      "2337.9247868837806\n",
      "Iter 001 | Energy = 2156.816105\n"
     ]
    }
   ],
   "source": [
    "N = 512\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "\n",
    "\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "\n",
    "\n",
    "\n",
    "# Create GBP graph\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "gbp_graph.num_undamped_iters = 0\n",
    "gbp_graph.min_linear_iters = 2000\n",
    "opts=[{\"label\":\"base\",\"value\":\"base\"}]\n",
    "\n",
    "\n",
    "kk = 8\n",
    "k_next = 1\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(kk or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure base graph has run at least once\n",
    "for i in range(24):\n",
    "    layers[-1][\"graph\"].synchronous_iteration() \n",
    "    print(i, energy_map(layers[-1][\"graph\"], include_priors=True, include_factors=True))\n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "\n",
    "abs_layer_idx = 2\n",
    "k = 1\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(\n",
    "    layers, r_reduced=2)\n",
    "\n",
    "\n",
    "\n",
    "k_next = 2\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(kk or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure super graph has run at least once\n",
    "#layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "\n",
    "abs_layer_idx = 4\n",
    "k = 2\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(\n",
    "    layers, r_reduced=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k_next = 3\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(kk or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure super graph has run at least once\n",
    "#layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "abs_layer_idx = 6\n",
    "k = 3\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(layers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "k_next = 4\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(kk or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "\n",
    "abs_layer_idx = 8\n",
    "k = 4\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(layers)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i in range(2000):\n",
    "    layers[1][\"graph\"].synchronous_iteration()\n",
    "    layers[0][\"graph\"].synchronous_iteration()\n",
    "    layers[2][\"graph\"].synchronous_iteration()\n",
    "    #top_down_modify_super_graph(layers[:])\n",
    "    #top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {i+1:03d} | Energy = {energy:.6f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True))\n",
    "vg = VGraph(layers)\n",
    "energy_prev = 0\n",
    "counter = 0\n",
    "for _ in range(1):\n",
    "    vg.layers = layers\n",
    "    vg.r_reduced=2\n",
    "    vg.eta_damping = 0\n",
    "    vg.layers = vg.vloop()\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    if np.abs(energy_prev-energy) < 1e-5:\n",
    "        counter += 1\n",
    "        if counter >= 2:\n",
    "            break\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")\n",
    "\n",
    "    #energy_prev = energy\n",
    "refresh_gbp_results(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(layers[0][\"graph\"].n_var_nodes):\n",
    "    print(layers[0][\"graph\"].var_nodes[i].belief.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1feb8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  33.77824538 -319.19195985]\n",
      "[  33.4303663  -316.11302393]\n",
      "[[7.21656636e-11 0.00000000e+00]\n",
      " [0.00000000e+00 7.21656636e-11]]\n",
      "[[0.66495268 0.        ]\n",
      " [0.         0.66495268]]\n"
     ]
    }
   ],
   "source": [
    "print(layers[0][\"graph\"].var_nodes[100].mu)\n",
    "print(layers[0][\"graph\"].var_nodes[100].GT)\n",
    "print(layers[0][\"graph\"].var_nodes[100].prior.lam)\n",
    "print(layers[0][\"graph\"].var_nodes[2].belief.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a7d40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  33.77824538, -319.19195985])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0][\"graph\"].var_nodes[100].prior.mu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46dbfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2172.627927410766\n"
     ]
    }
   ],
   "source": [
    "layers[0][\"graph\"].synchronous_iteration()\n",
    "print(energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2466fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,f in enumerate(layers[0][\"graph\"].factors):\n",
    "    f.adj_beliefs = []\n",
    "    f.messages = []\n",
    "\n",
    "    for adj_var_node in f.adj_var_nodes:\n",
    "        f.adj_beliefs.append(NdimGaussian(adj_var_node.dofs))\n",
    "        f.messages.append(NdimGaussian(adj_var_node.dofs))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vg.layers[0][\"graph\"].synchronous_iteration()\n",
    "energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    layers[2][\"graph\"].synchronous_iteration() \n",
    "\n",
    "top_down_modify_super_graph(layers[:3])\n",
    "top_down_modify_base_and_abs_graph(layers[:2])\n",
    "\n",
    "energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58741077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# -------------------\n",
    "# 构造一个 toy example\n",
    "# -------------------\n",
    "B, m, n = 5000, 2, 4   # B = block 数量，m = 每块观测维度，n = 变量维度\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "J_list = [rng.normal(size=(m, n)) for _ in range(B)]\n",
    "L_list = [rng.normal(size=(m, m)) for _ in range(B)]\n",
    "# 让 L_i 正定一点\n",
    "L_list = [Li.T @ Li + 1e-3 * np.eye(m) for Li in L_list]\n",
    "z_list = [rng.normal(size=(m,)) for _ in range(B)]\n",
    "h_list = [rng.normal(size=(m,)) for _ in range(B)]\n",
    "x = rng.normal(size=(n,))\n",
    "\n",
    "# -------------------\n",
    "# 方案 A：你现在的 for-loop 小矩阵累加\n",
    "# -------------------\n",
    "def compute_loop(J_list, L_list, z_list, h_list, x):\n",
    "    n = x.shape[0]\n",
    "    lambda_factor = np.zeros((n, n))\n",
    "    eta_factor = np.zeros((n,))\n",
    "    for Ji, Li, zi, hi in zip(J_list, L_list, z_list, h_list):\n",
    "        ri = Ji @ x + zi - hi              # (m,)\n",
    "        lambda_factor += Ji.T @ Li @ Ji    # (n,n)\n",
    "        eta_factor += Ji.T @ (Li @ ri)     # (n,)\n",
    "    return lambda_factor, eta_factor\n",
    "\n",
    "# -------------------\n",
    "# 方案 B：batch 大矩阵版本（假设已经 stack 好）\n",
    "# -------------------\n",
    "def compute_batch(J, L, z, h, x):\n",
    "    \"\"\"\n",
    "    J: (B, m, n)\n",
    "    L: (B, m, m)\n",
    "    z: (B, m)\n",
    "    h: (B, m)\n",
    "    x: (n,)\n",
    "    \"\"\"\n",
    "    B_, m_, n_ = J.shape\n",
    "\n",
    "    # r_i = J_i x + z_i - h_i   -> (B, m)\n",
    "    r = J @ x                   # (B, m)\n",
    "    r = r + z - h               # (B, m)\n",
    "\n",
    "    # M_i = L_i J_i -> (B, m, n)\n",
    "    M = np.matmul(L, J)         # (B, m, n)\n",
    "\n",
    "    # Λ = sum_i J_i^T L_i J_i\n",
    "    # lam_blocks[b] = J_b^T L_b J_b\n",
    "    lam_blocks = np.einsum('bmi,bmj->bij', J, M)   # (B, n, n)\n",
    "    lambda_factor = lam_blocks.sum(axis=0)         # (n, n)\n",
    "\n",
    "    # η = sum_i J_i^T L_i r_i\n",
    "    Mr = np.einsum('bij,bi->bj', L, r)             # (B, m)  = L_i r_i\n",
    "    eta_blocks = np.einsum('bmi,bm->bi', J, Mr)    # (B, n)\n",
    "    eta_factor = eta_blocks.sum(axis=0)            # (n,)\n",
    "\n",
    "    return lambda_factor, eta_factor\n",
    "\n",
    "# 预先 stack 好（“warm” 情况）\n",
    "J = np.stack(J_list, axis=0)   # (B, m, n)\n",
    "L = np.stack(L_list, axis=0)   # (B, m, m)\n",
    "z = np.stack(z_list, axis=0)   # (B, m)\n",
    "h = np.stack(h_list, axis=0)   # (B, m)\n",
    "\n",
    "# -------------------\n",
    "# 方案 B'：把 np.stack 的时间也算进去（“cold”）\n",
    "# -------------------\n",
    "def compute_batch_cold(J_list, L_list, z_list, h_list, x):\n",
    "    J = np.stack(J_list, axis=0)\n",
    "    L = np.stack(L_list, axis=0)\n",
    "    z = np.stack(z_list, axis=0)\n",
    "    h = np.stack(h_list, axis=0)\n",
    "    return compute_batch(J, L, z, h, x)\n",
    "\n",
    "# -------------------\n",
    "# 校验数值一致\n",
    "# -------------------\n",
    "lam_A, eta_A = compute_loop(J_list, L_list, z_list, h_list, x)\n",
    "lam_B, eta_B = compute_batch(J, L, z, h, x)\n",
    "assert np.allclose(lam_A, lam_B, atol=1e-8)\n",
    "assert np.allclose(eta_A, eta_B, atol=1e-8)\n",
    "\n",
    "# -------------------\n",
    "# 简单基准测试\n",
    "# -------------------\n",
    "def bench(fn, iters=200):\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "print(\"loop       (for 每块小矩阵):\",\n",
    "      bench(lambda: compute_loop(J_list, L_list, z_list, h_list, x)) * 1e3, \"ms\")\n",
    "\n",
    "print(\"batch warm (提前 stack 好):\",\n",
    "      bench(lambda: compute_batch(J, L, z, h, x)) * 1e3, \"ms\")\n",
    "\n",
    "print(\"batch cold (每次都 np.stack):\",\n",
    "      bench(lambda: compute_batch_cold(J_list, L_list, z_list, h_list, x)) * 1e3, \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "nodes = [256, 512, 1024, 2048, 4096, 8196]\n",
    "\n",
    "iters_multi = [105, 77, 150, 100, 185, 304]\n",
    "iters_orig  = [868, 1669, 7566, 3434, 3449, 30000]  # lower bound for plot\n",
    "\n",
    "time_multi = [4.9899821281433105, 6.832485914230347, 28.867820739746094,\n",
    "              35.16724491119385, 156.85783433914185, 768.7591967582703]\n",
    "time_orig  = [10.840996980667114, 53.138646364212036, 593.8828203678131,\n",
    "              528.84259557724, 1184.7146832942963, 18000]  # lower bound for plot\n",
    "\n",
    "speedup_iters = [o/m for o, m in zip(iters_orig, iters_multi)]\n",
    "speedup_time  = [o/m for o, m in zip(time_orig, time_multi)]\n",
    "\n",
    "# Plot 1: iterations vs # of nodes\n",
    "plt.figure()\n",
    "plt.plot(nodes, iters_multi, marker='o', label='multi-level')\n",
    "plt.plot(nodes, iters_orig, marker='s', linestyle='--', label='original')\n",
    "\n",
    "# Label speedup above BLUE nodes (original curve)\n",
    "for x, y, s in zip(nodes, iters_orig, speedup_iters):\n",
    "    plt.text(x, y, f'{s:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('# of nodes')\n",
    "plt.ylabel('iterations')\n",
    "plt.title('Iterations vs # of Nodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log', base=2)\n",
    "\n",
    "# Plot 2: time vs # of nodes\n",
    "plt.figure()\n",
    "plt.plot(nodes, time_multi, marker='o', label='multi-level')\n",
    "plt.plot(nodes, time_orig, marker='s', linestyle='--', label='original')\n",
    "\n",
    "# Label speedup above BLUE nodes (original curve)\n",
    "for x, y, s in zip(nodes, time_orig, speedup_time):\n",
    "    plt.text(x, y, f'{s:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('# of nodes')\n",
    "plt.ylabel('time (s)')\n",
    "plt.title('Time vs # of Nodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "nodes = [128, 256, 512, 1024]\n",
    "\n",
    "# original\n",
    "iters_orig = [1171, 498, 1006, 4544]\n",
    "time_orig  = [15.3, 12.5, 52.0, 545.0]\n",
    "\n",
    "# wild-fire\n",
    "iters_wf = [666, 339, 598, 2123]\n",
    "time_wf  = [10.0, 9.7, 32.0, 298.0]\n",
    "\n",
    "# residual\n",
    "iters_res = [192, 147, 185, 460]\n",
    "time_res  = [8.4, 8.08, 19.5, 200.2]\n",
    "\n",
    "# NEW: original / residual speedup\n",
    "speedup_iters = [o / r for o, r in zip(iters_orig, iters_res)]\n",
    "speedup_time  = [o / r for o, r in zip(time_orig, time_res)]\n",
    "\n",
    "# Plot 1: iterations vs # of nodes\n",
    "plt.figure()\n",
    "plt.plot(nodes, iters_res, marker='^', linestyle='-',  label='residual')\n",
    "plt.plot(nodes, iters_wf,  marker='o', linestyle='-',  label='wild-fire')\n",
    "plt.plot(nodes, iters_orig,marker='s', linestyle='--', label='original')\n",
    "\n",
    "# 标注 original/residual 倍数（标在 original 节点上）\n",
    "for x, y, s in zip(nodes, iters_orig, speedup_iters):\n",
    "    plt.text(x, y, f'{s:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('# of nodes')\n",
    "plt.ylabel('iterations')\n",
    "plt.title('Iterations vs # of Nodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log', base=2)\n",
    "\n",
    "# Plot 2: time vs # of nodes\n",
    "plt.figure()\n",
    "plt.plot(nodes, time_res,  marker='^', linestyle='-',  label='residual')\n",
    "plt.plot(nodes, time_wf,   marker='o', linestyle='-',  label='wild-fire')\n",
    "plt.plot(nodes, time_orig, marker='s', linestyle='--', label='original')\n",
    "\n",
    "for x, y, s in zip(nodes, time_orig, speedup_time):\n",
    "    plt.text(x, y, f'{s:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('# of nodes')\n",
    "plt.ylabel('time (s)')\n",
    "plt.title('Time vs # of Nodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e42544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total iteration time: \", 0.1708+0.0386+0.0044+0.0015+0.0002+0.0001+0.0001+0.0018+0.0011+0.0145+0.0444+0.1793)\n",
    "print(\"super1: \", 0.1708+0.1793)\n",
    "print(\"ratio: \", 0.35009999999999997/0.4568)\n",
    "\n",
    "print(\"\")\n",
    "print(\"total building and backing time: \", 0.0774+0.1253+0.0925+0.4952+0.2424 + 0.0000+0.0003+0.0004+0.0081+0.0121+0.0548)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140348b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "aa = deepcopy(layers[1][\"graph\"])\n",
    "for i in range(1000):\n",
    "    aa.synchronous_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "for i in range(1000):\n",
    "    layers[-1][\"graph\"].synchronous_iteration()\n",
    "\n",
    "bb = deepcopy(layers[-1][\"graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21979a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[2][\"graph\"] = deepcopy(bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(\n",
    "    layers[:3], eta_damping=0, r_reduced=2)\n",
    "\n",
    "#layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "print(layers[abs_layer_idx][\"graph\"].var_nodes[0].mu)\n",
    "print(bb.var_nodes[0].mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[abs_layer_idx][\"graph\"].compute_all_messages()\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "print(layers[abs_layer_idx][\"graph\"].var_nodes[0].mu)\n",
    "bb.synchronous_iteration\n",
    "print(bb.var_nodes[0].mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_super_graph(layers[:3])\n",
    "top_down_modify_base_and_abs_graph(layers[:2])\n",
    "\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cffe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[abs_layer_idx][\"graph\"].var_nodes[0].Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.var_nodes[0].Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "abs_graph, Bs, ks, k2s = bottom_up_modify_abs_graph(layers[:], eta_damping=0)\n",
    "layers[2][\"graph\"] = abs_graph\n",
    "layers[2][\"Bs\"], layers[2][\"ks\"], layers[2][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "layers[2][\"graph\"].synchronous_iteration()\n",
    "layers[2][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_super_graph(layers[:3])\n",
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_base_and_abs_graph(layers[:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74759123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    for i in range(1, len(layers)):\n",
    "        name = layers[i][\"name\"]\n",
    "\n",
    "        if name.startswith(\"super1\"):\n",
    "            # Update super using the previous base graph's new linearization points\n",
    "            pass\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            layers[i][\"graph\"] = build_super_graph(layers[:i+1], eta_damping=0)\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Rebuild abs using the previous super\n",
    "            abs_graph, Bs, ks, k2s = bottom_up_modify_abs_graph(layers[:i+1], eta_damping=0)\n",
    "            layers[i][\"graph\"] = abs_graph\n",
    "            layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "        # After build, one iteration per layer\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "    # ---- top-down (pass mu) ----\n",
    "    for i in range(len(layers) - 1, 0, -1):\n",
    "        # After one iterations per layer, reproject\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # this is very important, but dont know why yet\n",
    "        # so abs layer need more iterations\n",
    "        #if name.startswith(\"abs\"):\n",
    "        #    layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "        name = layers[i][\"name\"]\n",
    "        if name.startswith(\"super\"):\n",
    "            # Split super.mu back to base/abs\n",
    "            top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Project abs.mu back to super\n",
    "            top_down_modify_super_graph(layers[:i+1])\n",
    "\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23715db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "for i in range(1):\n",
    "    layers[-1][\"graph\"].synchronous_iteration()\n",
    "    top_down_modify_super_graph(layers[:])\n",
    "    top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "    layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "    top_down_modify_super_graph(layers[:])\n",
    "    top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_down_modify_super_graph(layers[:])\n",
    "top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(layers[0][\"graph\"].var_nodes[0].adj_factors[1].adj_beliefs[0].lam, layers[0][\"graph\"].var_nodes[0].adj_factors[1].adj_beliefs[0].eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd50e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[0][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e157ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[0][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abc516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_energy(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2401-2324)/2324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "2402*(1.03313253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "# Create GBP graph\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(layers[0][\"nodes\"], layers[0][\"edges\"], k=2, layer_idx=1, tail_heavy=True)\n",
    "layers.append({\"name\":f\"super{1}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "layers[0][\"graph\"].synchronous_iteration()\n",
    "layers[1][\"graph\"] = build_super_graph(layers,eta_damping=0)\n",
    "supergraph = layers[-1][\"graph\"]\n",
    "\n",
    "\n",
    "total = 0\n",
    "a = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\n",
    "for i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\n",
    "    gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "    r = np.asarray(a[i][0:2], dtype=float) - gt\n",
    "    total += 0.5 * float(r.T @ r)\n",
    "print(total)\n",
    "\n",
    "abs_nodes, abs_edges = copy_to_abs(layers[1][\"nodes\"], layers[1][\"edges\"], 2)\n",
    "\n",
    "# Ensure super graph has run at least once\n",
    "#layers[1][\"graph\"].synchronous_iteration() \n",
    "r = 2\n",
    "layers.append({\"name\":f\"abs{1}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[2][\"graph\"], layers[2][\"Bs\"], layers[2][\"ks\"], layers[2][\"k2s\"] = build_abs_graph(layers, r_reduced=r)\n",
    "\n",
    "vg = VGraph(layers, eta_damping=0)\n",
    "for i in range(50):\n",
    "    vg.layers = layers\n",
    "    vg.layers = vg.vloop()\n",
    "    layers = vg.layers\n",
    "    refresh_gbp_results(layers)\n",
    "    energy = supergraph.energy_map(include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "\n",
    "@jit\n",
    "def matmul(x, y):\n",
    "    return x @ y\n",
    "\n",
    "x = jnp.ones((2000, 8000))\n",
    "y = jnp.ones((8000, 2000))\n",
    "\n",
    "# 预热（第一次会 JIT 编译）\n",
    "_ = matmul(x, y).block_until_ready()\n",
    "\n",
    "t0 = time.time()\n",
    "out = matmul(x, y).block_until_ready()\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Matmul time:\", t1 - t0, \"seconds\")\n",
    "print(\"jax devices:\", jax.devices())\n",
    "print(\"Result device:\", out.device)   # ✅ 这里不要加 ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7172e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(100):\n",
    "    out = matmul(x, y).block_until_ready()\n",
    "t1 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.ones((2000, 8000))\n",
    "y = np.ones((8000, 2000))\n",
    "t0 = time.time()\n",
    "for i in range(100):\n",
    "    out = x@y\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import nvidia.cusparse  # 先把 wheel 里的 cusparse 模块 import 进来\n",
    "\n",
    "# 找到 pip 安装的 cuSPARSE 动态库所在目录\n",
    "cusparse_lib_dir = str(pathlib.Path(nvidia.cusparse.__file__).parent)\n",
    "print(\"cusparse_lib_dir =\", cusparse_lib_dir)\n",
    "\n",
    "# 把 LD_LIBRARY_PATH 显式设置成这个目录（再附带上原来的其它目录）\n",
    "old = os.environ.get(\"LD_LIBRARY_PATH\") or \"\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = cusparse_lib_dir + (\":\" + old if old else \"\")\n",
    "print(\"LD_LIBRARY_PATH =\", os.environ[\"LD_LIBRARY_PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a395886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((6.988972036403319-6.911181)/6.988972036403319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = layers[1][\"graph\"].energy_map(include_priors=True, include_factors=True)\n",
    "print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[2][\"graph\"].var_nodes[0].mu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
