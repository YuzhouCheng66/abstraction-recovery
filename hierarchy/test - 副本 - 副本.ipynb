{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f39044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_two_cliques_graph(prior_sigma=10.0, odom_sigma=10.0, rng=None):\n",
    "    \"\"\"\n",
    "    构建一个确定性的小图：\n",
    "      节点 0..3 组成左 clique，全连接 + prior\n",
    "      节点 4..6 组成右 clique，全连接 + prior\n",
    "      跨边 (3,4)\n",
    "    返回: nodes, edges, factor_graph\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    nodes, edges = [], []\n",
    "\n",
    "    # 固定 node 坐标，便于可视化\n",
    "    positions = {\n",
    "        0: (0, 0),\n",
    "        1: (0, 10),\n",
    "        2: (0, 20),\n",
    "        3: (0, 30),\n",
    "        4: (50, 0),\n",
    "        5: (50, 10),\n",
    "        6: (50, 20),\n",
    "    }\n",
    "\n",
    "    # 添加节点\n",
    "    for i in range(7):\n",
    "        px, py = positions[i]\n",
    "        nodes.append({\n",
    "            \"data\": {\"id\": f\"{i}\", \"layer\": 0, \"dim\": 2},\n",
    "            \"position\": {\"x\": float(px), \"y\": float(py)}\n",
    "        })\n",
    "\n",
    "    # clique: 0-3\n",
    "    for i in range(4):\n",
    "        for j in range(i+1, 4):\n",
    "            edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{j}\"}})\n",
    "    # clique: 4-6\n",
    "    for i in range(4, 7):\n",
    "        for j in range(i+1, 7):\n",
    "            edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{j}\"}})\n",
    "    # 跨边 (3,4)\n",
    "    edges.append({\"data\": {\"source\": \"3\", \"target\": \"4\"}})\n",
    "\n",
    "    # 每个节点加 prior\n",
    "    for i in range(7):\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": \"prior\"}})\n",
    "\n",
    "    # ----------------- 构建 FactorGraph -----------------\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=0)\n",
    "    var_nodes = []\n",
    "    I2 = np.eye(2)\n",
    "    prior_noises, odom_noises = {}, {}\n",
    "\n",
    "    # 为边生成噪声\n",
    "    for e in edges:\n",
    "        s, t = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "        if t == \"prior\":\n",
    "            prior_noises[int(s)] = rng.normal(0.0, prior_sigma, size=2)\n",
    "        else:\n",
    "            odom_noises[(int(s), int(t))] = rng.normal(0.0, odom_sigma, size=2)\n",
    "\n",
    "    # variable nodes\n",
    "    for i in range(7):\n",
    "        v = VariableNode(i, dofs=2)\n",
    "        v.GT = np.array(positions[i], dtype=float)\n",
    "        v.prior.lam = 1e-10 * I2\n",
    "        v.prior.eta = np.zeros(2)\n",
    "        var_nodes.append(v)\n",
    "    fg.var_nodes = var_nodes\n",
    "    fg.n_var_nodes = len(var_nodes)\n",
    "\n",
    "    # 定义测量函数\n",
    "    def meas_fn_unary(x, *a): return x\n",
    "    def jac_fn_unary(x, *a): return np.eye(2)\n",
    "    def meas_fn(xy, *a): return xy[2:] - xy[:2]\n",
    "    def jac_fn(xy, *a): return np.array([[-1,0,1,0],[0,-1,0,1]], dtype=float)\n",
    "\n",
    "    factors = []\n",
    "    fid = 0\n",
    "    for e in edges:\n",
    "        s, t = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "        if t == \"prior\":\n",
    "            i = int(s)\n",
    "            vi = var_nodes[i]\n",
    "            z = vi.GT + prior_noises[i]\n",
    "            z_lambda = np.eye(len(z))/ (prior_sigma**2)\n",
    "            f = Factor(fid, [vi], z, z_lambda, meas_fn_unary, jac_fn_unary)\n",
    "            f.type = \"prior\"\n",
    "            f.compute_factor(linpoint=z, update_self=True)\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            fid += 1\n",
    "        else:\n",
    "            i, j = int(s), int(t)\n",
    "            vi, vj = var_nodes[i], var_nodes[j]\n",
    "            z = (vj.GT - vi.GT) + odom_noises[(i, j)]\n",
    "            z_lambda = np.eye(len(z))/ (odom_sigma**2)\n",
    "            f = Factor(fid, [vi, vj], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.type = \"odom\"\n",
    "            lin = np.r_[vi.GT, vj.GT]\n",
    "            f.compute_factor(linpoint=lin, update_self=True)\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            vj.adj_factors.append(f)\n",
    "            fid += 1\n",
    "\n",
    "    fg.factors = factors\n",
    "    fg.n_factor_nodes = len(factors)\n",
    "    return nodes, edges, fg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6f6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Defines classes for variable nodes, factor nodes and edges and factor graph.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.linalg\n",
    "\n",
    "from utils.gaussian import NdimGaussian\n",
    "from utils.distances import bhattacharyya, mahalanobis\n",
    "\n",
    "#from amg import classes as amg_cls\n",
    "#from amg import functions as amg_fnc\n",
    "\n",
    "class FactorGraph:\n",
    "    def __init__(self,\n",
    "                 nonlinear_factors=True,\n",
    "                 eta_damping=0.0,\n",
    "                 beta=None,\n",
    "                 num_undamped_iters=None,\n",
    "                 min_linear_iters=None,\n",
    "                 wild_thresh=0):\n",
    "\n",
    "        self.var_nodes = []\n",
    "        self.factors = []\n",
    "\n",
    "        self.n_var_nodes = 0\n",
    "        self.n_factor_nodes = 0\n",
    "        self.n_edges = 0\n",
    "        self.n_msgs = 0\n",
    "\n",
    "        self.nonlinear_factors = nonlinear_factors\n",
    "\n",
    "        self.eta_damping = eta_damping\n",
    "\n",
    "        self.Q = []\n",
    "        self.b_wild = False\n",
    "        self.wild_thresh = wild_thresh\n",
    "        self.multigrid_vars = [[]]\n",
    "        self.multigrid_factors = [[]]\n",
    "        self.multigrid = False\n",
    "        self.conv_width = 1\n",
    "        self.conv_stride = 1\n",
    "\n",
    "        self.energy_history = []\n",
    "        self.error_history = []\n",
    "        self.nmsgs_history = []\n",
    "        self.mus = []\n",
    "\n",
    "        if nonlinear_factors:\n",
    "            # For linearising nonlinear measurement factors.\n",
    "            self.beta = beta  # Threshold change in mean of adjacent beliefs for relinearisation.\n",
    "            self.num_undamped_iters = num_undamped_iters  # Number of undamped iterations after relinearisation before damping is set to 0.4\n",
    "            self.min_linear_iters = min_linear_iters  # Minimum number of linear iterations before a factor is allowed to realinearise.\n",
    "\n",
    "    def energy(self, vars=None):\n",
    "        \"\"\"\n",
    "            Computes the sum of all of the squared errors in the graph using the appropriate local loss function.\n",
    "        \"\"\"\n",
    "        # if slice_e is None:\n",
    "        #     slice_e = slice(len(self.factors))\n",
    "        # energy = 0\n",
    "        # for factor in self.factors[slice_e]:\n",
    "        #     # Variance of Gaussian noise at each factor is weighting of each term in squared loss.\n",
    "        #     energy += 0.5 * np.linalg.norm(factor.compute_residual()) ** 2\n",
    "        # return energy\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes\n",
    "        energy = 0\n",
    "        for var in vars:\n",
    "            if var.type != \"multigrid\":\n",
    "                # Variance of Gaussian noise at each factor is weighting of each term in squared loss.\n",
    "                energy += 0.5 * np.linalg.norm(var.residual) ** 2\n",
    "        return energy\n",
    "    \n",
    "    def energy_map(self, include_priors: bool = True, include_factors: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        实际上是距离平方和\n",
    "        \"\"\"\n",
    "        total = 0.0\n",
    "\n",
    "        for v in self.var_nodes[:self.n_var_nodes]:\n",
    "            gt = np.asarray(v.GT, dtype=float)\n",
    "            r = np.asarray(v.mu, dtype=float) - gt\n",
    "            total += 0.5 * float(r.T @ r)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def compute_all_messages(self, factors=None, level=None, local_relin=True):\n",
    "        if factors is None:\n",
    "            factors = self.factors[:self.n_factor_nodes]\n",
    "        if level is not None:\n",
    "            factors = self.multigrid_factors[level]\n",
    "        for count, factor in enumerate(factors):\n",
    "            if factor.active:\n",
    "                # If relinearisation is local then damping is also set locally per factor.\n",
    "                if self.nonlinear_factors and local_relin:\n",
    "                    if factor.iters_since_relin == self.num_undamped_iters:\n",
    "                        factor.eta_damping = self.eta_damping\n",
    "                    factor.compute_messages(factor.eta_damping)\n",
    "                else:\n",
    "                    factor.compute_messages(self.eta_damping)\n",
    "                    self.n_msgs += 2\n",
    "\n",
    "    def compute_all_smoothing_messages(self, factors=None, level=None, local_relin=True):\n",
    "        if factors is None:\n",
    "            factors = self.factors[:self.n_factor_nodes]\n",
    "        if level is not None:\n",
    "            factors = self.multigrid_factors[level]\n",
    "        for count, factor in enumerate(factors):\n",
    "            factor.smoothing_compute_messages(self.eta_damping)\n",
    "\n",
    "    def update_all_beliefs(self, vars=None, level=None, smoothing=False):\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "\n",
    "        for var in vars:\n",
    "            if var.active:\n",
    "                if smoothing:\n",
    "                    var.update_smooth_belief()\n",
    "                else:\n",
    "                    var.update_belief()\n",
    "\n",
    "\n",
    "    def update_all_residuals(self, vars=None, level=None, smoothing=False):\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "\n",
    "        for var in vars:\n",
    "            if var.active:\n",
    "                res = var.compute_residual()\n",
    "\n",
    "    def restrict_all_residuals(self, vars=None, level=None, smoothing=False):\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "\n",
    "        for var in vars:\n",
    "            if var.active:\n",
    "                var.multigrid.send_restricted_residual()\n",
    "\n",
    "    def update_all_residual_etas(self, vars=None, level=None, smoothing=False):\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "\n",
    "        for var in vars:\n",
    "            if var.active:\n",
    "                if var.type[0:5] == \"multi\":\n",
    "                    for i_var in var.multigrid.interpolation_vars:\n",
    "                        i_var.compute_residual()\n",
    "                        i_var.multigrid.send_restricted_residual()\n",
    "                    var.multigrid.update_eta()\n",
    "                else:\n",
    "                    print(\"You just tried to update the eta on a base variable... you should probably \\\n",
    "                        check something because this ain't it!\")\n",
    "                \n",
    "    def prolongate_corrections(self, vars=None, level=None, smoothing=False):\n",
    "        if vars is None:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "\n",
    "        for var in vars:\n",
    "            if var.active:\n",
    "                if var.type[0:5] == \"multi\":\n",
    "                    var.multigrid.send_corrections()\n",
    "                else:\n",
    "                    print(\"You just tried to prolongate using a base variable... you should probably \\\n",
    "                        check something because this ain't it!\")\n",
    "\n",
    "    def compute_all_factors(self, factors=None, level=None):\n",
    "        if factors is None:\n",
    "            factors = self.factors[:self.n_factor_nodes]\n",
    "        if level is not None:\n",
    "            factors = self.multigrid_factors[level]\n",
    "        for count, factor in enumerate(factors):\n",
    "            factor.compute_factor()\n",
    "\n",
    "    def relinearise_factors(self):\n",
    "        \"\"\"\n",
    "            Compute the factor distribution for all factors for which the local belief mean has deviated a distance\n",
    "            greater than beta from the current linearisation point.\n",
    "            Relinearisation is only allowed at a maximum frequency of once every min_linear_iters iterations.\n",
    "        \"\"\"\n",
    "        if self.nonlinear_factors:\n",
    "            for factor in self.factors:\n",
    "                adj_belief_means = np.array([])\n",
    "                for belief in factor.adj_beliefs:\n",
    "                    adj_belief_means = np.concatenate((adj_belief_means, 1/np.diagonal(belief.lam) * belief.eta))\n",
    "                if np.linalg.norm(factor.linpoint - adj_belief_means) > self.beta and factor.iters_since_relin >= self.min_linear_iters:\n",
    "                    factor.compute_factor(linpoint=adj_belief_means)\n",
    "                    factor.iters_since_relin = 0\n",
    "                    factor.eta_damping = 0.0\n",
    "                else:\n",
    "                    factor.iters_since_relin += 1\n",
    "\n",
    "    def robustify_all_factors(self):\n",
    "        for factor in self.factors[:self.n_factor_nodes]:\n",
    "            factor.robustify_loss()\n",
    "\n",
    "    def synchronous_iteration(self, factors=None, level=None, local_relin=True, robustify=False):\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "            factors = self.multigrid_factors[level]\n",
    "        else:\n",
    "            vars = self.var_nodes[:self.n_var_nodes]\n",
    "            factors = self.factors[:self.n_factor_nodes]\n",
    "\n",
    "        if robustify:\n",
    "            self.robustify_all_factors(factors)\n",
    "        if self.nonlinear_factors and local_relin:\n",
    "            self.relinearise_factors(factors)\n",
    "\n",
    "        self.compute_all_messages(factors, local_relin=local_relin)\n",
    "        time.sleep(1e-9)\n",
    "        self.update_all_beliefs(vars)\n",
    "\n",
    "    def synchronous_smooth(self, level=None, local_relin=True, robustify=False):\n",
    "        if level is not None:\n",
    "            vars = self.multigrid_vars[level]\n",
    "            factors = self.multigrid_factors[level]\n",
    "        else:\n",
    "            vars = self.var_nodes\n",
    "            factors = self.factors\n",
    "        if robustify:\n",
    "            self.robustify_all_factors()\n",
    "        if self.nonlinear_factors and local_relin:\n",
    "            self.relinearise_factors()\n",
    "        self.compute_all_smoothing_messages(local_relin=local_relin)\n",
    "        self.update_all_beliefs(smoothing=True)\n",
    "            \n",
    "\n",
    "    def synchronous_loop(self, vis):\n",
    "        i=0\n",
    "        # self.get_means()\n",
    "        while not vis.reset_event.isSet(): #i<1000 and \n",
    "            while vis.pause_event.isSet() and not vis.reset_event.isSet():\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            self.visualisation_sync(vis)\n",
    "\n",
    "            self.synchronous_iteration()\n",
    "            self.update_all_residuals()\n",
    "\n",
    "            i+=1\n",
    "            av_dist = np.mean(np.linalg.norm(np.array([var.mu - var.GT for var in self.var_nodes if var.type != \"multigrid\"]),axis=1))\n",
    "            self.energy_history.append(self.energy())\n",
    "            self.error_history.append(av_dist)\n",
    "            self.nmsgs_history.append(self.n_msgs)\n",
    "            print(f'Iteration {i}  // Energy {self.energy_history[-1]:.6f} // ' \n",
    "                  f'Average error {av_dist:.4f} // msgs sent {self.n_msgs/1e6:.3f}x10^6')\n",
    "            \n",
    "            self.get_multigrid_stats()\n",
    "            for level in range(len(self.n_active)):\n",
    "                if self.n_active[level] > 0:\n",
    "                    print(f'Multigrid stats // level {level} // {(self.n_coarse[level]/(len(self.multigrid_vars[level])))*100:.2f}% coarse ' \\\n",
    "                        f'// {(self.n_active[level]/(len(self.multigrid_vars[level])))*100:.2f}% active ' \\\n",
    "                        f'// {len(self.multigrid_vars[level])} total ')\n",
    "            \n",
    "            print('')        \n",
    "            if vis.skip_event.isSet():\n",
    "                vis.pause_event.set()\n",
    "                vis.skip_event.clear()\n",
    "\n",
    "    def vcycle_loop(self, vis):\n",
    "        i=0\n",
    "        # self.get_means()\n",
    "\n",
    "        while  not vis.reset_event.isSet():\n",
    "            while vis.pause_event.isSet() and not vis.reset_event.isSet():\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            self.visualisation_sync(vis)\n",
    "\n",
    "            # if i == 10:  # Number of damped iterations before applying undamping\n",
    "            #     self.eta_damping = 0.0\n",
    "\n",
    "            for _ in range(1):\n",
    "                self.synchronous_iteration(level=0)\n",
    "                time.sleep(1e-9)\n",
    "            # self.update_all_residuals(level=0)\n",
    "            # self.restrict_all_residuals(level=0)\n",
    "            \n",
    "            for level in range(1,len(self.multigrid_vars)):  #range(1, 5):\n",
    "                self.update_all_residual_etas(level=level)\n",
    "                self.update_all_beliefs(level=level) \n",
    "                for _ in range(1):\n",
    "                    self.synchronous_iteration(level=level)\n",
    "                    time.sleep(1e-9)\n",
    "                self.update_all_residuals(level=level)\n",
    "                #self.restrict_all_residuals(level=level)                \n",
    "                \n",
    "            for level in range(len(self.multigrid_vars)-1,0,-1):  #range(4,0,-1):\n",
    "                for _ in range(1):\n",
    "                    self.synchronous_iteration(level=level)\n",
    "                    time.sleep(1e-9)\n",
    "                self.prolongate_corrections(level=level)\n",
    "\n",
    "            i+=1\n",
    "            av_dist = np.mean(np.linalg.norm(np.array([var.mu - var.GT for var in self.var_nodes if var.type != \"multigrid\"]),axis=1))\n",
    "            self.energy_history.append(self.energy())\n",
    "            self.error_history.append(av_dist)\n",
    "            self.nmsgs_history.append(self.n_msgs)\n",
    "            print(f'Iteration {i}  // Energy {self.energy_history[-1]:.6f} // ' \n",
    "                  f'Average error {av_dist:.4f} // msgs sent {self.n_msgs/1e6:.3f}x10^6')\n",
    "            \n",
    "            self.get_multigrid_stats()\n",
    "            for level in range(len(self.n_active)):\n",
    "                if self.n_active[level] > 0:\n",
    "                    print(f'Multigrid stats // level {level} // {(self.n_coarse[level]/(len(self.multigrid_vars[level])))*100:.2f}% coarse ' \\\n",
    "                        f'// {(self.n_active[level]/(len(self.multigrid_vars[level])))*100:.2f}% active ' \\\n",
    "                        f'// {len(self.multigrid_vars[level])} total ')\n",
    "            \n",
    "            print('')\n",
    "            if vis.skip_event.isSet():\n",
    "                vis.pause_event.set()\n",
    "                vis.skip_event.clear()\n",
    "\n",
    "    def wildfire_iteration(self, vis, local_relin=True, robustify=False):\n",
    "        breakout_count = 0\n",
    "        i = 0\n",
    "        while not vis.reset_event.isSet():\n",
    "            if vis.pause_event.isSet() and not vis.reset_event.isSet() or not self.Q:\n",
    "                time.sleep(0.1)\n",
    "                _ , new_factors = self.visualisation_sync(vis)\n",
    "                if new_factors:\n",
    "                    self.Q = new_factors\n",
    "            else:\n",
    "                _ , new_factors = self.visualisation_sync(vis)\n",
    "                if new_factors:\n",
    "                    self.Q[0:0] = new_factors\n",
    "\n",
    "                self.Q[0].compute_messages(self.eta_damping)\n",
    "                self.n_msgs += 2\n",
    "\n",
    "                for count, var in enumerate(self.Q[0].adj_var_nodes):\n",
    "                    var.update_belief()\n",
    "                    var.compute_residual()\n",
    "                    breakout_count += 1\n",
    "                    if any(self.Q[0].messages_dist[count] > self.wild_thresh):\n",
    "                        for f in var.adj_factors:\n",
    "                            if f not in self.Q:\n",
    "                                self.Q.append(f)\n",
    "\n",
    "                self.Q.pop(0)\n",
    "\n",
    "                if (self.n_msgs / 2) % len(self.factors) == 0:\n",
    "                    i += 1\n",
    "                    vis.read_event.clear()\n",
    "                    av_dist = np.mean(np.linalg.norm(np.array([var.mu - var.GT for var in self.var_nodes if var.type != \"multigrid\"]),axis=1))\n",
    "                    self.energy_history.append(self.energy())\n",
    "                    self.error_history.append(av_dist)\n",
    "                    self.nmsgs_history.append(self.n_msgs)\n",
    "                    print(f'Iteration {i}  // Energy {self.energy_history[-1]:.6f} // ' \n",
    "                        f'Average error {av_dist:.4f} // msgs sent {self.n_msgs/1e6:.3f}x10^6')\n",
    "                    \n",
    "                    self.get_multigrid_stats()\n",
    "                    for level in range(len(self.n_active)):\n",
    "                        if self.n_active[level] > 0:\n",
    "                            print(f'Multigrid stats // level {level} // {(self.n_coarse[level]/(len(self.multigrid_vars[level])))*100:.2f}% coarse ' \\\n",
    "                                f'// {(self.n_active[level]/(len(self.multigrid_vars[level])))*100:.2f}% active ' \\\n",
    "                                f'// {len(self.multigrid_vars[level])} total ')\n",
    "                    \n",
    "                    print('')\n",
    "\n",
    "                    breakout_count = 0\n",
    "\n",
    "                    if vis.skip_event.isSet():\n",
    "                        vis.pause_event.set()\n",
    "                        vis.skip_event.clear()\n",
    "\n",
    "    def visualisation_sync(self, vis):\n",
    "        if vis.n_factors > self.n_factor_nodes:\n",
    "            while vis.write_event.is_set():\n",
    "                time.sleep(0.001)\n",
    "            vis.read_event.set()\n",
    "            new_n_factors = vis.n_factors - self.n_factor_nodes\n",
    "            new_n_vars = vis.n_vars - self.n_var_nodes\n",
    "            \n",
    "            new_vars = self.var_nodes[slice(int(len(self.var_nodes) - new_n_vars), int(len(self.var_nodes)))]\n",
    "            self.multigrid_vars[0].extend(new_vars)\n",
    "            new_factors = self.factors[slice(int(len(self.factors) - new_n_factors), int(len(self.factors)))]\n",
    "            self.multigrid_factors[0].extend(new_factors)\n",
    "\n",
    "            vars_to_update = []\n",
    "\n",
    "            for factor in new_factors:\n",
    "                if vis.b_wild:\n",
    "                    factor.b_calc_mess_dist = True\n",
    "                for adj_var in factor.adj_var_nodes:\n",
    "                    adj_var.adj_factors.append(factor)\n",
    "                    if adj_var not in vars_to_update:\n",
    "                        vars_to_update.append(adj_var)\n",
    "                \n",
    "            for var in vars_to_update:\n",
    "                var.update_belief()\n",
    "            \n",
    "            for factor in new_factors:\n",
    "                factor.compute_factor()\n",
    "\n",
    "            self.n_var_nodes = int(vis.n_vars)\n",
    "            self.n_factor_nodes = int(vis.n_factors)\n",
    "\n",
    "            #if new_vars and vis.b_multi: # i.e. if there are new vars\n",
    "            #    amg_fnc.coarsen_graph(self, vars_to_update)\n",
    "\n",
    "\n",
    "            vis.n_vars = int(self.n_var_nodes)\n",
    "            vis.n_factors = int(self.n_factor_nodes)\n",
    "            \n",
    "            self.n_vars_active = int(len(self.var_nodes))\n",
    "            nodes_removed = 0\n",
    "\n",
    "            for var_id in range(self.n_vars_active):\n",
    "                if vis.var_nodes[var_id - nodes_removed].type == 'dead':\n",
    "                    self.multigrid_vars[vis.var_nodes[var_id - nodes_removed].multigrid.level].remove(vis.var_nodes[var_id - nodes_removed])\n",
    "                    vis.var_nodes.pop(var_id - nodes_removed)\n",
    "                    nodes_removed += 1\n",
    "\n",
    "            self.n_vars_active = int(len(self.var_nodes))\n",
    "            self.n_factors_active = int(len(self.factors))\n",
    "\n",
    "            factors_removed = 0\n",
    "            for factor_id in range(self.n_factors_active):\n",
    "                if vis.factors[factor_id - factors_removed].type == 'dead':\n",
    "                    self.multigrid_factors[int(vis.factors[factor_id - factors_removed].level)].remove(vis.factors[factor_id - factors_removed])\n",
    "                    vis.factors.pop(factor_id - factors_removed)\n",
    "                    factors_removed += 1\n",
    "\n",
    "            self.n_factors_active = int(len(self.factors))\n",
    "\n",
    "            # print(\"{:} node(s) removed : {:} factor(s) removed\".format(nodes_removed ,factors_removed))\n",
    "            # self.n_var_nodes = int(vis.n_vars)\n",
    "            # self.n_factor_nodes = int(vis.n_factors)\n",
    "\n",
    "            vis.read_event.clear()\n",
    "\n",
    "            return new_vars, new_factors\n",
    "                \n",
    "        else:\n",
    "\n",
    "            return None, None\n",
    "    \n",
    "\n",
    "    def joint_distribution_inf(self):\n",
    "        \"\"\"\n",
    "            Get the joint distribution over all variables in the information form\n",
    "            If nonlinear factors, it is taken at the current linearisation point.\n",
    "        \"\"\"\n",
    "\n",
    "        eta = np.array([])\n",
    "        lam = np.array([])\n",
    "        var_ix = np.zeros(len(self.var_nodes)).astype(int)\n",
    "        tot_n_vars = 0\n",
    "        for var_node in self.var_nodes:\n",
    "            var_ix[var_node.variableID] = int(tot_n_vars)\n",
    "            tot_n_vars += var_node.dofs\n",
    "            eta = np.concatenate((eta, var_node.prior.eta))\n",
    "            if var_node.variableID == 0:\n",
    "                lam = var_node.prior.lam\n",
    "            else:\n",
    "                lam = scipy.linalg.block_diag(lam, var_node.prior.lam)\n",
    "\n",
    "        for count, factor in enumerate(self.factors):\n",
    "            factor_ix = 0\n",
    "            for adj_var_node in factor.adj_var_nodes:\n",
    "                vID = adj_var_node.variableID\n",
    "                # Diagonal contribution of factor\n",
    "                eta[var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                    factor.factor.eta[factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                    factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                other_factor_ix = 0\n",
    "                for other_adj_var_node in factor.adj_var_nodes:\n",
    "                    if other_adj_var_node.variableID > adj_var_node.variableID:\n",
    "                        other_vID = other_adj_var_node.variableID\n",
    "                        # Off diagonal contributions of factor\n",
    "                        lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs] += \\\n",
    "                            factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, other_factor_ix:other_factor_ix + other_adj_var_node.dofs]\n",
    "                        lam[var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                            factor.factor.lam[other_factor_ix:other_factor_ix + other_adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                    other_factor_ix += other_adj_var_node.dofs\n",
    "                factor_ix += adj_var_node.dofs\n",
    "\n",
    "        return eta, lam\n",
    "\n",
    "\n",
    "    def joint_distribution_cov(self):\n",
    "        \"\"\"\n",
    "            Get the joint distribution over all variables in the covariance.\n",
    "            If nonlinear factors, it is taken at the current linearisation point.\n",
    "        \"\"\"\n",
    "        eta, lam = self.joint_distribution_inf()\n",
    "        sigma = np.linalg.inv(lam)\n",
    "        mu = sigma @ eta\n",
    "        return mu, sigma\n",
    "    \n",
    "\n",
    "    def get_multigrid_stats(self):\n",
    "        self.n_coarse = [0 for _ in self.multigrid_vars]\n",
    "        self.n_active = [0 for _ in self.multigrid_vars]\n",
    "        self.n_fine = [0 for _ in self.multigrid_vars]\n",
    "        for var in self.var_nodes:\n",
    "            if var.active:\n",
    "                self.n_active[var.multigrid.level] += 1\n",
    "            if var.multigrid.classification == \"coarse\":\n",
    "                self.n_coarse[var.multigrid.level] += 1\n",
    "            elif var.multigrid.classification == \"fine\":\n",
    "                self.n_fine[var.multigrid.level] += 1\n",
    "\n",
    "\n",
    "\n",
    "    # def get_means(self, slice_m=None):\n",
    "    #     \"\"\"\n",
    "    #         Get an array containing all current estimates of belief means.\n",
    "    #     \"\"\"\n",
    "    #     if slice_m is None:\n",
    "    #         slice_m = slice(0,len(self.var_nodes),1)\n",
    "    #     if len(self.mus) != len(self.var_nodes):\n",
    "    #         self.mus = [None]*(len(self.var_nodes))\n",
    "\n",
    "    #     for index, var_node in enumerate(self.var_nodes[slice_m]):\n",
    "    #         self.mus[index] =  [var_node.mu[0],var_node.mu[1]]\n",
    "\n",
    "\n",
    "    #     return self.mus\n",
    "    \n",
    "    def get_sigmas(self):\n",
    "        \"\"\"\n",
    "            Get an array containing all current estimates of belief sigmas.\n",
    "        \"\"\"\n",
    "        sigmas = np.array([])\n",
    "        for var_node in self.var_nodes:\n",
    "            sigmas = np.concatenate((sigmas, var_node.Sigma[0]))\n",
    "        return sigmas\n",
    "    \n",
    "    def get_residuals(self, level=None):\n",
    "        \"\"\"\n",
    "            Get an array containing all current estimates of belief means.\n",
    "        \"\"\"\n",
    "        if level is not None:\n",
    "            slice_v = slice(int(self.multigrid_vars[level].var_ids[0]), int(self.multigrid_vars[level].var_ids[-1]+1))\n",
    "        else:\n",
    "            slice_v = slice(int(len(self.var_nodes)))\n",
    "\n",
    "        for var in self.var_nodes[slice_v]:\n",
    "            var.residual = np.zeros(var.dofs)\n",
    "            for factor in var.adj_factors:\n",
    "                residual = factor.compute_residual() * (1 - 2 * int(factor.adj_vIDs[1] == var.variableID))\n",
    "                var.residual += residual[:2]\n",
    "\n",
    "    def get_var_residuals(self, level=None):\n",
    "        if level is not None:\n",
    "            slice_v = slice(int(self.multigrid_vars[level].var_ids[0]), int(self.multigrid_vars[level].var_ids[-1]+1))\n",
    "        else:\n",
    "            slice_v = slice(int(len(self.var_nodes)))\n",
    "\n",
    "        for var in self.var_nodes[slice_v]:\n",
    "            var.residual = var.compute_residual()\n",
    "\n",
    "\n",
    "class VariableNode:\n",
    "    def __init__(self,\n",
    "                 variable_id,\n",
    "                 dofs, level=0):\n",
    "\n",
    "        self.variableID = variable_id\n",
    "        self.adj_factors = []\n",
    "        self.InfoMat = []  # Row vector of prior Information vector in factor order\n",
    "        self.EtaVec = []  # Vector of prior eta values\n",
    "        #self.multigrid = amg_cls.mutligrid_var_info(self)\n",
    "        self.type = \"None specified\"\n",
    "        self.active = True\n",
    "\n",
    "        # Node variables are position of landmark in world frame. Initialize variable nodes at origin\n",
    "        self.mu = np.zeros(dofs)\n",
    "        self.Sigma = np.zeros([dofs, dofs])\n",
    "        self.residual = np.zeros(dofs)\n",
    "\n",
    "        self.belief = NdimGaussian(dofs)\n",
    "\n",
    "        self.prior = NdimGaussian(dofs)\n",
    "        self.prior_lambda_end = -1  # -1 flag if the sigma of self.prior is prior_sigma_end\n",
    "        self.prior_lambda_logdiff = -1\n",
    "\n",
    "        self.dofs = dofs\n",
    "\n",
    "    def update_belief(self):\n",
    "        \"\"\"\n",
    "            Update local belief estimate by taking product of all incoming messages along all edges.\n",
    "            Then send belief to adjacent factor nodes.\n",
    "        \"\"\"\n",
    "        # Update local belief\n",
    "        eta = self.prior.eta.copy()\n",
    "        lam = self.prior.lam.copy()\n",
    "        for factor in self.adj_factors:\n",
    "            message_ix = factor.adj_var_nodes.index(self)\n",
    "            eta_inward, lam_inward = factor.messages[message_ix].eta, factor.messages[message_ix].lam\n",
    "            eta += eta_inward\n",
    "            lam += lam_inward\n",
    "\n",
    "        self.belief.eta = eta \n",
    "        self.belief.lam = lam\n",
    "\n",
    "        self.Sigma = np.linalg.inv(self.belief.lam)\n",
    "        self.mu = self.Sigma @ self.belief.eta\n",
    "        \n",
    "        # Send belief to adjacent factors\n",
    "        for factor in self.adj_factors:\n",
    "            belief_ix = factor.adj_var_nodes.index(self)\n",
    "            factor.adj_beliefs[belief_ix].eta, factor.adj_beliefs[belief_ix].lam = self.belief.eta, self.belief.lam\n",
    "\n",
    "\n",
    "    def update_smooth_belief(self):\n",
    "        \"\"\"\n",
    "            Update local belief estimate by taking product of all incoming messages along all edges.\n",
    "            Then send belief to adjacent factor nodes.\n",
    "        \"\"\"\n",
    "        # Update local belief\n",
    "        eta = self.prior.eta.copy()\n",
    "        lam = self.prior.lam.copy()\n",
    "        for factor in self.adj_factors:\n",
    "            message_ix = factor.adj_vIDs.index(self.variableID)\n",
    "            mu_inward, lam_inward = factor.messages[message_ix].eta, factor.messages[message_ix].lam\n",
    "            factor.messages_prior[message_ix].eta = mu_inward  # Update messages for belief calculation now sync itr is complete\n",
    "            factor.messages_prior[message_ix].lam = lam_inward  # If don't have a prior messages then its not truely parallel\n",
    "            eta += np.diag(lam_inward) * mu_inward\n",
    "            lam += lam_inward\n",
    "\n",
    "        self.belief.eta = eta \n",
    "        self.belief.lam = lam\n",
    "        self.Sigma = 1/np.diagonal(self.belief.lam)\n",
    "        self.mu = self.Sigma * self.belief.eta\n",
    "        \n",
    "        # Send belief to adjacent factors\n",
    "        for factor in self.adj_factors:\n",
    "            belief_ix = factor.adj_vIDs.index(self.variableID)\n",
    "            factor.adj_beliefs[belief_ix].eta, factor.adj_beliefs[belief_ix].lam = self.belief.eta, self.belief.lam\n",
    "\n",
    "\n",
    "    def compute_residual(self):\n",
    "        \n",
    "        # Ax = np.zeros(2)\n",
    "\n",
    "        # Ax += self.prior.lam @ self.mu\n",
    "\n",
    "        # for factor in self.adj_factors:\n",
    "        #     for var in factor.adj_var_nodes:\n",
    "        #         if var.variableID != self.variableID:\n",
    "        #             Ax += var.mu @ factor.factor.lam[0:2, 2:4]\n",
    "\n",
    "        # d = self.prior.eta - Ax\n",
    "\n",
    "        # res = self.prior.eta - self.prior.lam @ self.mu\n",
    "\n",
    "        # for factor in self.adj_factors:\n",
    "        #     # get factor residual. Second part flips the sign if the var is second var in the factor\n",
    "        #     res += factor.compute_residual() * (1 - 2 * int(factor.adj_vIDs[1] == self.variableID))\n",
    "\n",
    "        # self.residual = res\n",
    "\n",
    "        res = self.prior.eta - self.prior.lam @ self.mu\n",
    "        for factor in self.adj_factors:\n",
    "            if factor.adj_vIDs.index(self.variableID) == 0:\n",
    "                 res += factor.factor.eta[:self.dofs] - (factor.factor.lam[:self.dofs, :self.dofs] @ self.mu \\\n",
    "                        + factor.factor.lam[:self.dofs, self.dofs:] @ factor.adj_var_nodes[1].mu)\n",
    "            else:\n",
    "                res += factor.factor.eta[-self.dofs:] - (factor.factor.lam[-self.dofs:, -self.dofs:] @ self.mu  \\\n",
    "                        + factor.factor.lam[-self.dofs:, :-self.dofs] @ factor.adj_var_nodes[0].mu)\n",
    "        \n",
    "        self.residual = res\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self,\n",
    "                 factor_id,\n",
    "                 adj_var_nodes,\n",
    "                 measurement,\n",
    "                 measurement_lambda,\n",
    "                 meas_fn,\n",
    "                 jac_fn,\n",
    "                 loss=None,\n",
    "                 mahalanobis_threshold=2,\n",
    "                 wildfire=False,\n",
    "                 *args):\n",
    "        \"\"\"\n",
    "            n_stds: number of standard deviations from mean at which loss transitions to robust loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        self.factorID = factor_id\n",
    "\n",
    "        self.dofs_conditional_vars = 0\n",
    "        self.adj_var_nodes = adj_var_nodes\n",
    "        self.adj_vIDs = []\n",
    "        self.adj_beliefs = []\n",
    "        self.messages = []\n",
    "        self.messages_prior = []\n",
    "        self.messages_dist = []\n",
    "\n",
    "        self.active = True\n",
    "\n",
    "        self.level = 0\n",
    "\n",
    "        self.type = \"factor\"\n",
    "\n",
    "        for adj_var_node in self.adj_var_nodes:\n",
    "            self.dofs_conditional_vars += adj_var_node.dofs\n",
    "            self.adj_vIDs.append(adj_var_node.variableID)\n",
    "            self.adj_beliefs.append(NdimGaussian(adj_var_node.dofs))\n",
    "            self.messages.append(NdimGaussian(adj_var_node.dofs))#, eta=adj_var_node.prior.eta, lam=adj_var_node.prior.lam))\n",
    "            self.messages_prior.append(NdimGaussian(adj_var_node.dofs))\n",
    "            self.messages_dist.append(np.zeros(adj_var_node.dofs))\n",
    "\n",
    "        self.factor = NdimGaussian(self.dofs_conditional_vars)\n",
    "        self.linpoint = np.zeros(self.dofs_conditional_vars)  # linearisation point\n",
    "\n",
    "        self.residual = None\n",
    "        self.b_calc_mess_dist = wildfire\n",
    "\n",
    "        # Measurement model\n",
    "        self.measurement = measurement\n",
    "        self.measurement_lambda = measurement_lambda\n",
    "        self.meas_fn = meas_fn\n",
    "        self.jac_fn = jac_fn\n",
    "        self.args = args\n",
    "\n",
    "        # Robust loss function\n",
    "        self.loss = loss\n",
    "        self.mahalanobis_threshold = mahalanobis_threshold\n",
    "        self.robust_flag = False\n",
    "\n",
    "        # Local relinearisation\n",
    "        self.eta_damping = 0.\n",
    "        self.iters_since_relin = 1\n",
    "\n",
    "    def compute_residual(self):\n",
    "        \"\"\"\n",
    "            Calculate the reprojection error vector.\n",
    "        \"\"\"\n",
    "        adj_belief_means = []\n",
    "        for belief in self.adj_beliefs:\n",
    "            #adj_belief_means = np.concatenate((adj_belief_means, np.linalg.inv(belief.lam) @ belief.eta))\n",
    "            adj_belief_means = np.concatenate((adj_belief_means, 1/np.diagonal(belief.lam) * belief.eta))\n",
    "        \n",
    "        # d = (self.meas_fn(adj_belief_means, *self.args) - self.measurement) / self.adaptive_gauss_noise_var\n",
    "        # d = (np.array([[-1,0,1,0],[0,-1,0,1]]) @ adj_belief_means - self.measurement) / self.adaptive_gauss_noise_var\n",
    "        d = np.array(self.measurement) @ self.factor.lam[:2,2:] - self.factor.lam[:2,:] @ adj_belief_means\n",
    "        # d = np.array(self.measurement) * self.gauss_noise_var - (adj_belief_means[2:] - adj_belief_means[:2]) * self.adaptive_gauss_noise_var\n",
    "        # ^^^ This is equivalent to the equations below which are explicitly r = b - Ax\n",
    "        # J = self.jac_fn(self.linpoint, *self.args)\n",
    "        # meas_model_lambda = np.eye(len(self.measurement)) / self.adaptive_gauss_noise_var\n",
    "        # d = J.T @ meas_model_lambda @ self.measurement - self.factor.lam @ adj_belief_means\n",
    "\n",
    "        self.residual = d\n",
    "        \n",
    "        return d\n",
    "\n",
    "    def energy(self):\n",
    "        \"\"\"\n",
    "            Computes the squared error using the appropriate loss function.\n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(self.residual) ** 2\n",
    "\n",
    "    def compute_factor(self, linpoint=None, update_self=True):\n",
    "        \"\"\"\n",
    "            Compute the factor given the linearisation point.\n",
    "            If not given then linearisation point is mean of belief of adjacent nodes.\n",
    "            If measurement model is linear then factor will always be the same regardless of linearisation point.\n",
    "        \"\"\"\n",
    "        if linpoint is None:\n",
    "            self.linpoint = []\n",
    "            for belief in self.adj_beliefs:\n",
    "                self.linpoint += list(1/np.diagonal(belief.lam) * belief.eta)\n",
    "        else:\n",
    "            self.linpoint = linpoint\n",
    "\n",
    "        if isinstance(self.jac_fn, list):\n",
    "            J = np.array(self.jac_fn)\n",
    "            pred_measurement = J @ self.linpoint\n",
    "        else:\n",
    "            J = self.jac_fn(self.linpoint, *self.args)\n",
    "            pred_measurement = self.meas_fn(self.linpoint, *self.args)\n",
    "\n",
    "        if isinstance(self.measurement, float):\n",
    "            lambda_factor = self.measurement_lambda * np.outer(J, J)\n",
    "            eta_factor = self.measurement_lambda * J.T * (J @ self.linpoint + self.measurement - pred_measurement)\n",
    "        else:\n",
    "            lambda_factor = J.T @ self.measurement_lambda @ J\n",
    "            eta_factor = (J.T @ self.measurement_lambda) @ (J @ self.linpoint + self.measurement - pred_measurement)\n",
    "\n",
    "        if update_self:\n",
    "            self.factor.eta, self.factor.lam = eta_factor, lambda_factor\n",
    "\n",
    "        return eta_factor, lambda_factor\n",
    "\n",
    "    def robustify_loss(self):\n",
    "        \"\"\"\n",
    "            Rescale the variance of the noise in the Gaussian measurement model if necessary and update the factor\n",
    "            correspondingly.\n",
    "        \"\"\"\n",
    "        old_adaptive_gauss_noise_var = self.adaptive_gauss_noise_var\n",
    "        if self.loss is None:\n",
    "            self.adaptive_gauss_noise_var = self.gauss_noise_var\n",
    "\n",
    "        else:\n",
    "            adj_belief_means = np.array([])\n",
    "            for belief in self.adj_beliefs:\n",
    "                adj_belief_means = np.concatenate((adj_belief_means, 1/np.diagonal(belief.lam) * belief.eta))\n",
    "            pred_measurement = self.meas_fn(self.linpoint, *self.args)\n",
    "\n",
    "            if self.loss == 'huber':  # Loss is linear after Nstds from mean of measurement model\n",
    "                mahalanobis_dist = np.linalg.norm(self.measurement - pred_measurement) / np.sqrt(self.gauss_noise_var)\n",
    "                if mahalanobis_dist > self.mahalanobis_threshold:\n",
    "                    self.adaptive_gauss_noise_var = self.gauss_noise_var * mahalanobis_dist**2 / \\\n",
    "                            (2*(self.mahalanobis_threshold * mahalanobis_dist - 0.5 * self.mahalanobis_threshold**2))\n",
    "                    self.robust_flag = True\n",
    "                else:\n",
    "                    self.robust_flag = False\n",
    "                    self.adaptive_gauss_noise_var = self.gauss_noise_var\n",
    "\n",
    "            elif self.loss == 'constant':  # Loss is constant after Nstds from mean of measurement model\n",
    "                mahalanobis_dist = np.linalg.norm(self.measurement - pred_measurement) / np.sqrt(self.gauss_noise_var)\n",
    "                if mahalanobis_dist > self.mahalanobis_threshold:\n",
    "                    self.adaptive_gauss_noise_var = mahalanobis_dist**2\n",
    "                    self.robust_flag = True\n",
    "                else:\n",
    "                    self.robust_flag = False\n",
    "                    self.adaptive_gauss_noise_var = self.gauss_noise_var\n",
    "\n",
    "        # Update factor using existing linearisation point (we are not relinearising).\n",
    "        self.factor.eta *= old_adaptive_gauss_noise_var / self.adaptive_gauss_noise_var\n",
    "        self.factor.lam *= old_adaptive_gauss_noise_var / self.adaptive_gauss_noise_var\n",
    "\n",
    "    def relinearise(self, min_linear_iters, beta):\n",
    "        adj_belief_means = np.array([])\n",
    "        for belief in self.adj_beliefs:\n",
    "            adj_belief_means = np.concatenate((adj_belief_means, 1/np.diagonal(belief.lam) * belief.eta))\n",
    "        if np.linalg.norm(self.linpoint - adj_belief_means) > beta and self.iters_since_relin >= min_linear_iters:\n",
    "            self.compute_factor(linpoint=adj_belief_means)\n",
    "            self.iters_since_relin = 0\n",
    "            self.eta_damping = 0.0\n",
    "        else:\n",
    "            self.iters_since_relin += 1\n",
    "\n",
    "    #@profile\n",
    "    def compute_messages(self, eta_damping):\n",
    "        \"\"\"\n",
    "            Compute all outgoing messages from the factor.\n",
    "            This is specialised for one and two variable factors.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.adj_vIDs) == 1:\n",
    "            v = 0\n",
    "            if self.b_calc_mess_dist:\n",
    "                self.messages_dist[v] = mahalanobis(self.messages[v], NdimGaussian(len(messages_eta[v]), eta=messages_eta[v], lam=messages_lam[v]))\n",
    "            self.messages[v].eta = self.factor.eta.copy()\n",
    "            self.messages[v].lam = self.factor.lam.copy()\n",
    "            return\n",
    "        \n",
    "        \n",
    "        if self.type[0:5] == \"multi\":\n",
    "            eta_damping = eta_damping\n",
    "        messages_eta, messages_lam = [], []\n",
    "\n",
    "\n",
    "        for v in range(len(self.adj_vIDs)):\n",
    "            eta_factor, lam_factor = self.factor.eta.copy(), self.factor.lam.copy()\n",
    "\n",
    "            # Take product of factor with incoming messages\n",
    "            mess_start_dim = 0\n",
    "            for var in range(len(self.adj_vIDs)):\n",
    "                if var != v:\n",
    "                    var_dofs = self.adj_var_nodes[var].dofs\n",
    "                    eta_factor[mess_start_dim:mess_start_dim + var_dofs] += self.adj_beliefs[var].eta - self.messages[var].eta\n",
    "                    lam_factor[mess_start_dim:mess_start_dim + var_dofs, mess_start_dim:mess_start_dim + var_dofs] += self.adj_beliefs[var].lam - self.messages[var].lam\n",
    "                mess_start_dim += self.adj_var_nodes[var].dofs\n",
    "\n",
    "            # Divide up parameters of distribution\n",
    "            divide =  self.adj_var_nodes[0].dofs\n",
    "            if v == 0:\n",
    "                eo = eta_factor[:divide]\n",
    "                eno = eta_factor[divide:]\n",
    "\n",
    "                loo = lam_factor[:divide, :divide]\n",
    "                lono = lam_factor[:divide, divide:]\n",
    "                lnoo = lam_factor[divide:, :divide]\n",
    "                lnono = lam_factor[divide:, divide:]\n",
    "            elif v == 1:\n",
    "                eo = eta_factor[divide:]\n",
    "                eno = eta_factor[:divide]\n",
    "\n",
    "                loo = lam_factor[divide:, divide:]\n",
    "                lono = lam_factor[divide:, :divide]\n",
    "                lnoo = lam_factor[:divide, divide:]\n",
    "                lnono = lam_factor[:divide, :divide]\n",
    "\n",
    "            lnono += 1e-12 * np.eye(lnono.shape[0])\n",
    "            lnono_inv = np.linalg.inv(lnono)\n",
    "\n",
    "            new_message_lam = loo - lono @ lnono_inv @ lnoo\n",
    "            messages_lam.append((1 - eta_damping) * new_message_lam + eta_damping * self.messages[v].lam)\n",
    "            new_message_eta = eo - lono @ lnono_inv @ eno\n",
    "            messages_eta.append((1 - eta_damping) * new_message_eta + eta_damping * self.messages[v].eta)\n",
    "\n",
    "\n",
    "            \n",
    "        for v in range(len(self.adj_vIDs)):\n",
    "            #self.messages_dist[v] = bhattacharyya(self.messages[v], NdimGaussian(len(messages_eta[v]), eta=messages_eta[v], lam=messages_lam[v]))\n",
    "            if self.b_calc_mess_dist:\n",
    "                self.messages_dist[v] = mahalanobis(self.messages[v], NdimGaussian(len(messages_eta[v]), eta=messages_eta[v], lam=messages_lam[v]))\n",
    "            self.messages[v].lam = messages_lam[v]\n",
    "            self.messages[v].eta = messages_eta[v]\n",
    "\n",
    "\n",
    "        #time.sleep(0.00000001)\n",
    "\n",
    "        \n",
    "\n",
    "    def smoothing_compute_messages(self, eta_damping):\n",
    "\n",
    "\n",
    "        for v in range(len(self.adj_vIDs)):\n",
    "            # Pii = np.array([self.adj_var_nodes[v].prior.lam[0,0], self.adj_var_nodes[v].prior.lam[1,1]])\n",
    "            # uii = self.adj_var_nodes[v].prior.eta / Pii\n",
    "\n",
    "            # var_dofs = self.adj_var_nodes[v].dofs\n",
    "             \n",
    "            # Pki_sum = 0\n",
    "            # uki_sum = 0\n",
    "\n",
    "            # for factor in self.adj_var_nodes[v].adj_factors:\n",
    "            #     if factor.factorID is not self.factorID:\n",
    "            #         if factor.adj_vIDs[0] is self.adj_vIDs[v]:\n",
    "            #             v_ix = 0\n",
    "            #         else:\n",
    "            #             v_ix = 1\n",
    "                    \n",
    "            #         Pki = np.array([factor.messages[v_ix].lam[0,0], factor.messages[v_ix].lam[1,1]])\n",
    "            #         uki = factor.messages[v_ix].eta\n",
    "                 \n",
    "            #         Pki_sum += Pki\n",
    "            #         uki_sum += uki * Pki\n",
    "                     \n",
    "            # Aij = np.array([self.factor.lam[0,2],self.factor.lam[1,3]])\n",
    "            # Pij = (-Aij**2) / (Pii + Pki_sum)\n",
    "            # uij = ((Pii * uii) + uki_sum) / Aij\n",
    "\n",
    "            # self.messages[1-v].lam[0,0] = Pij[0]\n",
    "            # self.messages[1-v].lam[1,1] = Pij[1]\n",
    "            # self.messages[1-v].eta = uij \n",
    "\n",
    "            Aij = np.array([self.factor.lam[0,2],self.factor.lam[1,3]])\n",
    "            Pij = (-Aij**2) / (np.diagonal(self.adj_beliefs[v].lam - self.messages_prior[v].lam))\n",
    "            uij = (self.adj_beliefs[v].eta - self.messages_prior[v].eta * np.diag(self.messages_prior[v].lam)) / Aij\n",
    "\n",
    "            self.messages[1-v].lam[0,0] = (1 - eta_damping) * Pij[0] + eta_damping * self.messages[1-v].lam[0,0]\n",
    "            self.messages[1-v].lam[1,1] = (1 - eta_damping) * Pij[1] + eta_damping * self.messages[1-v].lam[1,1]\n",
    "            self.messages[1-v].eta = (1 - eta_damping) * uij + eta_damping * self.messages[1-v].eta\n",
    "\n",
    "            #self.adj_var_nodes[1-v].update_smooth_belief()\n",
    "\n",
    "        #time.sleep(0.00000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8f1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_to_super_kmeans(prev_nodes, prev_edges, k, layer_idx, max_iters=20, tol=1e-6, seed=0):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    n = positions.shape[0]\n",
    "    if k <= 0: \n",
    "        k = 1\n",
    "    k = min(k, n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # -------- 改进版初始化 --------\n",
    "    # 随机无放回抽 k 个点，保证一开始每簇有独立的点\n",
    "    init_idx = rng.choice(n, size=k, replace=False)\n",
    "    centers = positions[init_idx]\n",
    "\n",
    "    # Lloyd 迭代\n",
    "    for _ in range(max_iters):\n",
    "        d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "        assign = np.argmin(d2, axis=1)\n",
    "\n",
    "        # -------- 空簇修补 --------\n",
    "        counts = np.bincount(assign, minlength=k)\n",
    "        empty_clusters = np.where(counts == 0)[0]\n",
    "        for ci in empty_clusters:\n",
    "            # 找到最大簇\n",
    "            big_cluster = np.argmax(counts)\n",
    "            big_idxs = np.where(assign == big_cluster)[0]\n",
    "            # 偷一个点过来\n",
    "            steal_idx = big_idxs[0]\n",
    "            assign[steal_idx] = ci\n",
    "            counts[big_cluster] -= 1\n",
    "            counts[ci] += 1\n",
    "\n",
    "        moved = 0.0\n",
    "        for ci in range(k):\n",
    "            idxs = np.where(assign == ci)[0]\n",
    "            new_c = positions[idxs].mean(axis=0)\n",
    "            moved = max(moved, float(np.linalg.norm(new_c - centers[ci])))\n",
    "            centers[ci] = new_c\n",
    "        if moved < tol:\n",
    "            break\n",
    "\n",
    "    # final assign (再做一次保证)\n",
    "    d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "    assign = np.argmin(d2, axis=1)\n",
    "\n",
    "    counts = np.bincount(assign, minlength=k)\n",
    "    empty_clusters = np.where(counts == 0)[0]\n",
    "    for ci in empty_clusters:\n",
    "        big_cluster = np.argmax(counts)\n",
    "        big_idxs = np.where(assign == big_cluster)[0]\n",
    "        steal_idx = big_idxs[0]\n",
    "        assign[steal_idx] = ci\n",
    "        counts[big_cluster] -= 1\n",
    "        counts[ci] += 1\n",
    "\n",
    "    # ---------- 构造 super graph ----------\n",
    "    super_nodes, node_map = [], {}\n",
    "    for ci in range(k):\n",
    "        idxs = np.where(assign == ci)[0]\n",
    "        pts = positions[idxs]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in idxs]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        nid = f\"{ci}\"\n",
    "        super_nodes.append({\n",
    "            \"data\": {\"id\": nid, \"layer\": layer_idx, \"dim\": dim_val},\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in idxs:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "        if v != \"prior\":\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            else:\n",
    "                eid = (su, \"prior\")\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = (su, \"prior\")\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d9f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_to_super_grid(prev_nodes, prev_edges, gx, gy, layer_idx):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    xmin, ymin = positions.min(axis=0); xmax, ymax = positions.max(axis=0)\n",
    "    cell_w = (xmax - xmin) / gx if gx > 0 else 1.0\n",
    "    cell_h = (ymax - ymin) / gy if gy > 0 else 1.0\n",
    "    if cell_w == 0: cell_w = 1.0\n",
    "    if cell_h == 0: cell_h = 1.0\n",
    "    cell_map = {}\n",
    "    for idx, n in enumerate(prev_nodes):\n",
    "        x, y = n[\"position\"][\"x\"], n[\"position\"][\"y\"]\n",
    "        cx = min(int((x - xmin) / cell_w), gx - 1)\n",
    "        cy = min(int((y - ymin) / cell_h), gy - 1)\n",
    "        cid = cx + cy * gx\n",
    "        cell_map.setdefault(cid, []).append(idx)\n",
    "    super_nodes, node_map = [], {}\n",
    "    for cid, indices in cell_map.items():\n",
    "        pts = positions[indices]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in indices]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        nid = str(len(super_nodes))\n",
    "        super_nodes.append({\n",
    "            \"data\": {\"id\": nid, \"layer\": layer_idx, \"dim\": dim_val},\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in indices:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if v != \"prior\":\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            elif su == sv:\n",
    "                eid = tuple(sorted((su, \"prior\")))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "\n",
    "        elif v == \"prior\":\n",
    "            su = node_map[u]\n",
    "            eid = tuple(sorted((su, v)))\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13ace2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_super_graph(layers):\n",
    "    \"\"\"\n",
    "    基于 layers[-2] 的 base graph, 和 layers[-1] 的 super 分组，构造 super graph。\n",
    "    要求: layers[-2][\"graph\"] 已经是构建好的基图（含 unary/binary 因子）。\n",
    "    layers[-1][\"node_map\"]: { base_node_id(str, 如 'b12') -> super_node_id(str) }\n",
    "    \"\"\"\n",
    "    from scipy.linalg import block_diag\n",
    "    # ---------- 取出 base & super ----------\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_nodes = layers[-1][\"nodes\"]\n",
    "    super_edges = layers[-1][\"edges\"]\n",
    "    node_map    = layers[-1][\"node_map\"]   # 'bN' -> 'sX_...'\n",
    "\n",
    "    # base: id(int)->VariableNode，方便查 dofs 和 mu\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    # ---------- super_id -> [base_id(int)] ----------\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "\n",
    "    # ---------- 为每个 super 组建立 (start, dofs) 表 ----------\n",
    "    # local_idx[sid][bid] = (start, dofs), total_dofs[sid] = sum(dofs)\n",
    "    local_idx   = {}\n",
    "    total_dofs  = {}\n",
    "    for sid, group in super_groups.items():\n",
    "        off = 0\n",
    "        local_idx[sid] = {}\n",
    "        for bid in group:\n",
    "            d = id2var[bid].dofs\n",
    "            local_idx[sid][bid] = (off, d)\n",
    "            off += d\n",
    "        total_dofs[sid] = off\n",
    "\n",
    "\n",
    "    # ---------- 创建 super VariableNodes ----------\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=0)\n",
    "\n",
    "    super_var_nodes = {}\n",
    "    for i, sn in enumerate(super_nodes):\n",
    "        sid = sn[\"data\"][\"id\"]\n",
    "        dofs = total_dofs.get(sid, 0)\n",
    "\n",
    "        v = VariableNode(i, dofs=dofs)\n",
    "\n",
    "        # === 叠加 base GT ===\n",
    "        gt_vec = np.zeros(dofs)\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            gt_base = getattr(id2var[bid], \"GT\", None)\n",
    "            if gt_base is None or len(gt_base) != d:\n",
    "                gt_base = np.zeros(d)\n",
    "            gt_vec[st:st+d] = gt_base\n",
    "        v.GT = gt_vec\n",
    "        v.prior.lam = 1e-10 * np.eye(dofs, dtype=float)\n",
    "        v.prior.eta = np.zeros(dofs, dtype=float)\n",
    "\n",
    "        super_var_nodes[sid] = v\n",
    "        fg.var_nodes.append(v)\n",
    "\n",
    "\n",
    "    fg.n_var_nodes = len(fg.var_nodes)\n",
    "\n",
    "    # ---------- 工具：拼接某组的 linpoint（用 base belief 均值） ----------\n",
    "    def make_linpoint_for_group(sid):\n",
    "        x = np.zeros(total_dofs[sid])\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            mu = getattr(id2var[bid], \"mu\", None)\n",
    "            if mu is None or len(mu) != d:\n",
    "                mu = np.zeros(d)\n",
    "            x[st:st+d] = mu\n",
    "        return x\n",
    "\n",
    "    # ---------- 3) super prior（in_group unary + in_group binary） ----------\n",
    "    def make_super_prior_factor(sid, base_factors):\n",
    "        group = super_groups[sid]\n",
    "        idx_map = local_idx[sid]\n",
    "        ncols = total_dofs[sid]\n",
    "\n",
    "        # 选出：所有变量都在组内的因子（unary 或 binary）\n",
    "        in_group = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if all(vid in group for vid in vids):\n",
    "                in_group.append(f)\n",
    "\n",
    "        def meas_fn_super_prior(x_super, *args):\n",
    "            meas_fn = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # 拼本因子的局部 x\n",
    "                x_loc_list = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) if meas_fn else np.zeros(0)\n",
    "\n",
    "        def jac_fn_super_prior(x_super, *args):\n",
    "            Jrows = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # 构造本因子的局部 x，用于（潜在）非线性雅可比\n",
    "                x_loc_list = []\n",
    "                dims = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    dims.append(d)\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "                # 将 Jloc 列块映射回 super 变量的列\n",
    "                row = np.zeros((Jloc.shape[0], ncols))\n",
    "                c0 = 0\n",
    "                for vid, d in zip(vids, dims):\n",
    "                    st, _ = idx_map[vid]\n",
    "                    row[:, st:st+d] = Jloc[:, c0:c0+d]\n",
    "                    c0 += d\n",
    "\n",
    "                Jrows.append(row)\n",
    "            return np.vstack(Jrows) if Jrows else np.zeros((0, ncols))\n",
    "\n",
    "        # z_super：拼各 base 因子的 z\n",
    "        z_list = [f.measurement for f in in_group]\n",
    "        z_lambda_list = [f.measurement_lambda for f in in_group]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_prior, jac_fn_super_prior, z_super, z_super_lambda \n",
    "\n",
    "    # ---------- 4) super between（cross_group binary） ----------\n",
    "    def make_super_between_factor(sidA, sidB, base_factors):\n",
    "        groupA, groupB = super_groups[sidA], super_groups[sidB]\n",
    "        idxA, idxB = local_idx[sidA], local_idx[sidB]\n",
    "        nA, nB = total_dofs[sidA], total_dofs[sidB]\n",
    "\n",
    "        cross = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if len(vids) != 2:\n",
    "                continue\n",
    "            i, j = vids\n",
    "            # on side in A，the other side in B\n",
    "            if (i in groupA and j in groupB) or (i in groupB and j in groupA):\n",
    "                cross.append(f)\n",
    "\n",
    "\n",
    "        def meas_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            meas_fn = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) \n",
    "\n",
    "        def jac_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            Jrows = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                    left_start, right_start = si, nA + sj\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                    left_start, right_start = nA + si, sj\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "\n",
    "                row = np.zeros((Jloc.shape[0], nA + nB))\n",
    "                row[:, left_start:left_start+di]   = Jloc[:, :di] \n",
    "                row[:, right_start:right_start+dj] = Jloc[:, di:di+dj] \n",
    "\n",
    "                Jrows.append(row)\n",
    "            return np.vstack(Jrows) \n",
    "\n",
    "        z_list = [f.measurement for f in cross]\n",
    "        z_lambda_list = [f.measurement_lambda for f in cross]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_between, jac_fn_super_between, z_super, z_super_lambda\n",
    "\n",
    "\n",
    "    for e in super_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if v == \"prior\":\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_prior_factor(u, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.type = \"super_prior\"\n",
    "            lin0 = make_linpoint_for_group(u)\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            \n",
    "        else:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_between_factor(u, v, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u], super_var_nodes[v]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.type = \"super_between\"\n",
    "            lin0 = np.concatenate([make_linpoint_for_group(u), make_linpoint_for_group(v)])\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            super_var_nodes[v].adj_factors.append(f)\n",
    "\n",
    "\n",
    "    fg.n_factor_nodes = len(fg.factors)\n",
    "    return fg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e18f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_abs(super_nodes, super_edges, layer_idx):\n",
    "    abs_nodes = []\n",
    "    for n in super_nodes:\n",
    "        nid = n[\"data\"][\"id\"].replace(\"s\", \"a\", 1)\n",
    "        abs_nodes.append({\n",
    "            \"data\": {\"id\": nid, \"layer\": layer_idx, \"dim\": n[\"data\"][\"dim\"]},\n",
    "            \"position\": {\"x\": n[\"position\"][\"x\"], \"y\": n[\"position\"][\"y\"]}\n",
    "        })\n",
    "    abs_edges = []\n",
    "    for e in super_edges:\n",
    "        abs_edges.append({\"data\": {\n",
    "            \"source\": e[\"data\"][\"source\"].replace(\"s\", \"a\", 1),\n",
    "            \"target\": e[\"data\"][\"target\"].replace(\"s\", \"a\", 1)\n",
    "        }})\n",
    "    return abs_nodes, abs_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a414d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_abs_graph(\n",
    "    layers,\n",
    "    r_reduced = 2):\n",
    "\n",
    "    abs_var_nodes = {}\n",
    "    Bs = {}\n",
    "    ks = {}\n",
    "    r = 2\n",
    "\n",
    "    # === 1. Build Abstraction Variables ===\n",
    "    abs_fg = FactorGraph(nonlinear_factors=False, eta_damping=0)\n",
    "    sup_fg = layers[-2][\"graph\"]\n",
    "\n",
    "    for sn in sup_fg.var_nodes:#\n",
    "        if sn.dofs <= r_reduced:\n",
    "            r = sn.dofs  # No reduction if dofs already <= r\n",
    "        else:\n",
    "            r = r_reduced\n",
    "\n",
    "        sid = sn.variableID\n",
    "        varis_sup_mu = sn.mu\n",
    "        varis_sup_sigma = sn.Sigma\n",
    "        \n",
    "        # Step 1: Eigen decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(varis_sup_sigma)\n",
    "\n",
    "        # Step 2: Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
    "        idx = np.argsort(eigvals)[::-1]      # Get indices of sorted eigenvalues (largest first)\n",
    "        eigvals = eigvals[idx]               # Reorder eigenvalues\n",
    "        eigvecs = eigvecs[:, idx]            # Reorder corresponding eigenvectors\n",
    "\n",
    "        # Step 3: Select the top-k eigenvectors to form the projection matrix (principal subspace)\n",
    "        B_reduced = eigvecs[:, :r]                 # B_reduced: shape (sup_dof, r), projects r to sup_dof\n",
    "        Bs[sid] = B_reduced                        # Store the projection matrix for this variable\n",
    "\n",
    "        # Step 4: Project eta and Lam onto the reduced 2D subspace\n",
    "        varis_abs_mu = B_reduced.T @ varis_sup_mu          # Projected natural mean: shape (2,)\n",
    "        varis_abs_sigma = B_reduced.T @ varis_sup_sigma @ B_reduced  # Projected covariance: shape (2, 2)\n",
    "        ks[sid] = varis_sup_mu - B_reduced @ varis_abs_mu  # Store the offset for this variable\n",
    "\n",
    "        varis_abs_lam = np.linalg.inv(varis_abs_sigma)  # Inverse covariance (precision matrix): shape (2, 2)\n",
    "        varis_abs_eta = varis_abs_lam @ varis_abs_mu  # Natural parameters: shape (2,)\n",
    "\n",
    "        v = VariableNode(sid, dofs=r)\n",
    "        v.GT = sn.GT\n",
    "        v.prior.lam = 1e-10 * np.eye(r, dtype=float)\n",
    "        v.prior.eta = np.zeros(r, dtype=float)\n",
    "        v.mu = varis_abs_mu\n",
    "        v.Sigma = varis_abs_sigma\n",
    "        v.belief = NdimGaussian(r, varis_abs_eta, varis_abs_lam)\n",
    "\n",
    "        abs_var_nodes[sid] = v\n",
    "        abs_fg.var_nodes.append(v)\n",
    "    abs_fg.n_var_nodes = len(abs_fg.var_nodes)\n",
    "\n",
    "\n",
    "    # === 2. Abstract Prior ===\n",
    "    def make_abs_prior_factor(sup_factor):\n",
    "        abs_id = sup_factor.adj_var_nodes[0].variableID\n",
    "        B = Bs[abs_id]\n",
    "        k = ks[abs_id]\n",
    "\n",
    "        def meas_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.meas_fn(B @ x_abs + k)\n",
    "        \n",
    "        def jac_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.jac_fn(B @ x_abs + k) @ B\n",
    "\n",
    "\n",
    "        return meas_fn_abs_prior, jac_fn_abs_prior, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "\n",
    "    # === 3. Abstract Between ===\n",
    "    def make_abs_between_factor(sup_factor):\n",
    "        vids = [v.variableID for v in sup_factor.adj_var_nodes]\n",
    "        i, j = vids # two variable IDs\n",
    "        ni = abs_var_nodes[i].dofs\n",
    "        Bi, Bj = Bs[i], Bs[j]\n",
    "        ki, kj = ks[i], ks[j]                       \n",
    "\n",
    "\n",
    "        def meas_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            return sup_factor.meas_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "\n",
    "        def jac_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            J_sup = sup_factor.jac_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "            J_abs = np.zeros((J_sup.shape[0], ni + xj.shape[0]))\n",
    "            J_abs[:, :ni] = J_sup[:, :Bi.shape[0]] @ Bi\n",
    "            J_abs[:, ni:] = J_sup[:, Bi.shape[0]:] @ Bj\n",
    "            return J_abs\n",
    "        \n",
    "        return meas_fn_super_between, jac_fn_super_between, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "    for f in sup_fg.factors:\n",
    "        if len(f.adj_var_nodes) == 1:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_prior_factor(f)\n",
    "            v = abs_var_nodes[f.adj_var_nodes[0].variableID]\n",
    "            abs_f = Factor(f.factorID, [v], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_prior\"\n",
    "            abs_f.adj_beliefs = [v.belief]\n",
    "\n",
    "            lin0 = v.mu\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            v.adj_factors.append(abs_f)\n",
    "\n",
    "        elif len(f.adj_var_nodes) == 2:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_between_factor(f)\n",
    "            i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "            vi, vj = abs_var_nodes[i], abs_var_nodes[j]\n",
    "            abs_f = Factor(f.factorID, [vi, vj], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_between\"\n",
    "            abs_f.adj_beliefs = [vi.belief, vj.belief]\n",
    "\n",
    "            lin0 = np.concatenate([vi.mu, vj.mu])\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            vi.adj_factors.append(abs_f)\n",
    "            vj.adj_factors.append(abs_f)\n",
    "    abs_fg.n_factor_nodes = len(abs_fg.factors)\n",
    "\n",
    "\n",
    "    return abs_fg, Bs, ks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a726136",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 构建 GBP 图\n",
    "# -----------------------\n",
    "\n",
    "base_nodes, base_edges, gbp_graph = make_two_cliques_graph(prior_sigma=10.0, odom_sigma=10.0, rng=None)\n",
    "layers = [{\"name\": \"base\", \"nodes\": base_nodes, \"edges\": base_edges}]\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "pair_idx = 0\n",
    "opts=[{\"label\":\"base\",\"value\":\"base\"}]\n",
    "\n",
    "\n",
    "last = layers[-1]\n",
    "super_layer_idx = 1\n",
    "\n",
    "\n",
    "super_nodes, super_edges, node_map = fuse_to_super_grid(last[\"nodes\"], last[\"edges\"], 2, 1, super_layer_idx)\n",
    "super_nodes, super_edges, node_map = fuse_to_super_kmeans(last[\"nodes\"], last[\"edges\"], 2, super_layer_idx)\n",
    "layers.append({\"name\":f\"super{1}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "layers[-1][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "for i in range(100):\n",
    "    layers[-1][\"graph\"].synchronous_iteration()\n",
    "\n",
    "\n",
    "abs_nodes, abs_edges   = copy_to_abs(super_nodes, super_edges, super_layer_idx + 1)\n",
    "layers.append({\"name\":f\"abs{1}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[-1][\"graph\"], Bs, ks = build_abs_graph(layers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be439a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'source': '0', 'target': '1'}}\n",
      "{'data': {'source': '0', 'target': '2'}}\n",
      "{'data': {'source': '0', 'target': '3'}}\n",
      "{'data': {'source': '1', 'target': '2'}}\n",
      "{'data': {'source': '1', 'target': '3'}}\n",
      "{'data': {'source': '2', 'target': '3'}}\n",
      "{'data': {'source': '4', 'target': '5'}}\n",
      "{'data': {'source': '4', 'target': '6'}}\n",
      "{'data': {'source': '5', 'target': '6'}}\n",
      "{'data': {'source': '3', 'target': '4'}}\n",
      "{'data': {'source': '0', 'target': 'prior'}}\n",
      "{'data': {'source': '1', 'target': 'prior'}}\n",
      "{'data': {'source': '2', 'target': 'prior'}}\n",
      "{'data': {'source': '3', 'target': 'prior'}}\n",
      "{'data': {'source': '4', 'target': 'prior'}}\n",
      "{'data': {'source': '5', 'target': 'prior'}}\n",
      "{'data': {'source': '6', 'target': 'prior'}}\n"
     ]
    }
   ],
   "source": [
    "for edge in base_edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c7cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': {'id': '0', 'layer': 1, 'dim': 8}, 'position': {'x': 0.0, 'y': 15.0}}, {'data': {'id': '1', 'layer': 1, 'dim': 6}, 'position': {'x': 50.0, 'y': 10.0}}]\n",
      "[{'data': {'source': '0', 'target': 'prior'}}, {'data': {'source': '1', 'target': 'prior'}}, {'data': {'source': '0', 'target': '1'}}]\n",
      "{'0': '0', '1': '0', '2': '0', '3': '0', '4': '1', '5': '1', '6': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(super_nodes)\n",
    "print(super_edges)\n",
    "print(node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1ea9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5e78c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 315.810054\n",
      "Iter 002 | Energy = 182.343369\n",
      "Iter 003 | Energy = 190.018171\n",
      "Iter 004 | Energy = 191.794979\n",
      "Iter 005 | Energy = 194.556536\n",
      "Iter 006 | Energy = 193.906010\n",
      "Iter 007 | Energy = 194.076984\n",
      "Iter 008 | Energy = 193.931422\n",
      "Iter 009 | Energy = 193.556378\n",
      "Iter 010 | Energy = 193.490921\n",
      "Iter 011 | Energy = 193.451201\n",
      "Iter 012 | Energy = 193.397052\n",
      "Iter 013 | Energy = 193.380109\n",
      "Iter 014 | Energy = 193.371536\n",
      "Iter 015 | Energy = 193.364074\n",
      "Iter 016 | Energy = 193.360586\n",
      "Iter 017 | Energy = 193.358605\n",
      "Iter 018 | Energy = 193.357341\n",
      "Iter 019 | Energy = 193.356659\n",
      "Iter 020 | Energy = 193.356249\n",
      "Iter 021 | Energy = 193.356011\n",
      "Iter 022 | Energy = 193.355878\n",
      "Iter 023 | Energy = 193.355799\n",
      "Iter 024 | Energy = 193.355753\n",
      "Iter 025 | Energy = 193.355727\n",
      "Iter 026 | Energy = 193.355712\n",
      "Iter 027 | Energy = 193.355703\n",
      "Iter 028 | Energy = 193.355698\n",
      "Iter 029 | Energy = 193.355695\n",
      "Iter 030 | Energy = 193.355693\n",
      "Iter 031 | Energy = 193.355692\n",
      "Iter 032 | Energy = 193.355691\n",
      "Iter 033 | Energy = 193.355691\n",
      "Iter 034 | Energy = 193.355691\n",
      "Iter 035 | Energy = 193.355691\n",
      "Iter 036 | Energy = 193.355691\n",
      "Iter 037 | Energy = 193.355691\n",
      "Iter 038 | Energy = 193.355691\n",
      "Iter 039 | Energy = 193.355691\n",
      "Iter 040 | Energy = 193.355691\n",
      "Iter 041 | Energy = 193.355691\n",
      "Iter 042 | Energy = 193.355691\n",
      "Iter 043 | Energy = 193.355691\n",
      "Iter 044 | Energy = 193.355691\n",
      "Iter 045 | Energy = 193.355691\n",
      "Iter 046 | Energy = 193.355691\n",
      "Iter 047 | Energy = 193.355691\n",
      "Iter 048 | Energy = 193.355691\n",
      "Iter 049 | Energy = 193.355691\n",
      "Iter 050 | Energy = 193.355691\n",
      "Iter 051 | Energy = 193.355691\n",
      "Iter 052 | Energy = 193.355691\n",
      "Iter 053 | Energy = 193.355691\n",
      "Iter 054 | Energy = 193.355691\n",
      "Iter 055 | Energy = 193.355691\n",
      "Iter 056 | Energy = 193.355691\n",
      "Iter 057 | Energy = 193.355691\n",
      "Iter 058 | Energy = 193.355691\n",
      "Iter 059 | Energy = 193.355691\n",
      "Iter 060 | Energy = 193.355691\n",
      "Iter 061 | Energy = 193.355691\n",
      "Iter 062 | Energy = 193.355691\n",
      "Iter 063 | Energy = 193.355691\n",
      "Iter 064 | Energy = 193.355691\n",
      "Iter 065 | Energy = 193.355691\n",
      "Iter 066 | Energy = 193.355691\n",
      "Iter 067 | Energy = 193.355691\n",
      "Iter 068 | Energy = 193.355691\n",
      "Iter 069 | Energy = 193.355691\n",
      "Iter 070 | Energy = 193.355691\n",
      "Iter 071 | Energy = 193.355691\n",
      "Iter 072 | Energy = 193.355691\n",
      "Iter 073 | Energy = 193.355691\n",
      "Iter 074 | Energy = 193.355691\n",
      "Iter 075 | Energy = 193.355691\n",
      "Iter 076 | Energy = 193.355691\n",
      "Iter 077 | Energy = 193.355691\n",
      "Iter 078 | Energy = 193.355691\n",
      "Iter 079 | Energy = 193.355691\n",
      "Iter 080 | Energy = 193.355691\n",
      "Iter 081 | Energy = 193.355691\n",
      "Iter 082 | Energy = 193.355691\n",
      "Iter 083 | Energy = 193.355691\n",
      "Iter 084 | Energy = 193.355691\n",
      "Iter 085 | Energy = 193.355691\n",
      "Iter 086 | Energy = 193.355691\n",
      "Iter 087 | Energy = 193.355691\n",
      "Iter 088 | Energy = 193.355691\n",
      "Iter 089 | Energy = 193.355691\n",
      "Iter 090 | Energy = 193.355691\n",
      "Iter 091 | Energy = 193.355691\n",
      "Iter 092 | Energy = 193.355691\n",
      "Iter 093 | Energy = 193.355691\n",
      "Iter 094 | Energy = 193.355691\n",
      "Iter 095 | Energy = 193.355691\n",
      "Iter 096 | Energy = 193.355691\n",
      "Iter 097 | Energy = 193.355691\n",
      "Iter 098 | Energy = 193.355691\n",
      "Iter 099 | Energy = 193.355691\n",
      "Iter 100 | Energy = 193.355691\n"
     ]
    }
   ],
   "source": [
    "basegraph = layers[-3][\"graph\"]\n",
    "#supergraph = layers[-1][\"graph\"]\n",
    "\n",
    "for it in range(100):\n",
    "    basegraph.synchronous_iteration()\n",
    "    energy = basegraph.energy_map(include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf7f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 193.355691\n",
      "Iter 002 | Energy = 193.355691\n",
      "Iter 003 | Energy = 193.355691\n",
      "Iter 004 | Energy = 193.355691\n",
      "Iter 005 | Energy = 193.355691\n",
      "Iter 006 | Energy = 193.355691\n",
      "Iter 007 | Energy = 193.355691\n",
      "Iter 008 | Energy = 193.355691\n",
      "Iter 009 | Energy = 193.355691\n",
      "Iter 010 | Energy = 193.355691\n",
      "Iter 011 | Energy = 193.355691\n",
      "Iter 012 | Energy = 193.355691\n",
      "Iter 013 | Energy = 193.355691\n",
      "Iter 014 | Energy = 193.355691\n",
      "Iter 015 | Energy = 193.355691\n",
      "Iter 016 | Energy = 193.355691\n",
      "Iter 017 | Energy = 193.355691\n",
      "Iter 018 | Energy = 193.355691\n",
      "Iter 019 | Energy = 193.355691\n",
      "Iter 020 | Energy = 193.355691\n",
      "Iter 021 | Energy = 193.355691\n",
      "Iter 022 | Energy = 193.355691\n",
      "Iter 023 | Energy = 193.355691\n",
      "Iter 024 | Energy = 193.355691\n",
      "Iter 025 | Energy = 193.355691\n",
      "Iter 026 | Energy = 193.355691\n",
      "Iter 027 | Energy = 193.355691\n",
      "Iter 028 | Energy = 193.355691\n",
      "Iter 029 | Energy = 193.355691\n",
      "Iter 030 | Energy = 193.355691\n",
      "Iter 031 | Energy = 193.355691\n",
      "Iter 032 | Energy = 193.355691\n",
      "Iter 033 | Energy = 193.355691\n",
      "Iter 034 | Energy = 193.355691\n",
      "Iter 035 | Energy = 193.355691\n",
      "Iter 036 | Energy = 193.355691\n",
      "Iter 037 | Energy = 193.355691\n",
      "Iter 038 | Energy = 193.355691\n",
      "Iter 039 | Energy = 193.355691\n",
      "Iter 040 | Energy = 193.355691\n",
      "Iter 041 | Energy = 193.355691\n",
      "Iter 042 | Energy = 193.355691\n",
      "Iter 043 | Energy = 193.355691\n",
      "Iter 044 | Energy = 193.355691\n",
      "Iter 045 | Energy = 193.355691\n",
      "Iter 046 | Energy = 193.355691\n",
      "Iter 047 | Energy = 193.355691\n",
      "Iter 048 | Energy = 193.355691\n",
      "Iter 049 | Energy = 193.355691\n",
      "Iter 050 | Energy = 193.355691\n",
      "Iter 051 | Energy = 193.355691\n",
      "Iter 052 | Energy = 193.355691\n",
      "Iter 053 | Energy = 193.355691\n",
      "Iter 054 | Energy = 193.355691\n",
      "Iter 055 | Energy = 193.355691\n",
      "Iter 056 | Energy = 193.355691\n",
      "Iter 057 | Energy = 193.355691\n",
      "Iter 058 | Energy = 193.355691\n",
      "Iter 059 | Energy = 193.355691\n",
      "Iter 060 | Energy = 193.355691\n",
      "Iter 061 | Energy = 193.355691\n",
      "Iter 062 | Energy = 193.355691\n",
      "Iter 063 | Energy = 193.355691\n",
      "Iter 064 | Energy = 193.355691\n",
      "Iter 065 | Energy = 193.355691\n",
      "Iter 066 | Energy = 193.355691\n",
      "Iter 067 | Energy = 193.355691\n",
      "Iter 068 | Energy = 193.355691\n",
      "Iter 069 | Energy = 193.355691\n",
      "Iter 070 | Energy = 193.355691\n",
      "Iter 071 | Energy = 193.355691\n",
      "Iter 072 | Energy = 193.355691\n",
      "Iter 073 | Energy = 193.355691\n",
      "Iter 074 | Energy = 193.355691\n",
      "Iter 075 | Energy = 193.355691\n",
      "Iter 076 | Energy = 193.355691\n",
      "Iter 077 | Energy = 193.355691\n",
      "Iter 078 | Energy = 193.355691\n",
      "Iter 079 | Energy = 193.355691\n",
      "Iter 080 | Energy = 193.355691\n",
      "Iter 081 | Energy = 193.355691\n",
      "Iter 082 | Energy = 193.355691\n",
      "Iter 083 | Energy = 193.355691\n",
      "Iter 084 | Energy = 193.355691\n",
      "Iter 085 | Energy = 193.355691\n",
      "Iter 086 | Energy = 193.355691\n",
      "Iter 087 | Energy = 193.355691\n",
      "Iter 088 | Energy = 193.355691\n",
      "Iter 089 | Energy = 193.355691\n",
      "Iter 090 | Energy = 193.355691\n",
      "Iter 091 | Energy = 193.355691\n",
      "Iter 092 | Energy = 193.355691\n",
      "Iter 093 | Energy = 193.355691\n",
      "Iter 094 | Energy = 193.355691\n",
      "Iter 095 | Energy = 193.355691\n",
      "Iter 096 | Energy = 193.355691\n",
      "Iter 097 | Energy = 193.355691\n",
      "Iter 098 | Energy = 193.355691\n",
      "Iter 099 | Energy = 193.355691\n",
      "Iter 100 | Energy = 193.355691\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 找到所有 super_prior factors\n",
    "prior_factors = [f for f in supergraph.factors if getattr(f, \"type\", \"\") == \"super_prior\"]\n",
    "# 对 super_prior factors 的邻居变量\n",
    "prior_vars = []\n",
    "for f in prior_factors:\n",
    "    for v in f.adj_var_nodes:\n",
    "        if v not in prior_vars:\n",
    "            prior_vars.append(v)\n",
    "supergraph.compute_all_messages(prior_factors, local_relin=True)\n",
    "supergraph.update_all_beliefs(prior_vars)\n",
    "\n",
    "energy = supergraph.energy_map(include_priors=True, include_factors=True)\n",
    "print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")\n",
    "\"\"\"\n",
    "supergraph = layers[-2][\"graph\"]\n",
    "for it in range(100):\n",
    "    supergraph.synchronous_iteration()\n",
    "    energy = supergraph.energy_map(include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bccbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.34553367  3.38894029 -2.90695587 12.40028538  6.31483732 23.53412873\n",
      " -5.19983453 27.30061548]\n",
      "[-1.34553367  3.38894029 -2.90695587 12.40028538  6.31483732 23.53412873\n",
      " -5.19983453 27.30061548]\n"
     ]
    }
   ],
   "source": [
    "absgraph = layers[-1][\"graph\"]\n",
    "\n",
    "#print(Bs[0] @ absgraph.var_nodes[0].mu + ks[0])\n",
    "print(layers[-2][\"graph\"].var_nodes[0].mu)\n",
    "\n",
    "#print(absgraph.var_nodes[0].belief.eta)\n",
    "#print(absgraph.var_nodes[0].belief.lam)\n",
    "\n",
    "#absgraph.update_all_beliefs(absgraph.var_nodes)\n",
    "for i in range(100):\n",
    "    absgraph.compute_all_messages(absgraph.factors, local_relin=True)\n",
    "    absgraph.update_all_beliefs(absgraph.var_nodes)\n",
    "\n",
    "\n",
    "print(Bs[0] @ absgraph.var_nodes[0].mu + ks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65b2ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15158182 -0.12118028  0.220961    0.08883329  0.30506051  0.44922781]\n",
      "[[ 0.03714286  0.         -0.01        0.         -0.01        0.        ]\n",
      " [ 0.          0.03714286  0.         -0.01        0.         -0.01      ]\n",
      " [-0.01        0.          0.03        0.         -0.01        0.        ]\n",
      " [ 0.         -0.01        0.          0.03        0.         -0.01      ]\n",
      " [-0.01        0.         -0.01        0.          0.03        0.        ]\n",
      " [ 0.         -0.01        0.         -0.01        0.          0.03      ]]\n"
     ]
    }
   ],
   "source": [
    "print(supergraph.var_nodes[1].belief.eta)\n",
    "print(supergraph.var_nodes[1].belief.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67cb6111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02997088, 0.        ],\n",
       "       [0.        , 0.02997088]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basegraph.factors[6].adj_beliefs[0].lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38ca646c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02997088, 0.        ],\n",
       "       [0.        , 0.02997088]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basegraph.factors[7].adj_beliefs[0].lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba73f892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.1165681 ,  5.44711511])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(basegraph.factors[6].adj_beliefs[0].lam, basegraph.factors[6].adj_beliefs[0].eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "891cc038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.1165681 ,  5.44711511, 38.15757778, 11.67015352, 40.26006536,\n",
       "       20.68001639])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(supergraph.var_nodes[1].belief.lam, supergraph.var_nodes[1].belief.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c6508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.84210504,  0.        , 18.42105243,  0.        , 18.42105243,\n",
       "         0.        ],\n",
       "       [ 0.        , 36.84210504,  0.        , 18.42105243,  0.        ,\n",
       "        18.42105243],\n",
       "       [18.42105243,  0.        , 46.71052601,  0.        , 21.71052607,\n",
       "         0.        ],\n",
       "       [ 0.        , 18.42105243,  0.        , 46.71052601,  0.        ,\n",
       "        21.71052607],\n",
       "       [18.42105243,  0.        , 21.71052607,  0.        , 46.71052601,\n",
       "         0.        ],\n",
       "       [ 0.        , 18.42105243,  0.        , 21.71052607,  0.        ,\n",
       "        46.71052601]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(supergraph.var_nodes[1].belief.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c563c190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.36571711,  0.        ],\n",
       "       [ 0.        , 33.36571711]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(basegraph.factors[6].adj_beliefs[0].lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52d54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8cda2b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.686408732643704"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(layers[-1][\"graph\"].var_nodes[0].GT - (Bs[layers[-1][\"graph\"].var_nodes[0].variableID] @ layers[-1][\"graph\"].var_nodes[0].mu + ks[layers[-1][\"graph\"].var_nodes[0].variableID]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "64b1b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.507938987468407"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(layers[-1][\"graph\"].var_nodes[1].GT - (Bs[layers[-1][\"graph\"].var_nodes[1].variableID] @ layers[-1][\"graph\"].var_nodes[1].mu + ks[layers[-1][\"graph\"].var_nodes[1].variableID]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9572fcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.35569060552677"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(16.507938987308457**2 + 10.68640873270132**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2b345c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = supergraph.factors[2].jac_fn(np.concatenate([np.array([1,2,1,2,1,2,3,4]), np.array([0,1,2,3,4,5])])\n",
    ")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b18722b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.99,  0.  ,  0.99,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.99,  0.  ,\n",
       "         0.99,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.99,  0.  , -0.99,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.99,  0.  ,\n",
       "        -0.99,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supergraph.factors[2].factor.lam - a.T@a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f8895f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.34553367,  3.38894029, -2.90695587, 12.40028538,  6.31483732,\n",
       "       23.53412873, -5.19983453, 27.30061548])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[-2][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8507ed0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.34553367,  3.38894029, -2.90695587, 12.40028538,  6.31483732,\n",
       "       23.53412873, -5.19983453, 27.30061548])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supergraph.var_nodes[0].mu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
