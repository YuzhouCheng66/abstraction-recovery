{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "db67ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dash\n",
    "from dash import html, dcc, Input, Output, State, no_update\n",
    "import dash_cytoscape as cyto\n",
    "import numpy as np\n",
    "from scipy.linalg import block_diag\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "# ==== GBP import ====\n",
    "from gbp.gbp import *\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"Factor Graph SVD Abs&Recovery\"\n",
    "\n",
    "\n",
    "\n",
    "def update_super_graph_linearized(layers, eta_damping=0.4):\n",
    "    \"\"\"\n",
    "    Construct the super graph based on the base graph in layers[-2] and the super-grouping in layers[-1].\n",
    "    Requirement: layers[-2][\"graph\"] is an already-built base graph (with unary/binary factors).\n",
    "    layers[-1][\"node_map\"]: { base_node_id (str, e.g., 'b12') -> super_node_id (str) }\n",
    "    \"\"\"\n",
    "    # ---------- Extract base & super ----------\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_nodes = layers[-1][\"nodes\"]\n",
    "    super_edges = layers[-1][\"edges\"]\n",
    "    node_map    = layers[-1][\"node_map\"]   # 'bN' -> 'sX_...'\n",
    "\n",
    "    # base: id(int) -> VariableNode, handy to query dofs and mu\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    # ---------- super_id -> [base_id(int)] ----------\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "\n",
    "    # ---------- For each super group, build a (start, dofs) table ----------\n",
    "    # local_idx[sid][bid] = (start, dofs), total_dofs[sid] = sum(dofs)\n",
    "    local_idx   = {}\n",
    "    total_dofs  = {}\n",
    "    for sid, group in super_groups.items():\n",
    "        off = 0\n",
    "        local_idx[sid] = {}\n",
    "        for bid in group:\n",
    "            d = id2var[bid].dofs\n",
    "            local_idx[sid][bid] = (off, d)\n",
    "            off += d\n",
    "        total_dofs[sid] = off\n",
    "\n",
    "\n",
    "    # ---------- Create super VariableNodes ----------\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "\n",
    "    super_var_nodes = {}\n",
    "    for i, sn in enumerate(super_nodes):\n",
    "        sid = sn[\"data\"][\"id\"]\n",
    "        dofs = total_dofs.get(sid, 0)\n",
    "\n",
    "        v = VariableNode(i, dofs=dofs)\n",
    "        gt_vec = np.zeros(dofs)\n",
    "        mu_blocks = []\n",
    "        Sigma_blocks = []\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            # === Stack base GT ===\n",
    "            gt_base = getattr(id2var[bid], \"GT\", None)\n",
    "            if gt_base is None or len(gt_base) != d:\n",
    "                gt_base = np.zeros(d)\n",
    "            gt_vec[st:st+d] = gt_base\n",
    "\n",
    "            # === Stack base belief ===\n",
    "            vb = id2var[bid]\n",
    "            mu_blocks.append(vb.mu)\n",
    "            Sigma_blocks.append(vb.Sigma)\n",
    "\n",
    "        super_var_nodes[sid] = v\n",
    "        v.GT = gt_vec\n",
    "\n",
    "        mu_super = np.concatenate(mu_blocks) if mu_blocks else np.zeros(dofs)\n",
    "        Sigma_super = block_diag(*Sigma_blocks) if Sigma_blocks else np.eye(dofs)\n",
    "        lam = np.linalg.inv(Sigma_super)\n",
    "        eta = lam @ mu_super\n",
    "        v.mu = mu_super\n",
    "        v.Sigma = Sigma_super\n",
    "        v.belief = NdimGaussian(dofs, eta, lam)\n",
    "        v.prior.lam = 1e-10 * lam\n",
    "        v.prior.eta = 1e-10 * eta\n",
    "\n",
    "        fg.var_nodes.append(v)\n",
    "\n",
    "\n",
    "    fg.n_var_nodes = len(fg.var_nodes)\n",
    "\n",
    "    # ---------- Utility: assemble a group's linpoint (using base belief means) ----------\n",
    "    def make_linpoint_for_group(sid):\n",
    "        x = np.zeros(total_dofs[sid])\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            mu = getattr(id2var[bid], \"mu\", None)\n",
    "            if mu is None or len(mu) != d:\n",
    "                mu = np.zeros(d)\n",
    "            x[st:st+d] = mu\n",
    "        return x\n",
    "\n",
    "    # ---------- 3) super prior (in-group unary + in-group binary) ----------\n",
    "    def make_super_prior_factor(sid, lin0, base_factors):\n",
    "        group = super_groups[sid]\n",
    "        idx_map = local_idx[sid]\n",
    "        ncols = total_dofs[sid]\n",
    "\n",
    "        # Select factors whose variables are all within the group (unary or binary)\n",
    "        in_group = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if all(vid in group for vid in vids):\n",
    "                in_group.append(f)\n",
    "\n",
    "        def jac_fn_super_prior(x_super, *args):\n",
    "            Jrows = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # Build this factor's local x for (potentially) nonlinear Jacobian\n",
    "                x_loc_list = []\n",
    "                dims = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    dims.append(d)\n",
    "                    x_loc_list.append(lin0[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "                # Map Jloc column blocks back to the super variable columns\n",
    "                row = np.zeros((Jloc.shape[0], ncols))\n",
    "                c0 = 0\n",
    "                for vid, d in zip(vids, dims):\n",
    "                    st, _ = idx_map[vid]\n",
    "                    row[:, st:st+d] = Jloc[:, c0:c0+d]\n",
    "                    c0 += d\n",
    "\n",
    "                Jrows.append(row)\n",
    "            return np.vstack(Jrows) if Jrows else np.zeros((0, ncols))\n",
    "\n",
    "        J = jac_fn_super_prior(x_super=lin0)\n",
    "        meas_fn = []\n",
    "        for f in in_group:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            # Build this factor's local x for (potentially) nonlinear Jacobian\n",
    "            x_loc_list = []\n",
    "            dims = []\n",
    "            for vid in vids:\n",
    "                st, d = idx_map[vid]\n",
    "                dims.append(d)\n",
    "                x_loc_list.append(lin0[st:st+d])\n",
    "            x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "            meas_fn.append(f.meas_fn(x_loc))\n",
    "        meas_fn = np.concatenate(meas_fn) \n",
    "        def meas_fn_super_prior(x_super, *args):\n",
    "            return meas_fn + J@(x_super-lin0)\n",
    "\n",
    "        # z_super: concatenate each base factor's z\n",
    "        z_list = [f.measurement for f in in_group]\n",
    "        z_lambda_list = [f.measurement_lambda for f in in_group]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_prior, jac_fn_super_prior, z_super, z_super_lambda \n",
    "\n",
    "    # ---------- 4) super between (cross-group binary) ----------\n",
    "    def make_super_between_factor(sidA, sidB, lin0, base_factors):\n",
    "        groupA, groupB = super_groups[sidA], super_groups[sidB]\n",
    "        idxA, idxB = local_idx[sidA], local_idx[sidB]\n",
    "        nA, nB = total_dofs[sidA], total_dofs[sidB]\n",
    "\n",
    "        cross = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if len(vids) != 2:\n",
    "                continue\n",
    "            i, j = vids\n",
    "            # One side in A, the other side in B\n",
    "            if (i in groupA and j in groupB) or (i in groupB and j in groupA):\n",
    "                cross.append(f)\n",
    "\n",
    "        \n",
    "        def jac_fn_super_between(xAB, *args):\n",
    "            xA, xB = lin0[:nA], lin0[nA:]\n",
    "            Jrows = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                    left_start, right_start = si, nA + sj\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                    left_start, right_start = nA + si, sj\n",
    "\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "                row = np.zeros((Jloc.shape[0], nA + nB))\n",
    "                row[:, left_start:left_start+di]   = Jloc[:, :di] \n",
    "                row[:, right_start:right_start+dj] = Jloc[:, di:di+dj] \n",
    "                Jrows.append(row)\n",
    "\n",
    "            return np.vstack(Jrows) \n",
    "        \n",
    "        J = jac_fn_super_between(xAB=lin0)\n",
    "        xA, xB = lin0[:nA], lin0[nA:]\n",
    "        meas_fn = []\n",
    "        for f in cross:\n",
    "            i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "            if i in groupA:\n",
    "                si, di = idxA[i]\n",
    "                sj, dj = idxB[j]\n",
    "                xi = xA[si:si+di]\n",
    "                xj = xB[sj:sj+dj]\n",
    "            else:\n",
    "                si, di = idxB[i]\n",
    "                sj, dj = idxA[j]\n",
    "                xi = xB[si:si+di]\n",
    "                xj = xA[sj:sj+dj]\n",
    "            x_loc = np.concatenate([xi, xj])\n",
    "            meas_fn.append(f.meas_fn(x_loc))\n",
    "        meas_fn = np.concatenate(meas_fn) \n",
    "        def meas_fn_super_between(xAB, *args):\n",
    "            return meas_fn + J@(xAB-lin0)\n",
    "\n",
    "        z_list = [f.measurement for f in cross]\n",
    "        z_lambda_list = [f.measurement_lambda for f in cross]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_between, jac_fn_super_between, z_super, z_super_lambda\n",
    "\n",
    "\n",
    "    for e in super_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if v == \"prior\":\n",
    "            lin0 = make_linpoint_for_group(u)\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_prior_factor(u, lin0, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_prior\"\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            \n",
    "        else:\n",
    "            lin0 = np.concatenate([make_linpoint_for_group(u), make_linpoint_for_group(v)])\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_between_factor(u, v, lin0, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u], super_var_nodes[v]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_between\"\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            super_var_nodes[v].adj_factors.append(f)\n",
    "\n",
    "\n",
    "    fg.n_factor_nodes = len(fg.factors)\n",
    "    return fg\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# SLAM-like base graph\n",
    "# -----------------------\n",
    "def make_slam_like_graph(N=100, step_size=25, loop_prob=0.05, loop_radius=50, prior_prop=0.0, seed=None):\n",
    "    if seed is None :\n",
    "        rng = np.random.default_rng()  # ✅ Ensure we have an RNG\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    nodes, edges = [], []\n",
    "    positions = []\n",
    "    x, y = 0.0, 0.0\n",
    "    positions.append((x, y))\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: trajectory generation\n",
    "    for _ in range(1, int(N)):\n",
    "        dx, dy = rng.standard_normal(2)  # replace np.random.randn\n",
    "        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "        dx, dy = dx / norm * float(step_size), dy / norm * float(step_size)\n",
    "        x, y = x + dx, y + dy\n",
    "        positions.append((x, y))\n",
    "\n",
    "    # Sequential edges along the path\n",
    "    for i, (px, py) in enumerate(positions):\n",
    "        nodes.append({\n",
    "            \"data\": {\"id\": f\"{i}\", \"layer\": 0, \"dim\": 2, \"num_base\": 1},\n",
    "            \"position\": {\"x\": float(px), \"y\": float(py)}\n",
    "        })\n",
    "\n",
    "    for i in range(int(N) - 1):\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{i+1}\"}})\n",
    "\n",
    "    # ✅ Deterministic-by-RNG: loop-closure edges\n",
    "    for i in range(int(N)):\n",
    "        for j in range(i + 5, int(N)):\n",
    "            if rng.random() < float(loop_prob):  # replace np.random.rand\n",
    "                xi, yi = positions[i]\n",
    "                xj, yj = positions[j]\n",
    "                if np.hypot(xi - xj, yi - yj) < float(loop_radius):\n",
    "                    edges.append({\"data\": {\"source\": f\"{i}\", \"target\": f\"{j}\"}})\n",
    "\n",
    "    # ✅ Sample priors using the same RNG\n",
    "    if prior_prop <= 0.0:\n",
    "        strong_ids = {0}\n",
    "    elif prior_prop >= 1.0:\n",
    "        strong_ids = set(range(N))\n",
    "    else:\n",
    "        k = max(1, int(np.floor(prior_prop * N)))\n",
    "        strong_ids = set(rng.choice(N, size=k, replace=False).tolist())\n",
    "\n",
    "    # Add edges for nodes with strong priors\n",
    "    for i in strong_ids:\n",
    "        edges.append({\"data\": {\"source\": f\"{i}\", \"target\": \"prior\"}})\n",
    "\n",
    "    edges.append({\"data\": {\"source\": f\"{0}\", \"target\": \"anchor\"}}) \n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Grid aggregation\n",
    "# -----------------------\n",
    "def fuse_to_super_grid(prev_nodes, prev_edges, gx, gy, layer_idx):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    xmin, ymin = positions.min(axis=0); xmax, ymax = positions.max(axis=0)\n",
    "    cell_w = (xmax - xmin) / gx if gx > 0 else 1.0\n",
    "    cell_h = (ymax - ymin) / gy if gy > 0 else 1.0\n",
    "    if cell_w == 0: cell_w = 1.0\n",
    "    if cell_h == 0: cell_h = 1.0\n",
    "    cell_map = {}\n",
    "    for idx, n in enumerate(prev_nodes):\n",
    "        x, y = n[\"position\"][\"x\"], n[\"position\"][\"y\"]\n",
    "        cx = min(int((x - xmin) / cell_w), gx - 1)\n",
    "        cy = min(int((y - ymin) / cell_h), gy - 1)\n",
    "        cid = cx + cy * gx\n",
    "        cell_map.setdefault(cid, []).append(idx)\n",
    "    super_nodes, node_map = [], {}\n",
    "    for cid, indices in cell_map.items():\n",
    "        pts = positions[indices]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in indices]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in indices]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums))\n",
    "        nid = str(len(super_nodes))\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val   # Inherit the sum\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in indices:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            elif su == sv:\n",
    "                eid = tuple(sorted((su, \"prior\")))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = tuple(sorted((su, \"prior\")))\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "# -----------------------\n",
    "# K-Means aggregation\n",
    "# -----------------------\n",
    "def fuse_to_super_kmeans(prev_nodes, prev_edges, k, layer_idx, max_iters=20, tol=1e-6, seed=0):\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "    n = positions.shape[0]\n",
    "    if k <= 0: \n",
    "        k = 1\n",
    "    k = min(k, n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # -------- Improved initialization --------\n",
    "    # Randomly sample k points without replacement to ensure each cluster starts with a distinct point\n",
    "    init_idx = rng.choice(n, size=k, replace=False)\n",
    "    centers = positions[init_idx]\n",
    "\n",
    "    # Lloyd iterations\n",
    "    for _ in range(max_iters):\n",
    "        d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "        assign = np.argmin(d2, axis=1)\n",
    "\n",
    "        # -------- Empty-cluster fix --------\n",
    "        counts = np.bincount(assign, minlength=k)\n",
    "        empty_clusters = np.where(counts == 0)[0]\n",
    "        for ci in empty_clusters:\n",
    "            # Find the largest cluster\n",
    "            big_cluster = np.argmax(counts)\n",
    "            big_idxs = np.where(assign == big_cluster)[0]\n",
    "            # Steal one point over\n",
    "            steal_idx = big_idxs[0]\n",
    "            assign[steal_idx] = ci\n",
    "            counts[big_cluster] -= 1\n",
    "            counts[ci] += 1\n",
    "\n",
    "        moved = 0.0\n",
    "        for ci in range(k):\n",
    "            idxs = np.where(assign == ci)[0]\n",
    "            new_c = positions[idxs].mean(axis=0)\n",
    "            moved = max(moved, float(np.linalg.norm(new_c - centers[ci])))\n",
    "            centers[ci] = new_c\n",
    "        if moved < tol:\n",
    "            break\n",
    "\n",
    "    # Final assign (redo once to be safe)\n",
    "    d2 = ((positions[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
    "    assign = np.argmin(d2, axis=1)\n",
    "\n",
    "    counts = np.bincount(assign, minlength=k)\n",
    "    empty_clusters = np.where(counts == 0)[0]\n",
    "    for ci in empty_clusters:\n",
    "        big_cluster = np.argmax(counts)\n",
    "        big_idxs = np.where(assign == big_cluster)[0]\n",
    "        steal_idx = big_idxs[0]\n",
    "        assign[steal_idx] = ci\n",
    "        counts[big_cluster] -= 1\n",
    "        counts[ci] += 1\n",
    "\n",
    "    # ---------- Build the super graph ----------\n",
    "    super_nodes, node_map = [], {}\n",
    "    for ci in range(k):\n",
    "        idxs = np.where(assign == ci)[0]\n",
    "        pts = positions[idxs]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in idxs]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in idxs]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums)) \n",
    "        nid = f\"{ci}\"\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val   # Inherit the sum\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        for i in idxs:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            else:\n",
    "                eid = (su, \"prior\")\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = (su, \"prior\")\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "\n",
    "def copy_to_abs(super_nodes, super_edges, layer_idx):\n",
    "    abs_nodes = []\n",
    "    for n in super_nodes:\n",
    "        nid = n[\"data\"][\"id\"].replace(\"s\", \"a\", 1)\n",
    "        abs_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": n[\"data\"][\"dim\"],\n",
    "                \"num_base\": n[\"data\"].get(\"num_base\", 1)  # Inherit\n",
    "            },\n",
    "            \"position\": {\"x\": n[\"position\"][\"x\"], \"y\": n[\"position\"][\"y\"]}\n",
    "        })\n",
    "    abs_edges = []\n",
    "    for e in super_edges:\n",
    "        abs_edges.append({\"data\": {\n",
    "            \"source\": e[\"data\"][\"source\"].replace(\"s\", \"a\", 1),\n",
    "            \"target\": e[\"data\"][\"target\"].replace(\"s\", \"a\", 1)\n",
    "        }})\n",
    "    return abs_nodes, abs_edges\n",
    "\n",
    "# -----------------------\n",
    "# Sequential merge (tail group absorbs remainder)\n",
    "# -----------------------\n",
    "def fuse_to_super_order(prev_nodes, prev_edges, k, layer_idx, tail_heavy=True):\n",
    "    \"\"\"\n",
    "    Sequentially split prev_nodes in current order into k groups; the last group absorbs the remainder (tail_heavy=True).\n",
    "    Reuse existing rules for aggregating dim/num_base, deduplicating edges, and propagating prior.\n",
    "    \"\"\"\n",
    "    n = len(prev_nodes)\n",
    "    if k <= 0: k = 1\n",
    "    k = min(k, n)\n",
    "\n",
    "    # Group sizes\n",
    "    base = n // k\n",
    "    rem  = n %  k\n",
    "    if rem > 0:\n",
    "        sizes = [k]*(base) + [rem]     # Tail absorbs remainder: ..., last += rem\n",
    "    else:\n",
    "        sizes = [k]*(base)\n",
    "\n",
    "    # Build groups: record indices per group\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for s in sizes:\n",
    "        groups.append(list(range(start, start+s)))\n",
    "        start += s\n",
    "\n",
    "    # ---- Build super_nodes & node_map ----\n",
    "    positions = np.array([[n[\"position\"][\"x\"], n[\"position\"][\"y\"]] for n in prev_nodes], dtype=float)\n",
    "\n",
    "    super_nodes, node_map = [], {}\n",
    "    for gi, idxs in enumerate(groups):\n",
    "        pts = positions[idxs]\n",
    "        mean_x, mean_y = pts.mean(axis=0)\n",
    "\n",
    "        child_dims = [prev_nodes[i][\"data\"][\"dim\"] for i in idxs]\n",
    "        child_nums = [prev_nodes[i][\"data\"].get(\"num_base\", 1) for i in idxs]\n",
    "        dim_val = int(max(1, sum(child_dims)))\n",
    "        num_val = int(sum(child_nums))\n",
    "\n",
    "        nid = f\"{gi}\"  # Same as kmeans: use group index as id (string)\n",
    "        super_nodes.append({\n",
    "            \"data\": {\n",
    "                \"id\": nid,\n",
    "                \"layer\": layer_idx,\n",
    "                \"dim\": dim_val,\n",
    "                \"num_base\": num_val\n",
    "            },\n",
    "            \"position\": {\"x\": float(mean_x), \"y\": float(mean_y)}\n",
    "        })\n",
    "        # Build base-id -> super-id mapping (note: ids are strings throughout)\n",
    "        for i in idxs:\n",
    "            node_map[prev_nodes[i][\"data\"][\"id\"]] = nid\n",
    "\n",
    "    # ---- Super edges: keep and deduplicate inter-group edges; intra-group edges collapse to prior; prior edges roll up to their owning super ----\n",
    "    super_edges, seen = [], set()\n",
    "    for e in prev_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if (v != \"prior\") and (v != \"anchor\"):\n",
    "            su, sv = node_map[u], node_map[v]\n",
    "            if su != sv:\n",
    "                eid = tuple(sorted((su, sv)))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": sv}})\n",
    "                    seen.add(eid)\n",
    "            else:\n",
    "                # Intra-group pairwise edge → group prior (consistent with grid/kmeans handling)\n",
    "                eid = tuple(sorted((su, \"prior\")))\n",
    "                if eid not in seen:\n",
    "                    super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                    seen.add(eid)\n",
    "        else:\n",
    "            su = node_map[u]\n",
    "            eid = tuple(sorted((su, \"prior\")))\n",
    "            if eid not in seen:\n",
    "                super_edges.append({\"data\": {\"source\": su, \"target\": \"prior\"}})\n",
    "                seen.add(eid)\n",
    "\n",
    "    return super_nodes, super_edges, node_map\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Tools\n",
    "# -----------------------\n",
    "def parse_layer_name(name):\n",
    "    if name == \"base\": return (\"base\", 0)\n",
    "    m = re.match(r\"(super|abs)(\\d+)$\", name)\n",
    "    return (m.group(1), int(m.group(2))) if m else (\"base\", 0)\n",
    "\n",
    "def highest_pair_idx(names):\n",
    "    hi = 0\n",
    "    for nm in names:\n",
    "        kind, k = parse_layer_name(nm)\n",
    "        if kind in (\"super\",\"abs\"): hi = max(hi, k)\n",
    "    return hi\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Initialization & Boundary\n",
    "# -----------------------\n",
    "def init_layers(N=100, step_size=25, loop_prob=0.05, loop_radius=50, prior_prop=0.0, seed=None):\n",
    "    base_nodes, base_edges = make_slam_like_graph(N, step_size, loop_prob, loop_radius, prior_prop, seed)\n",
    "    return [{\"name\": \"base\", \"nodes\": base_nodes, \"edges\": base_edges}]\n",
    "\n",
    "VIEW_W, VIEW_H = 960, 600\n",
    "ASPECT = VIEW_W / VIEW_H\n",
    "AXIS_PAD=20.0\n",
    "# ==== Blobal Status ====\n",
    "layers = init_layers()\n",
    "\n",
    "\n",
    "def adjust_bounds_to_aspect(xmin, xmax, ymin, ymax, aspect):\n",
    "    cx=(xmin+xmax)/2; cy=(ymin+ymax)/2\n",
    "    dx=xmax-xmin; dy=ymax-ymin\n",
    "    if dx<=0: dx=1\n",
    "    if dy<=0: dy=1\n",
    "    if dx/dy > aspect:\n",
    "        dy_new=dx/aspect\n",
    "        return xmin,xmax,cy-dy_new/2,cy+dy_new/2\n",
    "    else:\n",
    "        dx_new=dy*aspect\n",
    "        return cx-dx_new/2,cx+dx_new/2,ymin,ymax\n",
    "\n",
    "def reset_global_bounds(base_nodes):\n",
    "    global GLOBAL_XMIN, GLOBAL_XMAX, GLOBAL_YMIN, GLOBAL_YMAX\n",
    "    global GLOBAL_XMIN_ADJ, GLOBAL_XMAX_ADJ, GLOBAL_YMIN_ADJ, GLOBAL_YMAX_ADJ\n",
    "    xs=[n[\"position\"][\"x\"] for n in base_nodes] or [0.0]\n",
    "    ys=[n[\"position\"][\"y\"] for n in base_nodes] or [0.0]\n",
    "    GLOBAL_XMIN,GLOBAL_XMAX=min(xs),max(xs)\n",
    "    GLOBAL_YMIN,GLOBAL_YMAX=min(ys),max(ys)\n",
    "    GLOBAL_XMIN_ADJ,GLOBAL_XMAX_ADJ,GLOBAL_YMIN_ADJ,GLOBAL_YMAX_ADJ=adjust_bounds_to_aspect(\n",
    "        GLOBAL_XMIN,GLOBAL_XMAX,GLOBAL_YMIN,GLOBAL_YMAX,ASPECT)\n",
    "\n",
    "# ==== Blobal Status ====\n",
    "layers = init_layers()\n",
    "pair_idx = 0\n",
    "reset_global_bounds(layers[0][\"nodes\"])\n",
    "gbp_graph = None\n",
    "\n",
    "# -----------------------\n",
    "# GBP Graph Construction\n",
    "# -----------------------\n",
    "def build_noisy_pose_graph(\n",
    "    nodes,\n",
    "    edges,\n",
    "    prior_sigma: float = 10,\n",
    "    odom_sigma: float = 10,\n",
    "    tiny_prior: float = 1e-12,\n",
    "    seed=None,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Construct a 2D pose-only factor graph (linear, Gaussian) and inject noise.\n",
    "    Parameters:\n",
    "      prior_sigma : standard deviation of the strong prior (smaller = stronger)\n",
    "      odom_sigma  : standard deviation of odometry measurement noise\n",
    "      prior_prop  : 0.0 = anchor only; (0,1) = randomly select by proportion; >=1.0 = all\n",
    "      tiny_prior  : a tiny prior added to all nodes to prevent singularity\n",
    "      seed        : random seed (for reproducibility)\n",
    "    \"\"\"\n",
    "\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=0)\n",
    "\n",
    "    var_nodes = []\n",
    "    I2 = np.eye(2, dtype=float)\n",
    "    N = len(nodes)\n",
    "\n",
    "    # ---- Pre-generate noise ----\n",
    "    prior_noises = {}\n",
    "    odom_noises = {}\n",
    "\n",
    "    if seed is None:\n",
    "        rng = np.random.default_rng()\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Generate noise for all edges\n",
    "    for e in edges:\n",
    "        src = e[\"data\"][\"source\"]; dst = e[\"data\"][\"target\"]\n",
    "        # Binary edge\n",
    "        if (dst != \"prior\") and (dst != \"anchor\"):\n",
    "            odom_noises[(int(src[:]), int(dst[:]))] = rng.normal(0.0, odom_sigma, size=2)\n",
    "        # Unary edge (strong prior)\n",
    "        elif dst == \"prior\":\n",
    "            prior_noises[int(src[:])] = rng.normal(0.0, prior_sigma, size=2)\n",
    "\n",
    "\n",
    "    # ---- variable nodes ----\n",
    "    for i, n in enumerate(nodes):\n",
    "        v = VariableNode(i, dofs=2)\n",
    "        v.GT = np.array([n[\"position\"][\"x\"], n[\"position\"][\"y\"]], dtype=float)\n",
    "\n",
    "        # Tiny prior\n",
    "        v.prior.lam = tiny_prior * I2\n",
    "        v.prior.eta = np.zeros(2, dtype=float)\n",
    "\n",
    "        var_nodes.append(v)\n",
    "\n",
    "    fg.var_nodes = var_nodes\n",
    "    fg.n_var_nodes = len(var_nodes)\n",
    "\n",
    "\n",
    "    # ---- prior factors ----\n",
    "    def meas_fn_unary(x, *args):\n",
    "        return x\n",
    "    def jac_fn_unary(x, *args):\n",
    "        return np.eye(2)\n",
    "    # ---- odometry factors ----\n",
    "    def meas_fn(xy, *args):\n",
    "        return xy[2:] - xy[:2]\n",
    "    def jac_fn(xy, *args):\n",
    "        return np.array([[-1, 0, 1, 0],\n",
    "                         [ 0,-1, 0, 1]], dtype=float)\n",
    "    \n",
    "    factors = []\n",
    "    fid = 0\n",
    "\n",
    "    for e in edges:\n",
    "        src = e[\"data\"][\"source\"]; dst = e[\"data\"][\"target\"]\n",
    "        if (dst != \"prior\") and (dst != \"anchor\"):\n",
    "            i, j = int(src[:]), int(dst[:])\n",
    "            vi, vj = var_nodes[i], var_nodes[j]\n",
    "\n",
    "            meas = (vj.GT - vi.GT) + odom_noises[(i, j)]\n",
    "\n",
    "            meas_lambda = np.eye(len(meas))/ (odom_sigma**2)\n",
    "            f = Factor(fid, [vi, vj], meas, meas_lambda, meas_fn, jac_fn)\n",
    "            f.type = \"base\"\n",
    "            linpoint = np.r_[vi.GT, vj.GT]\n",
    "            f.compute_factor(linpoint=linpoint, update_self=True)\n",
    "\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            vj.adj_factors.append(f)\n",
    "            fid += 1\n",
    "\n",
    "        elif dst == \"prior\":\n",
    "            i = int(src[:])\n",
    "            vi = var_nodes[i]\n",
    "            z = vi.GT + prior_noises[i]\n",
    "\n",
    "            z_lambda = np.eye(len(meas))/ (prior_sigma**2)\n",
    "            f = Factor(fid, [vi], z, z_lambda, meas_fn_unary, jac_fn_unary)\n",
    "            f.type = \"prior\"\n",
    "            f.compute_factor(linpoint=z, update_self=True)\n",
    "\n",
    "            factors.append(f)\n",
    "            vi.adj_factors.append(f)\n",
    "            fid += 1\n",
    "\n",
    "    # anchor for initial position\n",
    "    v0 = var_nodes[0]\n",
    "    z = v0.GT\n",
    "\n",
    "    z_lambda = np.eye(len(meas))/ ((1e-4)**2)\n",
    "    f = Factor(fid, [v0], z, z_lambda, meas_fn_unary, jac_fn_unary)\n",
    "    f.type = \"prior\"\n",
    "    f.compute_factor(linpoint=z, update_self=True)\n",
    "\n",
    "    factors.append(f)\n",
    "    v0.adj_factors.append(f)\n",
    "    fid += 1\n",
    "\n",
    "    fg.factors = factors\n",
    "    fg.n_factor_nodes = len(factors)\n",
    "    return fg\n",
    "\n",
    "\n",
    "def build_super_graph(layers, eta_damping=0.4):\n",
    "    \"\"\"\n",
    "    Construct the super graph based on the base graph in layers[-2] and the super-grouping in layers[-1].\n",
    "    Requirement: layers[-2][\"graph\"] is an already-built base graph (with unary/binary factors).\n",
    "    layers[-1][\"node_map\"]: { base_node_id (str, e.g., 'b12') -> super_node_id (str) }\n",
    "    \"\"\"\n",
    "    # ---------- Extract base & super ----------\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_nodes = layers[-1][\"nodes\"]\n",
    "    super_edges = layers[-1][\"edges\"]\n",
    "    node_map    = layers[-1][\"node_map\"]   # 'bN' -> 'sX_...'\n",
    "\n",
    "    # base: id(int) -> VariableNode, handy to query dofs and mu\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    # ---------- super_id -> [base_id(int)] ----------\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "\n",
    "    # ---------- For each super group, build a (start, dofs) table ----------\n",
    "    # local_idx[sid][bid] = (start, dofs), total_dofs[sid] = sum(dofs)\n",
    "    local_idx   = {}\n",
    "    total_dofs  = {}\n",
    "    for sid, group in super_groups.items():\n",
    "        off = 0\n",
    "        local_idx[sid] = {}\n",
    "        for bid in group:\n",
    "            d = id2var[bid].dofs\n",
    "            local_idx[sid][bid] = (off, d)\n",
    "            off += d\n",
    "        total_dofs[sid] = off\n",
    "\n",
    "\n",
    "    # ---------- Create super VariableNodes ----------\n",
    "    fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "\n",
    "    super_var_nodes = {}\n",
    "    for i, sn in enumerate(super_nodes):\n",
    "        sid = sn[\"data\"][\"id\"]\n",
    "        dofs = total_dofs.get(sid, 0)\n",
    "\n",
    "        v = VariableNode(i, dofs=dofs)\n",
    "        gt_vec = np.zeros(dofs)\n",
    "        mu_blocks = []\n",
    "        Sigma_blocks = []\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            # === Stack base GT ===\n",
    "            gt_base = getattr(id2var[bid], \"GT\", None)\n",
    "            if gt_base is None or len(gt_base) != d:\n",
    "                gt_base = np.zeros(d)\n",
    "            gt_vec[st:st+d] = gt_base\n",
    "\n",
    "            # === Stack base belief ===\n",
    "            vb = id2var[bid]\n",
    "            mu_blocks.append(vb.mu)\n",
    "            Sigma_blocks.append(vb.Sigma)\n",
    "\n",
    "        super_var_nodes[sid] = v\n",
    "        v.GT = gt_vec\n",
    "\n",
    "        mu_super = np.concatenate(mu_blocks) if mu_blocks else np.zeros(dofs)\n",
    "        Sigma_super = block_diag(*Sigma_blocks) if Sigma_blocks else np.eye(dofs)\n",
    "        lam = np.linalg.inv(Sigma_super)\n",
    "        eta = lam @ mu_super\n",
    "        v.mu = mu_super\n",
    "        v.Sigma = Sigma_super\n",
    "        v.belief = NdimGaussian(dofs, eta, lam)\n",
    "        v.prior.lam = 1e-12 * lam\n",
    "        v.prior.eta = 1e-12 * eta\n",
    "\n",
    "        fg.var_nodes.append(v)\n",
    "\n",
    "    fg.n_var_nodes = len(fg.var_nodes)\n",
    "\n",
    "    # ---------- Utility: assemble a group's linpoint (using base belief means) ----------\n",
    "    def make_linpoint_for_group(sid):\n",
    "        x = np.zeros(total_dofs[sid])\n",
    "        for bid, (st, d) in local_idx[sid].items():\n",
    "            mu = getattr(id2var[bid], \"mu\", None)\n",
    "            if mu is None or len(mu) != d:\n",
    "                mu = np.zeros(d)\n",
    "            x[st:st+d] = mu\n",
    "        return x\n",
    "\n",
    "    # ---------- 3) super prior (in-group unary + in-group binary) ----------\n",
    "    def make_super_prior_factor(sid, base_factors):\n",
    "        group = super_groups[sid]\n",
    "        idx_map = local_idx[sid]\n",
    "        ncols = total_dofs[sid]\n",
    "\n",
    "        # Select factors whose variables are all within the group (unary or binary)\n",
    "        in_group = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if all(vid in group for vid in vids):\n",
    "                in_group.append(f)\n",
    "\n",
    "        def meas_fn_super_prior(x_super, *args):\n",
    "            meas_fn = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # Assemble this factor's local x\n",
    "                x_loc_list = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) if meas_fn else np.zeros(0)\n",
    "\n",
    "        def jac_fn_super_prior(x_super, *args):\n",
    "            Jrows = []\n",
    "            for f in in_group:\n",
    "                vids = [v.variableID for v in f.adj_var_nodes]\n",
    "                # Build this factor's local x for (potentially) nonlinear Jacobian\n",
    "                x_loc_list = []\n",
    "                dims = []\n",
    "                for vid in vids:\n",
    "                    st, d = idx_map[vid]\n",
    "                    dims.append(d)\n",
    "                    x_loc_list.append(x_super[st:st+d])\n",
    "                x_loc = np.concatenate(x_loc_list) if x_loc_list else np.zeros(0)\n",
    "\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "                \n",
    "                # Map Jloc column blocks back to the super variable columns\n",
    "                row = np.zeros((Jloc.shape[0], ncols))\n",
    "                c0 = 0\n",
    "                for vid, d in zip(vids, dims):\n",
    "                    st, _ = idx_map[vid]\n",
    "                    row[:, st:st+d] = Jloc[:, c0:c0+d]\n",
    "                    c0 += d\n",
    "\n",
    "                Jrows.append(row)\n",
    "            return np.vstack(Jrows) if Jrows else np.zeros((0, ncols))\n",
    "\n",
    "        # z_super: concatenate each base factor's z\n",
    "        z_list = [f.measurement for f in in_group]\n",
    "        z_lambda_list = [f.measurement_lambda for f in in_group]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_prior, jac_fn_super_prior, z_super, z_super_lambda \n",
    "\n",
    "    # ---------- 4) super between (cross-group binary) ----------\n",
    "    def make_super_between_factor(sidA, sidB, base_factors):\n",
    "        groupA, groupB = super_groups[sidA], super_groups[sidB]\n",
    "        idxA, idxB = local_idx[sidA], local_idx[sidB]\n",
    "        nA, nB = total_dofs[sidA], total_dofs[sidB]\n",
    "\n",
    "        cross = []\n",
    "        for f in base_factors:\n",
    "            vids = [v.variableID for v in f.adj_var_nodes]\n",
    "            if len(vids) != 2:\n",
    "                continue\n",
    "            i, j = vids\n",
    "            # One side in A, the other side in B\n",
    "            if (i in groupA and j in groupB) or (i in groupB and j in groupA):\n",
    "                cross.append(f)\n",
    "\n",
    "\n",
    "        def meas_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            meas_fn = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                meas_fn.append(f.meas_fn(x_loc))\n",
    "            return np.concatenate(meas_fn) \n",
    "\n",
    "        def jac_fn_super_between(xAB, *args):\n",
    "            xA, xB = xAB[:nA], xAB[nA:]\n",
    "            Jrows = []\n",
    "            for f in cross:\n",
    "                i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "                if i in groupA:\n",
    "                    si, di = idxA[i]\n",
    "                    sj, dj = idxB[j]\n",
    "                    xi = xA[si:si+di]\n",
    "                    xj = xB[sj:sj+dj]\n",
    "                    left_start, right_start = si, nA + sj\n",
    "                else:\n",
    "                    si, di = idxB[i]\n",
    "                    sj, dj = idxA[j]\n",
    "                    xi = xB[si:si+di]\n",
    "                    xj = xA[sj:sj+dj]\n",
    "                    left_start, right_start = nA + si, sj\n",
    "                x_loc = np.concatenate([xi, xj])\n",
    "                Jloc = f.jac_fn(x_loc)\n",
    "\n",
    "                row = np.zeros((Jloc.shape[0], nA + nB))\n",
    "                row[:, left_start:left_start+di]   = Jloc[:, :di] \n",
    "                row[:, right_start:right_start+dj] = Jloc[:, di:di+dj] \n",
    "\n",
    "                Jrows.append(row)\n",
    "            \n",
    "            return np.vstack(Jrows) \n",
    "\n",
    "        z_list = [f.measurement for f in cross]\n",
    "        z_lambda_list = [f.measurement_lambda for f in cross]\n",
    "        z_super = np.concatenate(z_list) \n",
    "        z_super_lambda = block_diag(*z_lambda_list)\n",
    "\n",
    "        return meas_fn_super_between, jac_fn_super_between, z_super, z_super_lambda\n",
    "\n",
    "\n",
    "    for e in super_edges:\n",
    "        u, v = e[\"data\"][\"source\"], e[\"data\"][\"target\"]\n",
    "\n",
    "        if v == \"prior\":\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_prior_factor(u, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_prior\"\n",
    "            lin0 = make_linpoint_for_group(u)\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            \n",
    "        else:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_super_between_factor(u, v, base_graph.factors)\n",
    "            f = Factor(len(fg.factors), [super_var_nodes[u], super_var_nodes[v]], z, z_lambda, meas_fn, jac_fn)\n",
    "            f.adj_beliefs = [vn.belief for vn in f.adj_var_nodes]\n",
    "            f.type = \"super_between\"\n",
    "            lin0 = np.concatenate([make_linpoint_for_group(u), make_linpoint_for_group(v)])\n",
    "            f.compute_factor(linpoint=lin0, update_self=True)\n",
    "            fg.factors.append(f)\n",
    "            super_var_nodes[u].adj_factors.append(f)\n",
    "            super_var_nodes[v].adj_factors.append(f)\n",
    "\n",
    "\n",
    "    fg.n_factor_nodes = len(fg.factors)\n",
    "    return fg\n",
    "\n",
    "\n",
    "def build_abs_graph(\n",
    "    layers,\n",
    "    r_reduced = 2,\n",
    "    eta_damping=0.4):\n",
    "\n",
    "    abs_var_nodes = {}\n",
    "    Bs = {}\n",
    "    ks = {}\n",
    "    k2s = {}\n",
    "\n",
    "    # === 1. Build Abstraction Variables ===\n",
    "    abs_fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "    sup_fg = layers[-2][\"graph\"]\n",
    "\n",
    "    for sn in sup_fg.var_nodes:\n",
    "        if sn.dofs <= r_reduced:\n",
    "            r = sn.dofs  # No reduction if dofs already <= r\n",
    "        else:\n",
    "            r = r_reduced\n",
    "\n",
    "        sid = sn.variableID\n",
    "        varis_sup_mu = sn.mu\n",
    "        varis_sup_sigma = sn.Sigma\n",
    "        \n",
    "        # Step 1: Eigen decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(varis_sup_sigma)\n",
    "\n",
    "        # Step 2: Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
    "        idx = np.argsort(eigvals)[::-1]      # Get indices of sorted eigenvalues (largest first)\n",
    "        eigvals = eigvals[idx]               # Reorder eigenvalues\n",
    "        eigvecs = eigvecs[:, idx]            # Reorder corresponding eigenvectors\n",
    "\n",
    "        # Step 3: Select the top-k eigenvectors to form the projection matrix (principal subspace)\n",
    "        B_reduced = eigvecs[:, :r]                 # B_reduced: shape (sup_dof, r), projects r to sup_dof\n",
    "        #B_reduced = np.eye(B_reduced.shape[0])\n",
    "        Bs[sid] = B_reduced                        # Store the projection matrix for this variable\n",
    "\n",
    "        # Step 4: Project eta and Lam onto the reduced 2D subspace\n",
    "        varis_abs_mu = B_reduced.T @ varis_sup_mu          # Projected natural mean: shape (2,)\n",
    "        varis_abs_sigma = B_reduced.T @ varis_sup_sigma @ B_reduced  # Projected covariance: shape (2, 2)\n",
    "        ks[sid] = varis_sup_mu - B_reduced @ varis_abs_mu  # Store the mean offset for this variable\n",
    "        #k2s[sid] = varis_sup_sigma - B_reduced @ varis_abs_sigma @ B_reduced.T  # Residual covariance\n",
    "\n",
    "        varis_abs_lam = np.linalg.inv(varis_abs_sigma)  # Inverse covariance (precision matrix): shape (2, 2)\n",
    "        varis_abs_eta = varis_abs_lam @ varis_abs_mu  # Natural parameters: shape (2,)\n",
    "\n",
    "        v = VariableNode(sid, dofs=r)\n",
    "        v.GT = sn.GT\n",
    "        v.prior.lam = 1e-10 * varis_abs_lam\n",
    "        v.prior.eta = 1e-10 * varis_abs_eta\n",
    "        v.mu = varis_abs_mu\n",
    "        v.Sigma = varis_abs_sigma\n",
    "        v.belief = NdimGaussian(r, varis_abs_eta, varis_abs_lam)\n",
    "\n",
    "        abs_var_nodes[sid] = v\n",
    "        abs_fg.var_nodes.append(v)\n",
    "    abs_fg.n_var_nodes = len(abs_fg.var_nodes)\n",
    "\n",
    "\n",
    "    # === 2. Abstract Prior ===\n",
    "    def make_abs_prior_factor(sup_factor):\n",
    "        abs_id = sup_factor.adj_var_nodes[0].variableID\n",
    "        B = Bs[abs_id]\n",
    "        k = ks[abs_id]\n",
    "\n",
    "        def meas_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.meas_fn(B @ x_abs + k)\n",
    "        \n",
    "        def jac_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.jac_fn(B @ x_abs + k) @ B\n",
    "\n",
    "        return meas_fn_abs_prior, jac_fn_abs_prior, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "\n",
    "    # === 3. Abstract Between ===\n",
    "    def make_abs_between_factor(sup_factor):\n",
    "        vids = [v.variableID for v in sup_factor.adj_var_nodes]\n",
    "        i, j = vids # two variable IDs\n",
    "        ni = abs_var_nodes[i].dofs\n",
    "        Bi, Bj = Bs[i], Bs[j]\n",
    "        ki, kj = ks[i], ks[j]                       \n",
    "    \n",
    "\n",
    "        def meas_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            return sup_factor.meas_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "\n",
    "        def jac_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            J_sup = sup_factor.jac_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "            J_abs = np.zeros((J_sup.shape[0], ni + xj.shape[0]))\n",
    "            J_abs[:, :ni] = J_sup[:, :Bi.shape[0]] @ Bi\n",
    "            J_abs[:, ni:] = J_sup[:, Bi.shape[0]:] @ Bj\n",
    "            return J_abs\n",
    "        \n",
    "        return meas_fn_super_between, jac_fn_super_between, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "    for f in sup_fg.factors:\n",
    "        if len(f.adj_var_nodes) == 1:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_prior_factor(f)\n",
    "            v = abs_var_nodes[f.adj_var_nodes[0].variableID]\n",
    "            abs_f = Factor(f.factorID, [v], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_prior\"\n",
    "            abs_f.adj_beliefs = [v.belief]\n",
    "\n",
    "            lin0 = v.mu\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            v.adj_factors.append(abs_f)\n",
    "\n",
    "        elif len(f.adj_var_nodes) == 2:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_between_factor(f)\n",
    "            i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "            vi, vj = abs_var_nodes[i], abs_var_nodes[j]\n",
    "            abs_f = Factor(f.factorID, [vi, vj], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_between\"\n",
    "            abs_f.adj_beliefs = [vi.belief, vj.belief]\n",
    "\n",
    "            lin0 = np.concatenate([vi.mu, vj.mu])\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            vi.adj_factors.append(abs_f)\n",
    "            vj.adj_factors.append(abs_f)\n",
    "\n",
    "    abs_fg.n_factor_nodes = len(abs_fg.factors)\n",
    "\n",
    "\n",
    "    return abs_fg, Bs, ks, k2s\n",
    "\n",
    "\n",
    "def bottom_up_modify_abs_graph(\n",
    "    layers,\n",
    "    r_reduced = 2,\n",
    "    eta_damping=0.4):\n",
    "\n",
    "    abs_var_nodes = {}\n",
    "    Bs = {}\n",
    "    ks = {}\n",
    "    k2s = {}\n",
    "\n",
    "    # === 1. Build Abstraction Variables ===\n",
    "    abs_fg = FactorGraph(nonlinear_factors=False, eta_damping=eta_damping)\n",
    "    abs_fg_old = deepcopy(layers[-1][\"graph\"])\n",
    "    sup_fg = layers[-2][\"graph\"]\n",
    "\n",
    "    for sn in sup_fg.var_nodes:\n",
    "        if sn.dofs <= r_reduced:\n",
    "            r = sn.dofs  # No reduction if dofs already <= r\n",
    "        else:\n",
    "            r = r_reduced\n",
    "\n",
    "        sid = sn.variableID\n",
    "        varis_sup_mu = sn.mu\n",
    "        varis_sup_sigma = sn.Sigma\n",
    "        \n",
    "        # Step 1: Eigen decomposition of the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(varis_sup_sigma)\n",
    "\n",
    "        # Step 2: Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
    "        idx = np.argsort(eigvals)[::-1]      # Get indices of sorted eigenvalues (largest first)\n",
    "        eigvals = eigvals[idx]               # Reorder eigenvalues\n",
    "        eigvecs = eigvecs[:, idx]            # Reorder corresponding eigenvectors\n",
    "\n",
    "        # Step 3: Select the top-k eigenvectors to form the projection matrix (principal subspace)\n",
    "        B_reduced = eigvecs[:, :r]                 # B_reduced: shape (sup_dof, r), projects r to sup_dof\n",
    "        #B_reduced = layers[-1][\"Bs\"][sid]\n",
    "        #B_reduced = np.eye(B_reduced.shape[0])\n",
    "        Bs[sid] = B_reduced                        # Store the projection matrix for this variable\n",
    "\n",
    "        # Step 4: Project eta and Lam onto the reduced 2D subspace\n",
    "        varis_abs_mu = B_reduced.T @ varis_sup_mu          # Projected natural mean: shape (2,)\n",
    "        #varis_abs_sigma = B_reduced.T @ varis_sup_sigma @ B_reduced  # Projected covariance: shape (2, 2)\n",
    "\n",
    "        ks[sid] = varis_sup_mu - B_reduced @ varis_abs_mu  # Store the mean offset for this variable\n",
    "        #k2s[sid] = varis_sup_sigma - B_reduced @ varis_abs_sigma @ B_reduced.T  # Residual covariance\n",
    "\n",
    "        #varis_abs_lam = np.linalg.inv(varis_abs_sigma)  # Inverse covariance (precision matrix): shape (2, 2)\n",
    "\n",
    "        varis_abs_lam = abs_fg_old.var_nodes[sid].belief.lam\n",
    "        varis_abs_eta = varis_abs_lam @ varis_abs_mu  # Natural parameters: shape (2,)\n",
    "\n",
    "        v = VariableNode(sid, dofs=r)\n",
    "        v.GT = sn.GT\n",
    "        v.prior.lam = 1e-10 * varis_abs_lam\n",
    "        v.prior.eta = 1e-10 * varis_abs_eta\n",
    "        v.mu = varis_abs_mu\n",
    "        v.Sigma = abs_fg_old.var_nodes[sid].Sigma\n",
    "        v.belief = NdimGaussian(r, varis_abs_eta, varis_abs_lam)\n",
    "\n",
    "        abs_var_nodes[sid] = v\n",
    "        abs_fg.var_nodes.append(v)\n",
    "    abs_fg.n_var_nodes = len(abs_fg.var_nodes)\n",
    "\n",
    "\n",
    "    # === 2. Abstract Prior ===\n",
    "    def make_abs_prior_factor(sup_factor):\n",
    "        abs_id = sup_factor.adj_var_nodes[0].variableID\n",
    "        B = Bs[abs_id]\n",
    "        k = ks[abs_id]\n",
    "\n",
    "        def meas_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.meas_fn(B @ x_abs + k)\n",
    "        \n",
    "        def jac_fn_abs_prior(x_abs, *args):\n",
    "            return sup_factor.jac_fn(B @ x_abs + k) @ B\n",
    "\n",
    "        return meas_fn_abs_prior, jac_fn_abs_prior, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "\n",
    "    # === 3. Abstract Between ===\n",
    "    def make_abs_between_factor(sup_factor):\n",
    "        vids = [v.variableID for v in sup_factor.adj_var_nodes]\n",
    "        i, j = vids # two variable IDs\n",
    "        ni = abs_var_nodes[i].dofs\n",
    "        Bi, Bj = Bs[i], Bs[j]\n",
    "        ki, kj = ks[i], ks[j]                       \n",
    "    \n",
    "\n",
    "        def meas_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            return sup_factor.meas_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "\n",
    "        def jac_fn_super_between(xij, *args):\n",
    "            xi, xj = xij[:ni], xij[ni:]\n",
    "            J_sup = sup_factor.jac_fn(np.concatenate([Bi @ xi + ki, Bj @ xj + kj]))\n",
    "            J_abs = np.zeros((J_sup.shape[0], ni + xj.shape[0]))\n",
    "            J_abs[:, :ni] = J_sup[:, :Bi.shape[0]] @ Bi\n",
    "            J_abs[:, ni:] = J_sup[:, Bi.shape[0]:] @ Bj\n",
    "            return J_abs\n",
    "        \n",
    "        return meas_fn_super_between, jac_fn_super_between, sup_factor.measurement, sup_factor.measurement_lambda\n",
    "    \n",
    "\n",
    "    for f in sup_fg.factors:\n",
    "        if len(f.adj_var_nodes) == 1:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_prior_factor(f)\n",
    "            v = abs_var_nodes[f.adj_var_nodes[0].variableID]\n",
    "            abs_f = Factor(f.factorID, [v], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_prior\"\n",
    "            abs_f.adj_beliefs = [v.belief]\n",
    "\n",
    "            lin0 = v.mu\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            v.adj_factors.append(abs_f)\n",
    "\n",
    "        elif len(f.adj_var_nodes) == 2:\n",
    "            meas_fn, jac_fn, z, z_lambda = make_abs_between_factor(f)\n",
    "            i, j = [v.variableID for v in f.adj_var_nodes]\n",
    "            vi, vj = abs_var_nodes[i], abs_var_nodes[j]\n",
    "            abs_f = Factor(f.factorID, [vi, vj], z, z_lambda, meas_fn, jac_fn)\n",
    "            abs_f.type = \"abs_between\"\n",
    "            abs_f.adj_beliefs = [vi.belief, vj.belief]\n",
    "\n",
    "            lin0 = np.concatenate([vi.mu, vj.mu])\n",
    "            abs_f.compute_factor(linpoint=lin0, update_self=True)\n",
    "\n",
    "            abs_fg.factors.append(abs_f)\n",
    "            vi.adj_factors.append(abs_f)\n",
    "            vj.adj_factors.append(abs_f)\n",
    "\n",
    "    abs_fg.n_factor_nodes = len(abs_fg.factors)\n",
    "\n",
    "\n",
    "    return abs_fg, Bs, ks, k2s\n",
    "\n",
    "\n",
    "def bottom_up_modify_super_graph(layers):\n",
    "    \"\"\"\n",
    "    Update super-node means (mu) from base nodes,\n",
    "    and simultaneously adjust variable beliefs and adjacent messages.\n",
    "    \"\"\"\n",
    "\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    super_graph = layers[-1][\"graph\"]\n",
    "    node_map = layers[-1][\"node_map\"]\n",
    "\n",
    "    id2var = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "    sid2idx = {sn[\"data\"][\"id\"]: i for i, sn in enumerate(layers[-1][\"nodes\"])}\n",
    "\n",
    "    for sid, group in super_groups.items():\n",
    "        mu_blocks = [id2var[bid].mu for bid in group]\n",
    "        mu_super = np.concatenate(mu_blocks) if mu_blocks else np.zeros(0)\n",
    "\n",
    "        if sid in sid2idx:\n",
    "            idx = sid2idx[sid]\n",
    "            v = super_graph.var_nodes[idx]\n",
    "\n",
    "            # Old belief\n",
    "            old_belief = v.belief\n",
    "\n",
    "            # 1. Update mu\n",
    "            #v.mu = mu_super\n",
    "\n",
    "            # 2. New belief (use old Sigma + new mu)\n",
    "            #lam = np.linalg.inv(v.Sigma)\n",
    "            #eta = lam @ v.mu\n",
    "            #new_belief = NdimGaussian(v.dofs, eta, lam)\n",
    "            #v.belief = new_belief\n",
    "            #v.prior = NdimGaussian(v.dofs)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            # 3. update adj_beliefs and messages\n",
    "            if v.adj_factors:\n",
    "                n_adj = len(v.adj_factors)\n",
    "                d_eta = new_belief.eta - old_belief.eta\n",
    "                d_lam = new_belief.lam - old_belief.lam\n",
    "                for f in v.adj_factors:\n",
    "                    if v in f.adj_var_nodes:\n",
    "                        idx_in_factor = f.adj_var_nodes.index(v)\n",
    "                        # update factor's adj_belief\n",
    "                        f.adj_beliefs[idx_in_factor] = new_belief\n",
    "                        # update corresponding messages\n",
    "                        msg = f.messages[idx_in_factor]\n",
    "                        msg.eta += d_eta / n_adj\n",
    "                        msg.lam += d_lam / n_adj\n",
    "                        f.messages[idx_in_factor] = msg\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def top_down_modify_base_and_abs_graph(layers):\n",
    "    \"\"\"\n",
    "    From the super graph downward, split μ to the base graph,\n",
    "    and simultaneously update the base variables' beliefs and the adjacent factors'\n",
    "    adj_beliefs / messages.\n",
    "\n",
    "    Assume layers[-1] is the super layer and layers[-2] is the base layer.\n",
    "    \"\"\"\n",
    "    super_graph = layers[-1][\"graph\"]\n",
    "    base_graph = layers[-2][\"graph\"]\n",
    "    node_map   = layers[-1][\"node_map\"]  # { base_id(str) -> super_id(str) }\n",
    "\n",
    "\n",
    "    # super_id -> [base_id(int)]\n",
    "    super_groups = {}\n",
    "    for b_str, s_id in node_map.items():\n",
    "        b_int = int(b_str)\n",
    "        super_groups.setdefault(s_id, []).append(b_int)\n",
    "\n",
    "    # child lookup\n",
    "    id2var_base = {vn.variableID: vn for vn in base_graph.var_nodes}\n",
    "\n",
    "    a = 0\n",
    "    for s_var in super_graph.var_nodes:\n",
    "        sid = str(s_var.variableID)\n",
    "        if sid not in super_groups:\n",
    "            continue\n",
    "        base_ids = super_groups[sid]\n",
    "\n",
    "        # === split super.mu to base ===\n",
    "        mu_super = s_var.mu\n",
    "        off = 0\n",
    "        for bid in base_ids:\n",
    "            v = id2var_base[bid]\n",
    "            d = v.dofs\n",
    "            mu_child = mu_super[off:off+d]\n",
    "            off += d\n",
    "\n",
    "            old_belief = v.belief\n",
    "\n",
    "            # 1. update mu\n",
    "            v.mu = mu_child\n",
    "\n",
    "            # 2. new belief（keep Σ unchanged，use new mu）\n",
    "            eta = v.belief.lam @ v.mu\n",
    "            new_belief = NdimGaussian(v.dofs, eta, v.belief.lam)\n",
    "            v.belief = new_belief\n",
    "            v.prior = NdimGaussian(v.dofs, 1e-10*eta, 1e-10*v.belief.lam)\n",
    "\n",
    "            # 3. Sync to adjacent factors (this step is important)\n",
    "            if v.adj_factors:\n",
    "                n_adj = len(v.adj_factors)\n",
    "                d_eta = new_belief.eta - old_belief.eta\n",
    "                d_lam = new_belief.lam - old_belief.lam\n",
    "\n",
    "                for f in v.adj_factors:\n",
    "                    if v in f.adj_var_nodes:\n",
    "                        idx = f.adj_var_nodes.index(v)\n",
    "                        # update adj_beliefs\n",
    "                        f.adj_beliefs[idx] = new_belief\n",
    "                        # correct coresponding message\n",
    "                        msg = f.messages[idx]\n",
    "                        msg.eta += d_eta / n_adj\n",
    "                        msg.lam += d_lam / n_adj\n",
    "                        f.messages[idx] = msg\n",
    "\n",
    "    return base_graph\n",
    "\n",
    "\n",
    "def top_down_modify_super_graph(layers):\n",
    "    \"\"\"\n",
    "    From the abs graph downward, project mu / Sigma back to the super graph,\n",
    "    and simultaneously update the super variables' beliefs and the adjacent\n",
    "    factors' adj_beliefs / messages.\n",
    "\n",
    "    Requirements:\n",
    "      - layers[-1] is abs, layers[-2] is super\n",
    "      - Factors at the abs level and the super level share the same factorID (one-to-one)\n",
    "      - The columns of B are orthonormal (from covariance eigenvectors; eigenvectors from np.linalg.eigh are orthogonal)\n",
    "    \"\"\"\n",
    "\n",
    "    abs_graph   = layers[-1][\"graph\"]\n",
    "    super_graph = layers[-2][\"graph\"]\n",
    "    Bs  = layers[-1][\"Bs\"]   # { super_id(int) -> B (d_super × r) }\n",
    "    ks  = layers[-1][\"ks\"]   # { super_id(int) -> k (d_super,) }\n",
    "    #k2s = layers[-1][\"k2s\"]  # { super_id(int) -> residual covariance (d_super × d_super) }\n",
    "\n",
    "    # Prebuild abs factor index: factorID -> Factor\n",
    "    #abs_f_by_id = {f.factorID: f for f in getattr(abs_graph, \"factors\", [])}\n",
    "\n",
    "    # ---- First project variables' mu / Sigma and update beliefs ----\n",
    "    for sn in super_graph.var_nodes:\n",
    "        sid = sn.variableID\n",
    "        if sid not in Bs or sid not in ks:\n",
    "            continue\n",
    "        B  = Bs[sid]    # (d_s × r)\n",
    "        k  = ks[sid]    # (d_s,)\n",
    "        #k2 = k2s[sid]   # (d_s × d_s)\n",
    "\n",
    "        # x_s = B x_a + k; Σ_s = B Σ_a Bᵀ + k2\n",
    "        mu_a    = abs_graph.var_nodes[sid].mu\n",
    "        mu_s    = B @ mu_a + k\n",
    "        sn.mu   = mu_s\n",
    "\n",
    "        # Refresh super belief (natural parameters) with the new μ and Σ\n",
    "        eta = sn.belief.lam @ sn.mu\n",
    "        new_belief = NdimGaussian(sn.dofs, eta, sn.belief.lam)\n",
    "        sn.belief  = new_belief\n",
    "        sn.prior = NdimGaussian(sn.dofs, 1e-10*eta, 1e-10*sn.belief.lam)\n",
    "\n",
    "    # ---- Then push abs messages back to super, preserving the original super messages on the orthogonal complement ----\n",
    "    # Idea: for the side of the super factor f_sup connected to variable sid:\n",
    "    #   η_s_new = B η_a + (I - B Bᵀ) η_s_old\n",
    "    #   Λ_s_new = B Λ_a Bᵀ + (I - B Bᵀ) Λ_s_old (I - B Bᵀ)\n",
    "    # This ensures the subspace is governed by the abs message, while the orthogonal complement keeps the old super message.\n",
    "    for sn in super_graph.var_nodes:\n",
    "        # Iterate over super factors adjacent to this super variable\n",
    "        for f_sup in sn.adj_factors:\n",
    "            idx_side = f_sup.adj_var_nodes.index(sn)\n",
    "            # update the factor's recorded adjacent belief on that side (optional; usually refreshed in the next iteration)\n",
    "            f_sup.adj_beliefs[idx_side] = sn.belief\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def refresh_gbp_results(layers):\n",
    "    \"\"\"\n",
    "    Precompute an affine map to the base plane for each layer:\n",
    "      base:   A_i = I2, b_i = 0\n",
    "      super:  A_s = (1/m) [A_c1, A_c2, ..., A_cm], b_s = (1/m) Σ b_cj\n",
    "      abs:    A_a = A_super(s) @ B_s,             b_a = A_super(s) @ k_s + b_super(s)\n",
    "    Then refresh gbp_result via pos = A @ mu + b.\n",
    "    Convention: use string keys everywhere (aligned with Cytoscape ids).\n",
    "    \"\"\"\n",
    "    if not layers:\n",
    "        return\n",
    "\n",
    "    # ---------- 1) Bottom-up: compute A, b for each layer ----------\n",
    "    for li, L in enumerate(layers):\n",
    "        g = L.get(\"graph\")\n",
    "        if g is None:\n",
    "            L.pop(\"A\", None); L.pop(\"b\", None); L.pop(\"gbp_result\", None)\n",
    "            continue\n",
    "\n",
    "        name = L[\"name\"]\n",
    "        # ---- base ----\n",
    "        if name.startswith(\"base\"):\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for v in g.var_nodes:\n",
    "                key = str(v.variableID)\n",
    "                L[\"A\"][key] = np.eye(2)\n",
    "                L[\"b\"][key] = np.zeros(2, dtype=float)\n",
    "\n",
    "        # ---- super ----\n",
    "        elif name.startswith(\"super\"):\n",
    "            parent = layers[li - 1]\n",
    "            node_map = L[\"node_map\"]  # { prev_id(str) -> super_id(str) }\n",
    "\n",
    "            # Grouping (preserve insertion order to match the concatenation order in build_super_graph)\n",
    "            groups = {}\n",
    "            for prev_id, s_id in node_map.items():\n",
    "                prev_id = str(prev_id); s_id = str(s_id)\n",
    "                groups.setdefault(s_id, []).append(prev_id)\n",
    "\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for s_id, children in groups.items():\n",
    "                m = len(children)\n",
    "                # Horizontal concatenation [A_c1, A_c2, ...]\n",
    "                A_blocks = [parent[\"A\"][cid] for cid in children]  # each block has shape 2×d_c\n",
    "                A_concat = np.hstack(A_blocks) if A_blocks else np.zeros((2, 0))\n",
    "                b_sum = sum((parent[\"b\"][cid] for cid in children), start=np.zeros(2, dtype=float))\n",
    "                L[\"A\"][s_id] = (1.0 / m) * A_concat\n",
    "                L[\"b\"][s_id] = (1.0 / m) * b_sum\n",
    "\n",
    "        # ---- abs ----\n",
    "        elif name.startswith(\"abs\"):\n",
    "            parent = layers[li - 1]  # the corresponding super layer\n",
    "            Bs, ks = L[\"Bs\"], L[\"ks\"]  # Note: keys are the super variableIDs (int)\n",
    "\n",
    "            # Build a mapping between super variableID (int) and the super string id (follow node list order)\n",
    "            # The order of nodes in the parent (super) and this (abs) layer is consistent (copy_to_abs preserves order)\n",
    "            int2sid = {i: str(parent[\"nodes\"][i][\"data\"][\"id\"]) for i in range(len(parent[\"nodes\"]))}\n",
    "\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "            for av in g.var_nodes:\n",
    "                sid_int = av.variableID              # super variableID (int)\n",
    "                s_id = int2sid.get(sid_int, str(sid_int))  # super string id (also the abs node id)\n",
    "                B = Bs[sid_int]                       # (sum d_c) × r\n",
    "                k = ks[sid_int]                       # (sum d_c,)\n",
    "\n",
    "                A_sup = parent[\"A\"][s_id]             # shape 2 × (sum d_c)\n",
    "                b_sup = parent[\"b\"][s_id]             # shape (2,)\n",
    "\n",
    "                L[\"A\"][s_id] = A_sup @ B              # 2 × r\n",
    "                L[\"b\"][s_id] = A_sup @ k + b_sup      # 2,\n",
    "\n",
    "        else:\n",
    "            # Unknown layer type\n",
    "            L[\"A\"], L[\"b\"] = {}, {}\n",
    "\n",
    "    # ---------- 2) Compute gbp_result ----------\n",
    "    for li, L in enumerate(layers):\n",
    "        g = L.get(\"graph\")\n",
    "        if g is None:\n",
    "            L.pop(\"gbp_result\", None)\n",
    "            continue\n",
    "\n",
    "        name = L[\"name\"]\n",
    "        res = {}\n",
    "\n",
    "        if name.startswith(\"base\"):\n",
    "            for v in g.var_nodes:\n",
    "                vid = str(v.variableID)\n",
    "                res[vid] = v.mu[:2].tolist()\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Directly use A_super, b_super mapping\n",
    "            # nodes order is consistent with var_nodes order\n",
    "            for i, v in enumerate(g.var_nodes):\n",
    "                s_id = str(L[\"nodes\"][i][\"data\"][\"id\"])\n",
    "                A, b = L[\"A\"][s_id], L[\"b\"][s_id]   # A: 2×(sum d_c)\n",
    "                res[s_id] = (A @ v.mu + b).tolist()\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            parent = layers[li - 1]\n",
    "            # Also align via string ids\n",
    "            for i, v in enumerate(g.var_nodes):\n",
    "                a_id = str(L[\"nodes\"][i][\"data\"][\"id\"])  # same text as the super s_id\n",
    "                A, b = L[\"A\"][a_id], L[\"b\"][a_id]        # A: 2×r\n",
    "                res[a_id] = (A @ v.mu + b).tolist()\n",
    "\n",
    "        L[\"gbp_result\"] = res\n",
    "\n",
    "\n",
    "\n",
    "def vloop(layers):\n",
    "    \"\"\"\n",
    "    Simplified V-cycle:\n",
    "    1) bottom-up: rebuild and iterate once for base / super / abs in order\n",
    "    2) top-down: propagate mu from super -> base\n",
    "    3) refresh gbp_result on each layer for UI use\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- bottom-up ----\n",
    "    #if layers and \"graph\" in layers[0]:\n",
    "    #    layers[0][\"graph\"].synchronous_iteration()\n",
    "        \n",
    "    for i in range(1, len(layers)):\n",
    "        name = layers[i][\"name\"]\n",
    "\n",
    "        if name.startswith(\"super1\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            # layers[i][\"graph\"] = build_super_graph(layers[:i+1])\n",
    "            #layers[i][\"graph\"].synchronous_iteration()\n",
    "            #bottom_up_modify_super_graph(layers[:i+1])\n",
    "            #build_super_graph(layers[:i+1])\n",
    "            #update_super_graph_linearized(layers[:i+1])\n",
    "            pass\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            layers[i][\"graph\"] = build_super_graph(layers[:i+1])\n",
    "            #layers[i][\"graph\"] = update_super_graph_linearized(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Rebuild abs using the previous super\n",
    "            abs_graph, Bs, ks, k2s = build_abs_graph(layers[:i+1])\n",
    "            layers[i][\"graph\"] = abs_graph\n",
    "            layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "        # After build, one iteration per layer\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "    # ---- top-down (pass mu) ----\n",
    "    for i in range(len(layers) - 1, 0, -1):\n",
    "        # After one iterations per layer, reproject\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # this is very important, but dont know why yet\n",
    "        # so abs layer need more iterations\n",
    "        #if name.startswith(\"abs\"):\n",
    "            #layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "        name = layers[i][\"name\"]\n",
    "        if name.startswith(\"super\"):\n",
    "            # Split super.mu back to base/abs\n",
    "            top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Project abs.mu back to super\n",
    "            top_down_modify_super_graph(layers[:i+1])\n",
    "    \n",
    "\n",
    "    # ---- refresh gbp_result for UI ----\n",
    "    refresh_gbp_results(layers)\n",
    "\n",
    "\n",
    "\n",
    "def compute_energy(layers):\n",
    "    \"\"\"\n",
    "    energy = 0.5 * sum_i || mu_i[0:2] - GT_i[0:2] ||^2  over base layer variables\n",
    "    vectorized version (no per-node np calls)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_graph = layers[0].get(\"graph\", None)\n",
    "        var_nodes = getattr(base_graph, \"var_nodes\", None)\n",
    "        if base_graph is None or not var_nodes:\n",
    "            return \"Energy: -\"\n",
    "\n",
    "        # Stack mu[:2] and GT[:2] for all variables into (N, 2)\n",
    "        mus_2 = np.stack([np.asarray(v.mu[:2], dtype=float) for v in var_nodes], axis=0)\n",
    "        gts_2 = np.stack([np.asarray(v.GT[:2], dtype=float) for v in var_nodes], axis=0)\n",
    "\n",
    "        diff = mus_2 - gts_2                      # shape (N, 2)\n",
    "        total = 0.5 * np.sum(diff * diff)         # 0.5 * sum of squared norms\n",
    "\n",
    "        return f\"Energy: {float(total):.4f}\"\n",
    "    except Exception:\n",
    "        return \"Energy: -\"\n",
    "    \n",
    "\n",
    "\n",
    "class VGraph:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 nonlinear_factors=True,\n",
    "                 eta_damping=0.2,\n",
    "                 r_reduced=2,\n",
    "                 beta=0.0,\n",
    "                 iters_since_relinear=0,\n",
    "                 num_undamped_iters=0,\n",
    "                 min_linear_iters=100,\n",
    "                 wild_thresh=0):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.iters_since_relinear = iters_since_relinear\n",
    "        self.min_linear_iters = min_linear_iters\n",
    "        self.nonlinear_factors = nonlinear_factors\n",
    "        self.eta_damping = eta_damping\n",
    "        self.r_reduced = r_reduced\n",
    "        self.wild_thresh = wild_thresh\n",
    "\n",
    "        #self.energy_history = []\n",
    "        #self.error_history = []\n",
    "        #self.nmsgs_history = []\n",
    "        #self.mus = []\n",
    "\n",
    "\n",
    "    def vloop(self):\n",
    "        \"\"\"\n",
    "        Simplified V-cycle:\n",
    "        1) bottom-up: rebuild and iterate once for base / super / abs in order\n",
    "        2) top-down: propagate mu from super -> base\n",
    "        3) refresh gbp_result on each layer for UI use\n",
    "        \"\"\"\n",
    "\n",
    "        layers = self.layers\n",
    "\n",
    "        # ---- bottom-up ----\n",
    "        #if layers and \"graph\" in layers[0]:\n",
    "        #    layers[0][\"graph\"].synchronous_iteration()\n",
    "            \n",
    "        for i in range(1, len(layers)):\n",
    "            name = layers[i][\"name\"]\n",
    "\n",
    "            if name.startswith(\"super1\"):\n",
    "                # Update super using the previous base graph's new linearization points\n",
    "                pass\n",
    "\n",
    "            elif name.startswith(\"super\"):\n",
    "                # Update super using the previous layer's graph\n",
    "                layers[i][\"graph\"] = build_super_graph(layers[:i+1], eta_damping=self.eta_damping)\n",
    "\n",
    "            elif name.startswith(\"abs\"):\n",
    "                # Rebuild abs using the previous super\n",
    "                abs_graph, Bs, ks, k2s = bottom_up_modify_abs_graph(layers[:i+1], eta_damping=self.eta_damping, r_reduced=self.r_reduced)\n",
    "                layers[i][\"graph\"] = abs_graph\n",
    "                layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "            # After build, one iteration per layer\n",
    "            if \"graph\" in layers[i]:\n",
    "                layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # ---- top-down (pass mu) ----\n",
    "        for i in range(len(layers) - 1, 0, -1):\n",
    "            # After one iterations per layer, reproject\n",
    "            if \"graph\" in layers[i]:\n",
    "                layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "            # this is very important, but dont know why yet\n",
    "            # so abs layer need more iterations\n",
    "            #if name.startswith(\"abs\"):\n",
    "            #    layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "            name = layers[i][\"name\"]\n",
    "            if name.startswith(\"super\"):\n",
    "                # Split super.mu back to base/abs\n",
    "                top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "            elif name.startswith(\"abs\"):\n",
    "                # Project abs.mu back to super\n",
    "                top_down_modify_super_graph(layers[:i+1])\n",
    "\n",
    "        # ---- refresh gbp_result for UI ----\n",
    "        #refresh_gbp_results(layers)\n",
    "        return layers\n",
    "vg = VGraph(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "edefd26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntotal = 0\\nimport time \\nxxxx = time.time()\\neta, lam = layers[0][\"graph\"].joint_distribution_inf()\\nprint(\"time:\", time.time() - xxxx)\\nxxxx = time.time()\\nsigma = np.linalg.inv(lam)\\nmu = sigma @ eta\\nprint(\"time:\", time.time() - xxxx)\\na = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\\nfor i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\\n    gt = np.asarray(v.GT[0:2], dtype=float)\\n    r = np.asarray(a[i][0:2], dtype=float) - gt\\n    total += 0.5 * float(r.T @ r)\\nprint(total)\\n'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=512\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "\n",
    "\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "\n",
    "\n",
    "# 构建 GBP 图\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "gbp_graph.num_undamped_iters = 0\n",
    "gbp_graph.min_linear_iters = 2000\n",
    "opts=[{\"label\":\"base\",\"value\":\"base\"}]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "total = 0\n",
    "import time \n",
    "xxxx = time.time()\n",
    "eta, lam = layers[0][\"graph\"].joint_distribution_inf()\n",
    "print(\"time:\", time.time() - xxxx)\n",
    "xxxx = time.time()\n",
    "sigma = np.linalg.inv(lam)\n",
    "mu = sigma @ eta\n",
    "print(\"time:\", time.time() - xxxx)\n",
    "a = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\n",
    "for i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\n",
    "    gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "    r = np.asarray(a[i][0:2], dtype=float) - gt\n",
    "    total += 0.5 * float(r.T @ r)\n",
    "print(total)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc411380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_map(graph, include_priors: bool = True, include_factors: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    It is actually the sum of squares of distances.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "\n",
    "    for v in graph.var_nodes[:graph.n_var_nodes]:\n",
    "        gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "        r = np.asarray(v.mu[0:2], dtype=float) - gt\n",
    "        total += 0.5 * float(r.T @ r)\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "225fc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 2484749.149338\n",
      "Iter 002 | Energy = 3320732.449448\n",
      "Iter 003 | Energy = 3450726.825488\n",
      "Iter 004 | Energy = 3357693.395221\n",
      "Iter 005 | Energy = 3272807.985112\n"
     ]
    }
   ],
   "source": [
    "basegraph = layers[0][\"graph\"]\n",
    "for it in range(5):\n",
    "    basegraph.synchronous_iteration()\n",
    "    energy = energy_map(basegraph, include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b7a83d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4063.420012620206\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "a = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\n",
    "for i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\n",
    "    gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "    r = np.asarray(a[i][0:2], dtype=float) - gt\n",
    "    total += 0.5 * float(r.T @ r)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0dcec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 11453015.696525\n",
      "Iter 002 | Energy = 15150.477989\n",
      "Iter 003 | Energy = 2438.977856\n",
      "Iter 004 | Energy = 2414.854940\n",
      "Iter 005 | Energy = 2358.150798\n",
      "Iter 006 | Energy = 2330.323331\n",
      "Iter 007 | Energy = 2305.154596\n",
      "Iter 008 | Energy = 2287.420086\n",
      "Iter 009 | Energy = 2273.975239\n",
      "Iter 010 | Energy = 2264.035471\n",
      "Iter 011 | Energy = 2256.671855\n",
      "Iter 012 | Energy = 2251.321491\n",
      "Iter 013 | Energy = 2247.548610\n",
      "Iter 014 | Energy = 2245.030197\n",
      "Iter 015 | Energy = 2243.512787\n",
      "Iter 016 | Energy = 2242.795155\n",
      "Iter 017 | Energy = 2242.714696\n",
      "Iter 018 | Energy = 2243.138949\n",
      "Iter 019 | Energy = 2243.959325\n",
      "Iter 020 | Energy = 2245.086514\n",
      "Iter 021 | Energy = 2246.446952\n",
      "Iter 022 | Energy = 2247.980061\n",
      "Iter 023 | Energy = 2249.636044\n",
      "Iter 024 | Energy = 2251.374110\n",
      "Iter 025 | Energy = 2253.161018\n",
      "Iter 026 | Energy = 2254.969873\n",
      "Iter 027 | Energy = 2256.779120\n",
      "Iter 028 | Energy = 2258.571692\n",
      "Iter 029 | Energy = 2260.334290\n",
      "Iter 030 | Energy = 2262.056768\n",
      "Iter 031 | Energy = 2263.731605\n",
      "Iter 032 | Energy = 2265.353451\n",
      "Iter 033 | Energy = 2266.918745\n",
      "Iter 034 | Energy = 2268.425373\n",
      "Iter 035 | Energy = 2269.872396\n",
      "Iter 036 | Energy = 2271.259799\n",
      "Iter 037 | Energy = 2272.588294\n",
      "Iter 038 | Energy = 2273.859146\n",
      "Iter 039 | Energy = 2275.074027\n",
      "Iter 040 | Energy = 2276.234907\n",
      "Iter 041 | Energy = 2277.343945\n",
      "Iter 042 | Energy = 2278.403420\n",
      "Iter 043 | Energy = 2279.415667\n",
      "Iter 044 | Energy = 2280.383022\n",
      "Iter 045 | Energy = 2281.307793\n",
      "Iter 046 | Energy = 2282.192222\n",
      "Iter 047 | Energy = 2283.038473\n",
      "Iter 048 | Energy = 2283.848615\n",
      "Iter 049 | Energy = 2284.624609\n",
      "Iter 050 | Energy = 2285.368310\n",
      "Iter 051 | Energy = 2286.081460\n",
      "Iter 052 | Energy = 2286.765693\n",
      "Iter 053 | Energy = 2287.422536\n",
      "Iter 054 | Energy = 2288.053413\n",
      "Iter 055 | Energy = 2288.659650\n",
      "Iter 056 | Energy = 2289.242483\n",
      "Iter 057 | Energy = 2289.803062\n",
      "Iter 058 | Energy = 2290.342456\n",
      "Iter 059 | Energy = 2290.861663\n",
      "Iter 060 | Energy = 2291.361613\n",
      "Iter 061 | Energy = 2291.843174\n",
      "Iter 062 | Energy = 2292.307159\n",
      "Iter 063 | Energy = 2292.754328\n",
      "Iter 064 | Energy = 2293.185396\n",
      "Iter 065 | Energy = 2293.601033\n",
      "Iter 066 | Energy = 2294.001874\n",
      "Iter 067 | Energy = 2294.388516\n",
      "Iter 068 | Energy = 2294.761523\n",
      "Iter 069 | Energy = 2295.121433\n",
      "Iter 070 | Energy = 2295.468755\n",
      "Iter 071 | Energy = 2295.803972\n",
      "Iter 072 | Energy = 2296.127546\n",
      "Iter 073 | Energy = 2296.439919\n",
      "Iter 074 | Energy = 2296.741510\n",
      "Iter 075 | Energy = 2297.032723\n",
      "Iter 076 | Energy = 2297.313943\n",
      "Iter 077 | Energy = 2297.585541\n",
      "Iter 078 | Energy = 2297.847870\n",
      "Iter 079 | Energy = 2298.101271\n",
      "Iter 080 | Energy = 2298.346072\n",
      "Iter 081 | Energy = 2298.582587\n",
      "Iter 082 | Energy = 2298.811118\n",
      "Iter 083 | Energy = 2299.031956\n",
      "Iter 084 | Energy = 2299.245379\n",
      "Iter 085 | Energy = 2299.451658\n",
      "Iter 086 | Energy = 2299.651050\n",
      "Iter 087 | Energy = 2299.843806\n",
      "Iter 088 | Energy = 2300.030163\n",
      "Iter 089 | Energy = 2300.210354\n",
      "Iter 090 | Energy = 2300.384599\n",
      "Iter 091 | Energy = 2300.553112\n",
      "Iter 092 | Energy = 2300.716099\n",
      "Iter 093 | Energy = 2300.873758\n",
      "Iter 094 | Energy = 2301.026277\n",
      "Iter 095 | Energy = 2301.173842\n",
      "Iter 096 | Energy = 2301.316628\n",
      "Iter 097 | Energy = 2301.454803\n",
      "Iter 098 | Energy = 2301.588532\n",
      "Iter 099 | Energy = 2301.717972\n",
      "Iter 100 | Energy = 2301.843272\n",
      "Iter 101 | Energy = 2301.964579\n",
      "Iter 102 | Energy = 2302.082032\n",
      "Iter 103 | Energy = 2302.195766\n",
      "Iter 104 | Energy = 2302.305910\n",
      "Iter 105 | Energy = 2302.412589\n",
      "Iter 106 | Energy = 2302.515922\n",
      "Iter 107 | Energy = 2302.616025\n",
      "Iter 108 | Energy = 2302.713008\n",
      "Iter 109 | Energy = 2302.806980\n",
      "Iter 110 | Energy = 2302.898041\n",
      "Iter 111 | Energy = 2302.986293\n",
      "Iter 112 | Energy = 2303.071829\n",
      "Iter 113 | Energy = 2303.154741\n",
      "Iter 114 | Energy = 2303.235119\n",
      "Iter 115 | Energy = 2303.313046\n",
      "Iter 116 | Energy = 2303.388606\n",
      "Iter 117 | Energy = 2303.461876\n",
      "Iter 118 | Energy = 2303.532934\n",
      "Iter 119 | Energy = 2303.601851\n",
      "Iter 120 | Energy = 2303.668700\n",
      "Iter 121 | Energy = 2303.733548\n",
      "Iter 122 | Energy = 2303.796460\n",
      "Iter 123 | Energy = 2303.857499\n",
      "Iter 124 | Energy = 2303.916728\n",
      "Iter 125 | Energy = 2303.974204\n",
      "Iter 126 | Energy = 2304.029984\n",
      "Iter 127 | Energy = 2304.084122\n",
      "Iter 128 | Energy = 2304.136672\n",
      "Iter 129 | Energy = 2304.187685\n",
      "Iter 130 | Energy = 2304.237208\n",
      "Iter 131 | Energy = 2304.285291\n",
      "Iter 132 | Energy = 2304.331977\n",
      "Iter 133 | Energy = 2304.377312\n",
      "Iter 134 | Energy = 2304.421337\n",
      "Iter 135 | Energy = 2304.464095\n",
      "Iter 136 | Energy = 2304.505624\n",
      "Iter 137 | Energy = 2304.545963\n",
      "Iter 138 | Energy = 2304.585149\n",
      "Iter 139 | Energy = 2304.623217\n",
      "Iter 140 | Energy = 2304.660202\n",
      "Iter 141 | Energy = 2304.696138\n",
      "Iter 142 | Energy = 2304.731056\n",
      "Iter 143 | Energy = 2304.764988\n",
      "Iter 144 | Energy = 2304.797963\n",
      "Iter 145 | Energy = 2304.830011\n",
      "Iter 146 | Energy = 2304.861160\n",
      "Iter 147 | Energy = 2304.891437\n",
      "Iter 148 | Energy = 2304.920869\n",
      "Iter 149 | Energy = 2304.949480\n",
      "Iter 150 | Energy = 2304.977295\n",
      "Iter 151 | Energy = 2305.004337\n",
      "Iter 152 | Energy = 2305.030631\n",
      "Iter 153 | Energy = 2305.056198\n",
      "Iter 154 | Energy = 2305.081060\n",
      "Iter 155 | Energy = 2305.105237\n",
      "Iter 156 | Energy = 2305.128750\n",
      "Iter 157 | Energy = 2305.151617\n",
      "Iter 158 | Energy = 2305.173859\n",
      "Iter 159 | Energy = 2305.195493\n",
      "Iter 160 | Energy = 2305.216536\n",
      "Iter 161 | Energy = 2305.237007\n",
      "Iter 162 | Energy = 2305.256921\n",
      "Iter 163 | Energy = 2305.276295\n",
      "Iter 164 | Energy = 2305.295145\n",
      "Iter 165 | Energy = 2305.313484\n",
      "Iter 166 | Energy = 2305.331328\n",
      "Iter 167 | Energy = 2305.348692\n",
      "Iter 168 | Energy = 2305.365588\n",
      "Iter 169 | Energy = 2305.382030\n",
      "Iter 170 | Energy = 2305.398032\n",
      "Iter 171 | Energy = 2305.413604\n",
      "Iter 172 | Energy = 2305.428760\n",
      "Iter 173 | Energy = 2305.443512\n",
      "Iter 174 | Energy = 2305.457870\n",
      "Iter 175 | Energy = 2305.471845\n",
      "Iter 176 | Energy = 2305.485450\n",
      "Iter 177 | Energy = 2305.498692\n",
      "Iter 178 | Energy = 2305.511584\n",
      "Iter 179 | Energy = 2305.524134\n",
      "Iter 180 | Energy = 2305.536352\n",
      "Iter 181 | Energy = 2305.548248\n",
      "Iter 182 | Energy = 2305.559830\n",
      "Iter 183 | Energy = 2305.571106\n",
      "Iter 184 | Energy = 2305.582086\n",
      "Iter 185 | Energy = 2305.592777\n",
      "Iter 186 | Energy = 2305.603188\n",
      "Iter 187 | Energy = 2305.613325\n",
      "Iter 188 | Energy = 2305.623197\n",
      "Iter 189 | Energy = 2305.632810\n",
      "Iter 190 | Energy = 2305.642173\n",
      "Iter 191 | Energy = 2305.651290\n",
      "Iter 192 | Energy = 2305.660170\n",
      "Iter 193 | Energy = 2305.668819\n",
      "Iter 194 | Energy = 2305.677242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[238]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     91\u001b[39m vg.layers = layers\n\u001b[32m     92\u001b[39m vg.eta_damping = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m vg.layers = \u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m energy = energy_map(layers[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgraph\u001b[39m\u001b[33m\"\u001b[39m], include_priors=\u001b[38;5;28;01mTrue\u001b[39;00m, include_factors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_+\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Energy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menergy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[237]\u001b[39m\u001b[32m, line 1759\u001b[39m, in \u001b[36mVGraph.vloop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers) - \u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m):\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# After one iterations per layer, reproject\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgraph\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m layers[i]:\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m         \u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgraph\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronous_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m     \u001b[38;5;66;03m# this is very important, but dont know why yet\u001b[39;00m\n\u001b[32m   1762\u001b[39m     \u001b[38;5;66;03m# so abs layer need more iterations\u001b[39;00m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;66;03m#if name.startswith(\"abs\"):\u001b[39;00m\n\u001b[32m   1764\u001b[39m     \u001b[38;5;66;03m#    layers[i][\"graph\"].synchronous_iteration()  \u001b[39;00m\n\u001b[32m   1766\u001b[39m     name = layers[i][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/abstraction-recovery/hierarchy/gbp/gbp.py:223\u001b[39m, in \u001b[36mFactorGraph.synchronous_iteration\u001b[39m\u001b[34m(self, factors, level, local_relin, robustify)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_all_messages(factors, local_relin=local_relin)\n\u001b[32m    222\u001b[39m time.sleep(\u001b[32m1e-9\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_all_beliefs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/abstraction-recovery/hierarchy/gbp/gbp.py:124\u001b[39m, in \u001b[36mFactorGraph.update_all_beliefs\u001b[39m\u001b[34m(self, vars, level, smoothing)\u001b[39m\n\u001b[32m    122\u001b[39m     var.update_smooth_belief()\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_belief\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/abstraction-recovery/hierarchy/gbp/gbp.py:607\u001b[39m, in \u001b[36mVariableNode.update_belief\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    605\u001b[39m     eta_inward, lam_inward = factor.messages[message_ix].eta, factor.messages[message_ix].lam\n\u001b[32m    606\u001b[39m     eta += eta_inward\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     lam += lam_inward\n\u001b[32m    609\u001b[39m \u001b[38;5;28mself\u001b[39m.belief.eta = eta \n\u001b[32m    610\u001b[39m \u001b[38;5;28mself\u001b[39m.belief.lam = lam\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "N=512\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "\n",
    "\n",
    "\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "\n",
    "\n",
    "# Create GBP graph\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "gbp_graph.num_undamped_iters = 0\n",
    "gbp_graph.min_linear_iters = 200\n",
    "opts=[{\"label\":\"base\",\"value\":\"base\"}]\n",
    "\n",
    "basegraph = layers[0][\"graph\"]\n",
    "\n",
    "\n",
    "kk = 5\n",
    "k_next = 1\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(kk or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "abs_layer_idx = 2\n",
    "k = 1\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(layers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "k_next = 2\n",
    "super_layer_idx = k_next*2 - 1\n",
    "last = layers[-1]\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(last[\"nodes\"], last[\"edges\"], int(6 or 8), super_layer_idx, tail_heavy=True)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"super{k_next}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "if super_layer_idx > 1:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "else:\n",
    "    layers[super_layer_idx][\"graph\"] = build_super_graph(layers)\n",
    "\n",
    "\n",
    "abs_layer_idx = 4\n",
    "k = 2\n",
    "last = layers[-1]\n",
    "abs_nodes, abs_edges = copy_to_abs(last[\"nodes\"], last[\"edges\"], abs_layer_idx)\n",
    "# Ensure super graph has run at least once\n",
    "layers[-1][\"graph\"].synchronous_iteration() \n",
    "layers.append({\"name\":f\"abs{k}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = build_abs_graph(layers)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "for i in range(2000):\n",
    "    layers[1][\"graph\"].synchronous_iteration()\n",
    "    layers[0][\"graph\"].synchronous_iteration()\n",
    "    layers[2][\"graph\"].synchronous_iteration()\n",
    "    #top_down_modify_super_graph(layers[:])\n",
    "    #top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {i+1:03d} | Energy = {energy:.6f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "vg = VGraph(layers)\n",
    "for _ in range(1000):\n",
    "    vg.layers = layers\n",
    "    vg.eta_damping = 0\n",
    "    vg.layers = vg.vloop()\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")\n",
    "\n",
    "refresh_gbp_results(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "140348b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "aa = deepcopy(layers[1][\"graph\"])\n",
    "for i in range(1000):\n",
    "    aa.synchronous_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "16e6eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "for i in range(1000):\n",
    "    layers[-1][\"graph\"].synchronous_iteration()\n",
    "\n",
    "bb = deepcopy(layers[-1][\"graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "21979a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[2][\"graph\"] = deepcopy(bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4dbd2bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.22525806e+00, -9.17650876e-17],\n",
       "       [-9.17657957e-17,  4.22525806e+00]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "\n",
    "layers[abs_layer_idx][\"graph\"].var_nodes[0].Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4728882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 2342.435012\n"
     ]
    }
   ],
   "source": [
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_super_graph(layers[:3])\n",
    "top_down_modify_base_and_abs_graph(layers[:2])\n",
    "\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e8cffe9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.55637974e+00, -2.08956516e-16],\n",
       "       [-2.08957089e-16,  3.55637974e+00]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[abs_layer_idx][\"graph\"].var_nodes[0].Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "695c9bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.55637974e+00, -2.08956516e-16],\n",
       "       [-2.08957089e-16,  3.55637974e+00]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.var_nodes[0].Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e989d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 2328.725297\n"
     ]
    }
   ],
   "source": [
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "abs_graph, Bs, ks, k2s = bottom_up_modify_abs_graph(layers[:], eta_damping=0)\n",
    "layers[2][\"graph\"] = abs_graph\n",
    "layers[2][\"Bs\"], layers[2][\"ks\"], layers[2][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "layers[2][\"graph\"].synchronous_iteration()\n",
    "layers[2][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_super_graph(layers[:3])\n",
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "\n",
    "top_down_modify_base_and_abs_graph(layers[:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74759123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | Energy = 2328.725279\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for i in range(1, len(layers)):\n",
    "        name = layers[i][\"name\"]\n",
    "\n",
    "        if name.startswith(\"super1\"):\n",
    "            # Update super using the previous base graph's new linearization points\n",
    "            pass\n",
    "\n",
    "        elif name.startswith(\"super\"):\n",
    "            # Update super using the previous layer's graph\n",
    "            layers[i][\"graph\"] = build_super_graph(layers[:i+1], eta_damping=0)\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Rebuild abs using the previous super\n",
    "            abs_graph, Bs, ks, k2s = bottom_up_modify_abs_graph(layers[:i+1], eta_damping=0)\n",
    "            layers[i][\"graph\"] = abs_graph\n",
    "            layers[i][\"Bs\"], layers[i][\"ks\"], layers[i][\"k2s\"] = Bs, ks, k2s\n",
    "\n",
    "        # After build, one iteration per layer\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "    # ---- top-down (pass mu) ----\n",
    "    for i in range(len(layers) - 1, 0, -1):\n",
    "        # After one iterations per layer, reproject\n",
    "        if \"graph\" in layers[i]:\n",
    "            layers[i][\"graph\"].synchronous_iteration()\n",
    "\n",
    "        # this is very important, but dont know why yet\n",
    "        # so abs layer need more iterations\n",
    "        #if name.startswith(\"abs\"):\n",
    "        #    layers[i][\"graph\"].synchronous_iteration()  \n",
    "\n",
    "        name = layers[i][\"name\"]\n",
    "        if name.startswith(\"super\"):\n",
    "            # Split super.mu back to base/abs\n",
    "            top_down_modify_base_and_abs_graph(layers[:i+1])\n",
    "\n",
    "        elif name.startswith(\"abs\"):\n",
    "            # Project abs.mu back to super\n",
    "            top_down_modify_super_graph(layers[:i+1])\n",
    "\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23715db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000 | Energy = 2333.057456\n",
      "Iter 1000 | Energy = 2342.434528\n",
      "Iter 1000 | Energy = 2346.848939\n",
      "Iter 1000 | Energy = 2342.829773\n",
      "Iter 1000 | Energy = 2335.406501\n",
      "Iter 1000 | Energy = 2329.460781\n",
      "Iter 1000 | Energy = 2326.647628\n",
      "Iter 1000 | Energy = 2326.626961\n",
      "Iter 1000 | Energy = 2328.345039\n",
      "Iter 1000 | Energy = 2330.722005\n",
      "Iter 1000 | Energy = 2332.962662\n",
      "Iter 1000 | Energy = 2334.638546\n",
      "Iter 1000 | Energy = 2335.631414\n",
      "Iter 1000 | Energy = 2336.017356\n",
      "Iter 1000 | Energy = 2335.954211\n",
      "Iter 1000 | Energy = 2335.604993\n",
      "Iter 1000 | Energy = 2335.101330\n",
      "Iter 1000 | Energy = 2334.535153\n",
      "Iter 1000 | Energy = 2333.964155\n",
      "Iter 1000 | Energy = 2333.420888\n",
      "Iter 1000 | Energy = 2332.920827\n",
      "Iter 1000 | Energy = 2332.468334\n",
      "Iter 1000 | Energy = 2332.060933\n",
      "Iter 1000 | Energy = 2331.692470\n",
      "Iter 1000 | Energy = 2331.355442\n",
      "Iter 1000 | Energy = 2331.042599\n",
      "Iter 1000 | Energy = 2330.747874\n",
      "Iter 1000 | Energy = 2330.466759\n",
      "Iter 1000 | Energy = 2330.196290\n",
      "Iter 1000 | Energy = 2329.934812\n",
      "Iter 1000 | Energy = 2329.681658\n",
      "Iter 1000 | Energy = 2329.436830\n",
      "Iter 1000 | Energy = 2329.200709\n",
      "Iter 1000 | Energy = 2328.973831\n",
      "Iter 1000 | Energy = 2328.756714\n",
      "Iter 1000 | Energy = 2328.549747\n",
      "Iter 1000 | Energy = 2328.353128\n",
      "Iter 1000 | Energy = 2328.166851\n",
      "Iter 1000 | Energy = 2327.990716\n",
      "Iter 1000 | Energy = 2327.824369\n",
      "Iter 1000 | Energy = 2327.667349\n",
      "Iter 1000 | Energy = 2327.519130\n",
      "Iter 1000 | Energy = 2327.379160\n",
      "Iter 1000 | Energy = 2327.246897\n",
      "Iter 1000 | Energy = 2327.121824\n",
      "Iter 1000 | Energy = 2327.003463\n",
      "Iter 1000 | Energy = 2326.891382\n",
      "Iter 1000 | Energy = 2326.785194\n",
      "Iter 1000 | Energy = 2326.684551\n",
      "Iter 1000 | Energy = 2326.589143\n",
      "Iter 1000 | Energy = 2326.498690\n",
      "Iter 1000 | Energy = 2326.412933\n",
      "Iter 1000 | Energy = 2326.331635\n",
      "Iter 1000 | Energy = 2326.254574\n",
      "Iter 1000 | Energy = 2326.181536\n",
      "Iter 1000 | Energy = 2326.112322\n",
      "Iter 1000 | Energy = 2326.046737\n",
      "Iter 1000 | Energy = 2325.984597\n",
      "Iter 1000 | Energy = 2325.925723\n",
      "Iter 1000 | Energy = 2325.869946\n",
      "Iter 1000 | Energy = 2325.817103\n",
      "Iter 1000 | Energy = 2325.767039\n",
      "Iter 1000 | Energy = 2325.719608\n",
      "Iter 1000 | Energy = 2325.674670\n",
      "Iter 1000 | Energy = 2325.632093\n",
      "Iter 1000 | Energy = 2325.591752\n",
      "Iter 1000 | Energy = 2325.553529\n",
      "Iter 1000 | Energy = 2325.517313\n",
      "Iter 1000 | Energy = 2325.482997\n",
      "Iter 1000 | Energy = 2325.450483\n",
      "Iter 1000 | Energy = 2325.419675\n",
      "Iter 1000 | Energy = 2325.390484\n",
      "Iter 1000 | Energy = 2325.362825\n",
      "Iter 1000 | Energy = 2325.336619\n",
      "Iter 1000 | Energy = 2325.311788\n",
      "Iter 1000 | Energy = 2325.288261\n",
      "Iter 1000 | Energy = 2325.265970\n",
      "Iter 1000 | Energy = 2325.244849\n",
      "Iter 1000 | Energy = 2325.224838\n",
      "Iter 1000 | Energy = 2325.205877\n",
      "Iter 1000 | Energy = 2325.187912\n",
      "Iter 1000 | Energy = 2325.170891\n",
      "Iter 1000 | Energy = 2325.154763\n",
      "Iter 1000 | Energy = 2325.139482\n",
      "Iter 1000 | Energy = 2325.125004\n",
      "Iter 1000 | Energy = 2325.111286\n",
      "Iter 1000 | Energy = 2325.098288\n",
      "Iter 1000 | Energy = 2325.085973\n",
      "Iter 1000 | Energy = 2325.074304\n",
      "Iter 1000 | Energy = 2325.063248\n",
      "Iter 1000 | Energy = 2325.052773\n",
      "Iter 1000 | Energy = 2325.042848\n",
      "Iter 1000 | Energy = 2325.033444\n",
      "Iter 1000 | Energy = 2325.024533\n",
      "Iter 1000 | Energy = 2325.016091\n",
      "Iter 1000 | Energy = 2325.008092\n",
      "Iter 1000 | Energy = 2325.000513\n",
      "Iter 1000 | Energy = 2324.993331\n",
      "Iter 1000 | Energy = 2324.986527\n",
      "Iter 1000 | Energy = 2324.980080\n",
      "Iter 1000 | Energy = 2324.973972\n",
      "Iter 1000 | Energy = 2324.968184\n",
      "Iter 1000 | Energy = 2324.962701\n",
      "Iter 1000 | Energy = 2324.957505\n",
      "Iter 1000 | Energy = 2324.952582\n",
      "Iter 1000 | Energy = 2324.947918\n",
      "Iter 1000 | Energy = 2324.943498\n",
      "Iter 1000 | Energy = 2324.939311\n",
      "Iter 1000 | Energy = 2324.935343\n",
      "Iter 1000 | Energy = 2324.931584\n",
      "Iter 1000 | Energy = 2324.928022\n",
      "Iter 1000 | Energy = 2324.924647\n",
      "Iter 1000 | Energy = 2324.921449\n",
      "Iter 1000 | Energy = 2324.918419\n",
      "Iter 1000 | Energy = 2324.915549\n",
      "Iter 1000 | Energy = 2324.912829\n",
      "Iter 1000 | Energy = 2324.910251\n",
      "Iter 1000 | Energy = 2324.907810\n",
      "Iter 1000 | Energy = 2324.905496\n",
      "Iter 1000 | Energy = 2324.903304\n",
      "Iter 1000 | Energy = 2324.901227\n",
      "Iter 1000 | Energy = 2324.899259\n",
      "Iter 1000 | Energy = 2324.897394\n",
      "Iter 1000 | Energy = 2324.895627\n",
      "Iter 1000 | Energy = 2324.893953\n",
      "Iter 1000 | Energy = 2324.892367\n",
      "Iter 1000 | Energy = 2324.890864\n",
      "Iter 1000 | Energy = 2324.889440\n",
      "Iter 1000 | Energy = 2324.888091\n",
      "Iter 1000 | Energy = 2324.886813\n",
      "Iter 1000 | Energy = 2324.885602\n",
      "Iter 1000 | Energy = 2324.884454\n",
      "Iter 1000 | Energy = 2324.883367\n",
      "Iter 1000 | Energy = 2324.882336\n",
      "Iter 1000 | Energy = 2324.881360\n",
      "Iter 1000 | Energy = 2324.880435\n",
      "Iter 1000 | Energy = 2324.879559\n",
      "Iter 1000 | Energy = 2324.878729\n",
      "Iter 1000 | Energy = 2324.877942\n",
      "Iter 1000 | Energy = 2324.877196\n",
      "Iter 1000 | Energy = 2324.876490\n",
      "Iter 1000 | Energy = 2324.875821\n",
      "Iter 1000 | Energy = 2324.875187\n",
      "Iter 1000 | Energy = 2324.874586\n",
      "Iter 1000 | Energy = 2324.874017\n",
      "Iter 1000 | Energy = 2324.873477\n",
      "Iter 1000 | Energy = 2324.872966\n",
      "Iter 1000 | Energy = 2324.872482\n",
      "Iter 1000 | Energy = 2324.872023\n",
      "Iter 1000 | Energy = 2324.871589\n",
      "Iter 1000 | Energy = 2324.871177\n",
      "Iter 1000 | Energy = 2324.870787\n",
      "Iter 1000 | Energy = 2324.870417\n",
      "Iter 1000 | Energy = 2324.870066\n",
      "Iter 1000 | Energy = 2324.869735\n",
      "Iter 1000 | Energy = 2324.869420\n",
      "Iter 1000 | Energy = 2324.869122\n",
      "Iter 1000 | Energy = 2324.868840\n",
      "Iter 1000 | Energy = 2324.868572\n",
      "Iter 1000 | Energy = 2324.868319\n",
      "Iter 1000 | Energy = 2324.868078\n",
      "Iter 1000 | Energy = 2324.867851\n",
      "Iter 1000 | Energy = 2324.867635\n",
      "Iter 1000 | Energy = 2324.867431\n",
      "Iter 1000 | Energy = 2324.867237\n",
      "Iter 1000 | Energy = 2324.867054\n",
      "Iter 1000 | Energy = 2324.866880\n",
      "Iter 1000 | Energy = 2324.866716\n",
      "Iter 1000 | Energy = 2324.866560\n",
      "Iter 1000 | Energy = 2324.866412\n",
      "Iter 1000 | Energy = 2324.866272\n",
      "Iter 1000 | Energy = 2324.866139\n",
      "Iter 1000 | Energy = 2324.866013\n",
      "Iter 1000 | Energy = 2324.865894\n",
      "Iter 1000 | Energy = 2324.865781\n",
      "Iter 1000 | Energy = 2324.865674\n",
      "Iter 1000 | Energy = 2324.865573\n",
      "Iter 1000 | Energy = 2324.865477\n",
      "Iter 1000 | Energy = 2324.865386\n",
      "Iter 1000 | Energy = 2324.865300\n",
      "Iter 1000 | Energy = 2324.865218\n",
      "Iter 1000 | Energy = 2324.865141\n",
      "Iter 1000 | Energy = 2324.865067\n",
      "Iter 1000 | Energy = 2324.864998\n",
      "Iter 1000 | Energy = 2324.864932\n",
      "Iter 1000 | Energy = 2324.864870\n",
      "Iter 1000 | Energy = 2324.864811\n",
      "Iter 1000 | Energy = 2324.864755\n",
      "Iter 1000 | Energy = 2324.864701\n",
      "Iter 1000 | Energy = 2324.864651\n",
      "Iter 1000 | Energy = 2324.864604\n",
      "Iter 1000 | Energy = 2324.864558\n",
      "Iter 1000 | Energy = 2324.864516\n",
      "Iter 1000 | Energy = 2324.864475\n",
      "Iter 1000 | Energy = 2324.864437\n",
      "Iter 1000 | Energy = 2324.864400\n",
      "Iter 1000 | Energy = 2324.864366\n",
      "Iter 1000 | Energy = 2324.864333\n",
      "Iter 1000 | Energy = 2324.864302\n",
      "Iter 1000 | Energy = 2324.864273\n"
     ]
    }
   ],
   "source": [
    "layers[1][\"graph\"] = deepcopy(aa)\n",
    "layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "for i in range(1):\n",
    "    layers[-1][\"graph\"].synchronous_iteration()\n",
    "    top_down_modify_super_graph(layers[:])\n",
    "    top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d353210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000 | Energy = 2333.057544\n",
      "Iter 1000 | Energy = 2339.952725\n",
      "Iter 1000 | Energy = 2347.586783\n",
      "Iter 1000 | Energy = 2352.982718\n",
      "Iter 1000 | Energy = 2357.693991\n",
      "Iter 1000 | Energy = 2361.411014\n",
      "Iter 1000 | Energy = 2365.057877\n",
      "Iter 1000 | Energy = 2368.261650\n",
      "Iter 1000 | Energy = 2371.582692\n",
      "Iter 1000 | Energy = 2374.661040\n",
      "Iter 1000 | Energy = 2377.845726\n",
      "Iter 1000 | Energy = 2380.855475\n",
      "Iter 1000 | Energy = 2383.912300\n",
      "Iter 1000 | Energy = 2386.818810\n",
      "Iter 1000 | Energy = 2389.717175\n",
      "Iter 1000 | Energy = 2392.477801\n",
      "Iter 1000 | Energy = 2395.191398\n",
      "Iter 1000 | Energy = 2397.777201\n",
      "Iter 1000 | Energy = 2400.292543\n",
      "Iter 1000 | Energy = 2402.689672\n",
      "Iter 1000 | Energy = 2405.004386\n",
      "Iter 1000 | Energy = 2407.210328\n",
      "Iter 1000 | Energy = 2409.329563\n",
      "Iter 1000 | Energy = 2411.349136\n",
      "Iter 1000 | Energy = 2413.282492\n",
      "Iter 1000 | Energy = 2415.124784\n",
      "Iter 1000 | Energy = 2416.884155\n",
      "Iter 1000 | Energy = 2418.560459\n",
      "Iter 1000 | Energy = 2420.158657\n",
      "Iter 1000 | Energy = 2421.681164\n",
      "Iter 1000 | Energy = 2423.131086\n",
      "Iter 1000 | Energy = 2424.512092\n",
      "Iter 1000 | Energy = 2425.826246\n",
      "Iter 1000 | Energy = 2427.077689\n",
      "Iter 1000 | Energy = 2428.267934\n",
      "Iter 1000 | Energy = 2429.401148\n",
      "Iter 1000 | Energy = 2430.478569\n",
      "Iter 1000 | Energy = 2431.504155\n",
      "Iter 1000 | Energy = 2432.479022\n",
      "Iter 1000 | Energy = 2433.406803\n",
      "Iter 1000 | Energy = 2434.288568\n",
      "Iter 1000 | Energy = 2435.127586\n",
      "Iter 1000 | Energy = 2435.924912\n",
      "Iter 1000 | Energy = 2436.683451\n",
      "Iter 1000 | Energy = 2437.404254\n",
      "Iter 1000 | Energy = 2438.089883\n",
      "Iter 1000 | Energy = 2438.741377\n",
      "Iter 1000 | Energy = 2439.360990\n",
      "Iter 1000 | Energy = 2439.949741\n",
      "Iter 1000 | Energy = 2440.509609\n",
      "Iter 1000 | Energy = 2441.041583\n",
      "Iter 1000 | Energy = 2441.547402\n",
      "Iter 1000 | Energy = 2442.028017\n",
      "Iter 1000 | Energy = 2442.484955\n",
      "Iter 1000 | Energy = 2442.919122\n",
      "Iter 1000 | Energy = 2443.331864\n",
      "Iter 1000 | Energy = 2443.724038\n",
      "Iter 1000 | Energy = 2444.096829\n",
      "Iter 1000 | Energy = 2444.451042\n",
      "Iter 1000 | Energy = 2444.787726\n",
      "Iter 1000 | Energy = 2445.107631\n",
      "Iter 1000 | Energy = 2445.411687\n",
      "Iter 1000 | Energy = 2445.700589\n",
      "Iter 1000 | Energy = 2445.975163\n",
      "Iter 1000 | Energy = 2446.236054\n",
      "Iter 1000 | Energy = 2446.483995\n",
      "Iter 1000 | Energy = 2446.719578\n",
      "Iter 1000 | Energy = 2446.943460\n",
      "Iter 1000 | Energy = 2447.156182\n",
      "Iter 1000 | Energy = 2447.358332\n",
      "Iter 1000 | Energy = 2447.550405\n",
      "Iter 1000 | Energy = 2447.732926\n",
      "Iter 1000 | Energy = 2447.906349\n",
      "Iter 1000 | Energy = 2448.071144\n",
      "Iter 1000 | Energy = 2448.227722\n",
      "Iter 1000 | Energy = 2448.376508\n",
      "Iter 1000 | Energy = 2448.517875\n",
      "Iter 1000 | Energy = 2448.652204\n",
      "Iter 1000 | Energy = 2448.779835\n",
      "Iter 1000 | Energy = 2448.901109\n",
      "Iter 1000 | Energy = 2449.016336\n",
      "Iter 1000 | Energy = 2449.125822\n",
      "Iter 1000 | Energy = 2449.229849\n",
      "Iter 1000 | Energy = 2449.328691\n",
      "Iter 1000 | Energy = 2449.422605\n",
      "Iter 1000 | Energy = 2449.511837\n",
      "Iter 1000 | Energy = 2449.596620\n",
      "Iter 1000 | Energy = 2449.677176\n",
      "Iter 1000 | Energy = 2449.753714\n",
      "Iter 1000 | Energy = 2449.826436\n",
      "Iter 1000 | Energy = 2449.895530\n",
      "Iter 1000 | Energy = 2449.961180\n",
      "Iter 1000 | Energy = 2450.023554\n",
      "Iter 1000 | Energy = 2450.082818\n",
      "Iter 1000 | Energy = 2450.139126\n",
      "Iter 1000 | Energy = 2450.192625\n",
      "Iter 1000 | Energy = 2450.243455\n",
      "Iter 1000 | Energy = 2450.291750\n",
      "Iter 1000 | Energy = 2450.337635\n",
      "Iter 1000 | Energy = 2450.381231\n",
      "Iter 1000 | Energy = 2450.422653\n",
      "Iter 1000 | Energy = 2450.462008\n",
      "Iter 1000 | Energy = 2450.499399\n",
      "Iter 1000 | Energy = 2450.534925\n",
      "Iter 1000 | Energy = 2450.568678\n",
      "Iter 1000 | Energy = 2450.600747\n",
      "Iter 1000 | Energy = 2450.631216\n",
      "Iter 1000 | Energy = 2450.660165\n",
      "Iter 1000 | Energy = 2450.687669\n",
      "Iter 1000 | Energy = 2450.713801\n",
      "Iter 1000 | Energy = 2450.738629\n",
      "Iter 1000 | Energy = 2450.762218\n",
      "Iter 1000 | Energy = 2450.784630\n",
      "Iter 1000 | Energy = 2450.805923\n",
      "Iter 1000 | Energy = 2450.826154\n",
      "Iter 1000 | Energy = 2450.845375\n",
      "Iter 1000 | Energy = 2450.863637\n",
      "Iter 1000 | Energy = 2450.880988\n",
      "Iter 1000 | Energy = 2450.897473\n",
      "Iter 1000 | Energy = 2450.913135\n",
      "Iter 1000 | Energy = 2450.928015\n",
      "Iter 1000 | Energy = 2450.942153\n",
      "Iter 1000 | Energy = 2450.955585\n",
      "Iter 1000 | Energy = 2450.968347\n",
      "Iter 1000 | Energy = 2450.980472\n",
      "Iter 1000 | Energy = 2450.991992\n",
      "Iter 1000 | Energy = 2451.002937\n",
      "Iter 1000 | Energy = 2451.013336\n",
      "Iter 1000 | Energy = 2451.023215\n",
      "Iter 1000 | Energy = 2451.032602\n",
      "Iter 1000 | Energy = 2451.041520\n",
      "Iter 1000 | Energy = 2451.049993\n",
      "Iter 1000 | Energy = 2451.058043\n",
      "Iter 1000 | Energy = 2451.065691\n",
      "Iter 1000 | Energy = 2451.072958\n",
      "Iter 1000 | Energy = 2451.079862\n",
      "Iter 1000 | Energy = 2451.086421\n",
      "Iter 1000 | Energy = 2451.092653\n",
      "Iter 1000 | Energy = 2451.098574\n",
      "Iter 1000 | Energy = 2451.104199\n",
      "Iter 1000 | Energy = 2451.109544\n",
      "Iter 1000 | Energy = 2451.114622\n",
      "Iter 1000 | Energy = 2451.119446\n",
      "Iter 1000 | Energy = 2451.124030\n",
      "Iter 1000 | Energy = 2451.128384\n",
      "Iter 1000 | Energy = 2451.132522\n",
      "Iter 1000 | Energy = 2451.136453\n",
      "Iter 1000 | Energy = 2451.140188\n",
      "Iter 1000 | Energy = 2451.143736\n",
      "Iter 1000 | Energy = 2451.147107\n",
      "Iter 1000 | Energy = 2451.150310\n",
      "Iter 1000 | Energy = 2451.153353\n",
      "Iter 1000 | Energy = 2451.156244\n",
      "Iter 1000 | Energy = 2451.158991\n",
      "Iter 1000 | Energy = 2451.161601\n",
      "Iter 1000 | Energy = 2451.164080\n",
      "Iter 1000 | Energy = 2451.166436\n",
      "Iter 1000 | Energy = 2451.168674\n",
      "Iter 1000 | Energy = 2451.170801\n",
      "Iter 1000 | Energy = 2451.172821\n",
      "Iter 1000 | Energy = 2451.174740\n",
      "Iter 1000 | Energy = 2451.176564\n",
      "Iter 1000 | Energy = 2451.178297\n",
      "Iter 1000 | Energy = 2451.179943\n",
      "Iter 1000 | Energy = 2451.181507\n",
      "Iter 1000 | Energy = 2451.182993\n",
      "Iter 1000 | Energy = 2451.184405\n",
      "Iter 1000 | Energy = 2451.185746\n",
      "Iter 1000 | Energy = 2451.187020\n",
      "Iter 1000 | Energy = 2451.188231\n",
      "Iter 1000 | Energy = 2451.189381\n",
      "Iter 1000 | Energy = 2451.190474\n",
      "Iter 1000 | Energy = 2451.191512\n",
      "Iter 1000 | Energy = 2451.192499\n",
      "Iter 1000 | Energy = 2451.193436\n",
      "Iter 1000 | Energy = 2451.194326\n",
      "Iter 1000 | Energy = 2451.195173\n",
      "Iter 1000 | Energy = 2451.195976\n",
      "Iter 1000 | Energy = 2451.196740\n",
      "Iter 1000 | Energy = 2451.197466\n",
      "Iter 1000 | Energy = 2451.198155\n",
      "Iter 1000 | Energy = 2451.198810\n",
      "Iter 1000 | Energy = 2451.199432\n",
      "Iter 1000 | Energy = 2451.200023\n",
      "Iter 1000 | Energy = 2451.200585\n",
      "Iter 1000 | Energy = 2451.201118\n",
      "Iter 1000 | Energy = 2451.201625\n",
      "Iter 1000 | Energy = 2451.202107\n",
      "Iter 1000 | Energy = 2451.202565\n",
      "Iter 1000 | Energy = 2451.203000\n",
      "Iter 1000 | Energy = 2451.203413\n",
      "Iter 1000 | Energy = 2451.203805\n",
      "Iter 1000 | Energy = 2451.204178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     layers[abs_layer_idx][\u001b[33m\"\u001b[39m\u001b[33mgraph\u001b[39m\u001b[33m\"\u001b[39m], layers[abs_layer_idx][\u001b[33m\"\u001b[39m\u001b[33mBs\u001b[39m\u001b[33m\"\u001b[39m], layers[abs_layer_idx][\u001b[33m\"\u001b[39m\u001b[33mks\u001b[39m\u001b[33m\"\u001b[39m], layers[abs_layer_idx][\u001b[33m\"\u001b[39m\u001b[33mk2s\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mbottom_up_modify_abs_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     layers[abs_layer_idx][\u001b[33m\"\u001b[39m\u001b[33mgraph\u001b[39m\u001b[33m\"\u001b[39m].synchronous_iteration()\n\u001b[32m      4\u001b[39m     top_down_modify_super_graph(layers[:])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1290\u001b[39m, in \u001b[36mbottom_up_modify_abs_graph\u001b[39m\u001b[34m(layers, r_reduced, eta_damping)\u001b[39m\n\u001b[32m   1288\u001b[39m i, j = [v.variableID \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m f.adj_var_nodes]\n\u001b[32m   1289\u001b[39m vi, vj = abs_var_nodes[i], abs_var_nodes[j]\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m abs_f = \u001b[43mFactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeas_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1291\u001b[39m abs_f.type = \u001b[33m\"\u001b[39m\u001b[33mabs_between\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1292\u001b[39m abs_f.adj_beliefs = [vi.belief, vj.belief]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/abstraction-recovery/hierarchy/gbp/gbp.py:720\u001b[39m, in \u001b[36mFactor.__init__\u001b[39m\u001b[34m(self, factor_id, adj_var_nodes, measurement, measurement_lambda, meas_fn, jac_fn, loss, mahalanobis_threshold, wildfire, *args)\u001b[39m\n\u001b[32m    718\u001b[39m     \u001b[38;5;28mself\u001b[39m.adj_beliefs.append(NdimGaussian(adj_var_node.dofs))\n\u001b[32m    719\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages.append(NdimGaussian(adj_var_node.dofs))\u001b[38;5;66;03m#, eta=adj_var_node.prior.eta, lam=adj_var_node.prior.lam))\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages_prior.append(\u001b[43mNdimGaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_var_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdofs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m.messages_dist.append(np.zeros(adj_var_node.dofs))\n\u001b[32m    723\u001b[39m \u001b[38;5;28mself\u001b[39m.factor = NdimGaussian(\u001b[38;5;28mself\u001b[39m.dofs_conditional_vars)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/abstraction-recovery/hierarchy/utils/gaussian.py:16\u001b[39m, in \u001b[36mNdimGaussian.__init__\u001b[39m\u001b[34m(self, dimensionality, eta, lam)\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.lam = lam\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28mself\u001b[39m.lam = np.zeros([\u001b[38;5;28mself\u001b[39m.dim, \u001b[38;5;28mself\u001b[39m.dim])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    layers[abs_layer_idx][\"graph\"], layers[abs_layer_idx][\"Bs\"], layers[abs_layer_idx][\"ks\"], layers[abs_layer_idx][\"k2s\"] = bottom_up_modify_abs_graph(layers[:3])\n",
    "    layers[abs_layer_idx][\"graph\"].synchronous_iteration()\n",
    "    top_down_modify_super_graph(layers[:])\n",
    "    top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "    energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 531 | Energy = 2335.263449\n"
     ]
    }
   ],
   "source": [
    "top_down_modify_super_graph(layers[:])\n",
    "top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dac739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 531 | Energy = 2334.490358\n"
     ]
    }
   ],
   "source": [
    "layers[1][\"graph\"].synchronous_iteration()\n",
    "top_down_modify_base_and_abs_graph(layers[0:2])\n",
    "energy = energy_map(layers[0][\"graph\"], include_priors=True, include_factors=True)\n",
    "print(f\"Iter {_+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce39dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.08825805e-09,  7.00557252e-09])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(layers[0][\"graph\"].var_nodes[0].adj_factors[1].adj_beliefs[0].lam, layers[0][\"graph\"].var_nodes[0].adj_factors[1].adj_beliefs[0].eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5dd50e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.08825829e-09,  7.00557253e-09])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e157ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.08825799e-09,  7.00557250e-09,  3.94963401e+00, -2.51772335e+01,\n",
       "        2.59936671e+01, -3.96819499e+01,  3.85359407e+01, -6.20493563e+01,\n",
       "        1.33215562e+01, -7.12420831e+01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[1][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13e7b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.08825829e-09,  7.00557253e-09])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0][\"graph\"].var_nodes[0].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abc516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a5cfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Energy: 2348.0945'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_energy(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2302c901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03313253012048193"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2401-2324)/2324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3786d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2481.58433706"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2402*(1.03313253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a5e78c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.988972036403319\n",
      "Iter 1000 | Energy = 7.815849\n",
      "Iter 1000 | Energy = 6.885167\n",
      "Iter 1000 | Energy = 6.902868\n",
      "Iter 1000 | Energy = 6.909297\n",
      "Iter 1000 | Energy = 6.910688\n",
      "Iter 1000 | Energy = 6.911066\n",
      "Iter 1000 | Energy = 6.911152\n",
      "Iter 1000 | Energy = 6.911174\n",
      "Iter 1000 | Energy = 6.911179\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n",
      "Iter 1000 | Energy = 6.911181\n"
     ]
    }
   ],
   "source": [
    "N=6\n",
    "step=25\n",
    "prob=0.05\n",
    "radius=50 \n",
    "prior_prop=0.02\n",
    "prior_sigma=1\n",
    "odom_sigma=1\n",
    "layers = []\n",
    "layers = init_layers(N=N, step_size=step, loop_prob=prob, loop_radius=radius, prior_prop=prior_prop, seed=2001)\n",
    "pair_idx = 0\n",
    "# Create GBP graph\n",
    "gbp_graph = build_noisy_pose_graph(layers[0][\"nodes\"], layers[0][\"edges\"],\n",
    "                                    prior_sigma=prior_sigma,\n",
    "                                    odom_sigma=odom_sigma,\n",
    "                                    seed=2001)\n",
    "layers[0][\"graph\"] = gbp_graph\n",
    "\n",
    "super_nodes, super_edges, node_map = fuse_to_super_order(layers[0][\"nodes\"], layers[0][\"edges\"], k=2, layer_idx=1, tail_heavy=True)\n",
    "layers.append({\"name\":f\"super{1}\", \"nodes\":super_nodes, \"edges\":super_edges, \"node_map\":node_map})\n",
    "layers[0][\"graph\"].synchronous_iteration()\n",
    "layers[1][\"graph\"] = build_super_graph(layers,eta_damping=0)\n",
    "supergraph = layers[-1][\"graph\"]\n",
    "\n",
    "\n",
    "total = 0\n",
    "a = layers[0][\"graph\"].joint_distribution_cov()[0].reshape(layers[0][\"graph\"].n_var_nodes,2)[:,:]\n",
    "for i,v in enumerate(layers[0][\"graph\"].var_nodes[:layers[0][\"graph\"].n_var_nodes]):\n",
    "    gt = np.asarray(v.GT[0:2], dtype=float)\n",
    "    r = np.asarray(a[i][0:2], dtype=float) - gt\n",
    "    total += 0.5 * float(r.T @ r)\n",
    "print(total)\n",
    "\n",
    "abs_nodes, abs_edges = copy_to_abs(layers[1][\"nodes\"], layers[1][\"edges\"], 2)\n",
    "\n",
    "# Ensure super graph has run at least once\n",
    "#layers[1][\"graph\"].synchronous_iteration() \n",
    "r = 2\n",
    "layers.append({\"name\":f\"abs{1}\", \"nodes\":abs_nodes, \"edges\":abs_edges})\n",
    "layers[2][\"graph\"], layers[2][\"Bs\"], layers[2][\"ks\"], layers[2][\"k2s\"] = build_abs_graph(layers, r_reduced=r)\n",
    "\n",
    "vg = VGraph(layers, eta_damping=0)\n",
    "for i in range(50):\n",
    "    vg.layers = layers\n",
    "    vg.layers = vg.vloop()\n",
    "    layers = vg.layers\n",
    "    refresh_gbp_results(layers)\n",
    "    energy = supergraph.energy_map(include_priors=True, include_factors=True)\n",
    "    print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8375df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00988371909967853\n"
     ]
    }
   ],
   "source": [
    "print((6.988972036403319-6.919895)/6.988972036403319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a395886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008997083362999205\n"
     ]
    }
   ],
   "source": [
    "print((6.988972036403319-6.911181)/6.988972036403319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3caf13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 010 | Energy = 6.911181\n"
     ]
    }
   ],
   "source": [
    "energy = layers[1][\"graph\"].energy_map(include_priors=True, include_factors=True)\n",
    "print(f\"Iter {it+1:03d} | Energy = {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "841bcb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.24417217, -24.77807379])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2][\"graph\"].var_nodes[0].mu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
