{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c973d09-a253-4b75-88e2-4e1bf637302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import imgui\n",
    "import moderngl\n",
    "from pyrr import Matrix44\n",
    "import moderngl_window as mglw\n",
    "from moderngl_window import geometry\n",
    "from moderngl_window.integrations.imgui import ModernglWindowRenderer\n",
    "import PIL\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Gaussian:\n",
    "    def __init__(self, eta, Lam):\n",
    "        self.eta = eta\n",
    "        self.Lam = Lam\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.eta, self.Lam), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "    def mu(self):\n",
    "        return jnp.where(\n",
    "            jnp.allclose(self.Lam, 0), self.eta, jnp.linalg.solve(self.Lam, self.eta)\n",
    "        )\n",
    "    \n",
    "    def sigma(self):\n",
    "        return jnp.linalg.inv(self.Lam)\n",
    "\n",
    "    def zero_like(self):\n",
    "        return Gaussian(jnp.zeros_like(self.eta), jnp.zeros_like(self.Lam))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Gaussian(eta={self.eta}, lam={self.Lam})\"\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Gaussian(self.eta + other.eta, self.Lam + other.Lam)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Gaussian(self.eta - other.eta, self.Lam - other.Lam)\n",
    "\n",
    "    def copy(self):\n",
    "        return Gaussian(self.eta.copy(), self.Lam.copy())\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Variable:\n",
    "    var_id: int\n",
    "    belief: Gaussian\n",
    "    msgs: Gaussian\n",
    "    adj_factor_idx: jnp.array\n",
    "\n",
    "    def __init__(self, var_id, belief, msgs, adj_factor_idx):\n",
    "        self.var_id = var_id\n",
    "        self.belief = belief\n",
    "        self.msgs = msgs\n",
    "        self.adj_factor_idx = adj_factor_idx\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.var_id, self.belief, self.msgs, self.adj_factor_idx), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "\n",
    "@register_pytree_node_class\n",
    "class Factor:\n",
    "    factor_id: jnp.array\n",
    "    z: jnp.ndarray\n",
    "    z_Lam: jnp.ndarray\n",
    "    threshold: jnp.ndarray\n",
    "    potential: Gaussian\n",
    "    adj_var_id: jnp.array\n",
    "    adj_var_idx: jnp.array\n",
    "\n",
    "    def __init__(\n",
    "        self, factor_id, z, z_Lam, threshold, potential, adj_var_id, adj_var_idx\n",
    "    ):\n",
    "        self.factor_id = factor_id\n",
    "        self.z = z\n",
    "        self.z_Lam = z_Lam\n",
    "        self.threshold = threshold\n",
    "        self.potential = potential\n",
    "        self.adj_var_id = adj_var_id\n",
    "        self.adj_var_idx = adj_var_idx\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (\n",
    "            self.factor_id,\n",
    "            self.z,\n",
    "            self.z_Lam,\n",
    "            self.threshold,\n",
    "            self.potential,\n",
    "            self.adj_var_id,\n",
    "            self.adj_var_idx,\n",
    "        ), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*children)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"i\", \"j\"])\n",
    "def marginalize(gaussians: Gaussian, i, j): # Equ. (46), (47); Compute msg to i:j Variables from connected factors\n",
    "    eta = gaussians.eta\n",
    "    Lam = gaussians.Lam\n",
    "    k = eta.size\n",
    "    idx = jnp.arange(0, k)\n",
    "    aa = idx[i:j] # index from i to j-1\n",
    "    bb = jnp.concat([idx[:i], idx[j:]]) # rest\n",
    "    aa_eta = eta[aa]\n",
    "    bb_eta = eta[bb]\n",
    "    aa_Lam = Lam[aa[:, None], aa]\n",
    "    ab_Lam = Lam[aa[:, None], bb]\n",
    "    bb_Lam = Lam[bb][:, bb]\n",
    "    if bb_Lam.size == 0:\n",
    "        return Gaussian(aa_eta, aa_Lam)\n",
    "    # print(\"How large? \", bb_Lam.shape)\n",
    "\n",
    "    bb_Cov = jnp.linalg.inv(bb_Lam)\n",
    "    eta = aa_eta - ab_Lam @ bb_Cov @ bb_eta\n",
    "    Lam = aa_Lam - ab_Lam @ bb_Cov @ ab_Lam.T\n",
    "    return Gaussian(eta, Lam)\n",
    "\n",
    "\n",
    "def tree_stack(tree, axis=0, use_np=True):\n",
    "    if use_np:\n",
    "        return jax.tree.map(lambda *v: jnp.array(np.stack(v, axis=axis)), *tree)\n",
    "    return jax.tree.map(lambda *v: jnp.stack(v, axis=axis), *tree)\n",
    "\n",
    "\n",
    "def h_fn(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def h2_fn(xs):\n",
    "    \"\"\"\n",
    "    xs: shape (2, D), where:\n",
    "        - xs[0] is x1\n",
    "        - xs[1] is x2\n",
    "    \"\"\"\n",
    "    x1 = xs[0]\n",
    "    x2 = xs[1]\n",
    "\n",
    "    #jax.debug.print(\"Shape of x2 - x1: {}\", (x2 - x1).shape)\n",
    "\n",
    "    return x2 - x1\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def update_belief(var: Variable, ftov_msgs): # Calculate Eq. (7)\n",
    "    belief = var.belief.zero_like()\n",
    "    for i in range(ftov_msgs.eta.shape[0]):\n",
    "        belief = belief * Gaussian(ftov_msgs.eta[i], ftov_msgs.Lam[i])\n",
    "\n",
    "\n",
    "    return belief\n",
    "\n",
    "\"\"\"\n",
    "# @jax.jit\n",
    "def update_belief(varis: Variable, ftov_msgs):\n",
    "    belief = varis.belief.zero_like()\n",
    "\n",
    "    mask = (varis.adj_factor_idx >= 0)[..., None]\n",
    "\n",
    "    # Debug: print mask\n",
    "    jax.debug.print(\"mask = {}\", mask)\n",
    "\n",
    "    varis.msgs.eta = varis.msgs.eta * mask\n",
    "    varis.msgs.Lam = varis.msgs.Lam * mask[..., None]\n",
    "\n",
    "    # Debug: print masked eta\n",
    "    jax.debug.print(\"masked msgs.eta = {}\", varis.msgs.eta)\n",
    "\n",
    "    for i in range(ftov_msgs.eta.shape[0]):\n",
    "        msg = Gaussian(ftov_msgs.eta[i], ftov_msgs.Lam[i])\n",
    "        belief = belief * msg\n",
    "\n",
    "    return belief\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def compute_vtof_msgs(var: Variable, ftov_msgs): # Eq.(19); do for each variable (x_m)\n",
    "    vtof_msgs = []\n",
    "    for i, idx in enumerate(var.adj_factor_idx): # for each f_si connected to x_m...\n",
    "        msg = var.belief / Gaussian(ftov_msgs.eta[i], ftov_msgs.Lam[i]) # Eq.(19) LHS subscript of SUM\n",
    "        eta = jnp.where(idx < 0, msg.zero_like().eta, msg.eta) # Those not connected should not affect the calculation (idx < 0)\n",
    "        Lam = jnp.where(idx < 0, msg.zero_like().Lam, msg.Lam) # The reason to not using \"if\" (while it's per-element) is to optimize better\n",
    "        vtof_msgs.append(Gaussian(eta, Lam)) # append (x_m -> f_si)\n",
    "    return tree_stack(vtof_msgs, use_np=False) # [(x_m -> f_s1), (x_m -> f_s2), ... ] # The length is Ni_v\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"h_fn\"])\n",
    "def factor_energy(factor, xs, h_fn):\n",
    "    h = h_fn(xs)\n",
    "    z = factor.z\n",
    "    z_Lam = factor.z_Lam\n",
    "    r = z - h\n",
    "    return 0.5 * r @ z_Lam @ r.T\n",
    "\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnames=[\"h_fn\", \"w\"])\n",
    "def factor_update(factor, xs, h_fn, w):\n",
    "    h = h_fn(xs)\n",
    "    J = jax.jacrev(h_fn)(xs).reshape(h.size, xs.size) # Jacobian auto-diff (J_s)\n",
    "    z = factor.z # I think this is a vector\n",
    "    z_Lam = factor.z_Lam\n",
    "    \n",
    "    r = z - h.reshape(-1) # TODO: reshape can be problematic\n",
    "    s = w(r.T @ z_Lam @ r, factor.threshold) # Scale to consider Robust Loss\n",
    "    Lam = s * J.T @ z_Lam @ J # Eq. (36)\n",
    "    eta = s * J.T @ z_Lam @ (J @ xs.reshape(-1) + r) # TODO: reshape can be problematic; Eq. (36); xs should be a vector\n",
    "    return Gaussian(eta, Lam) # Factor; represented w.r.t. neighboring variables xs\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def compute_ftov_msg(factor, vtof_msgs): # Ch 3.5 Message Passing at a Factor Node\n",
    "    N_adj, dim = vtof_msgs.eta.shape # #Ni_v * 3 (#Varis is vmapped)\n",
    "    pot = factor.potential.copy() # log(f_s), but for only a specific variable a factor is connected to.\n",
    "    i = 0\n",
    "    for n in range(N_adj): # Add all! (Produce all)\n",
    "        j = i + dim\n",
    "        pot.eta = pot.eta.at[i:j].add(vtof_msgs.eta[n])\n",
    "        pot.Lam = pot.Lam.at[i:j, i:j].add(vtof_msgs.Lam[n])\n",
    "        i = j\n",
    "\n",
    "\n",
    "    ftov_msgs = []\n",
    "    i = 0\n",
    "    for n in range(N_adj):\n",
    "        j = i + dim\n",
    "        pot_m_1 = pot.copy()\n",
    "        pot_m_1.eta = pot_m_1.eta.at[i:j].add(-vtof_msgs.eta[n]) # Subtract direction of going out! (42)\n",
    "        pot_m_1.Lam = pot_m_1.Lam.at[i:j, i:j].add(-vtof_msgs.Lam[n]) # (43)\n",
    "        msg = marginalize(pot_m_1, i, j) # (46), (47)\n",
    "        ftov_msgs.append(msg)\n",
    "        i = j\n",
    "    return tree_stack(ftov_msgs, use_np=False)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_variable(varis): # Update belief with receiving msgs and calculate msg to factors; varis.msgs are up-to-date and varis.belief are not\n",
    "    varis.belief = jax.vmap(update_belief)(varis, varis.msgs) # Eq. (7); varis.msgs is receiving msgs (ftov)\n",
    "    vtof_msgs = jax.vmap(compute_vtof_msgs)(varis, varis.msgs) # Variable -> Factor Msg; Eq. (19)\n",
    "    linpoints = jax.vmap(lambda x: x.mu())(varis.belief) # Current avg of belief! Belief is posterior\n",
    "\n",
    "    return varis, vtof_msgs, linpoints # vtof msgs: # Var * # Var-direction (factor, Ni_v) msgs\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"f\", \"w\"])\n",
    "def update_factor(facs, varis, vtof_msgs, linpoints, f, w): # f is factor function, w is robustifier\n",
    "    vtof_msgs_reordered = jax.tree_util.tree_map( # Variable to factor messages to specific (variable, factor; or variable-direction) pair\n",
    "        lambda x: x[facs.adj_var_id, facs.adj_var_idx], vtof_msgs # id: Variable id (one end), idx: direction (another end)\n",
    "    )\n",
    "    linpoints_reordered = jax.tree_util.tree_map(\n",
    "        lambda x: x[facs.adj_var_id], linpoints # Reorder linpoints by adj_var_id: variables' mean for factors' one ends\n",
    "    )\n",
    "    \n",
    "    facs.potential = jax.vmap(factor_update, in_axes=(0, 0, None, None))( # Calculate each factor potential (f_s(x, x_1, ..., x_M) of Eq. (15))\n",
    "        facs, linpoints_reordered, f, w # Each factor contribution of variable-direction pair (factor: variable-direction pair)\n",
    "    ) # 1 or 2-dimensional!! (gradient / prior factor or smoothness factor)\n",
    "    ftov_msgs = jax.vmap(compute_ftov_msg)(facs, vtof_msgs_reordered) # ftov calculation by Eq. (15), with potential f_s, and msg vtof\n",
    "\n",
    "    varis.msgs.eta = varis.msgs.eta.at[facs.adj_var_id, facs.adj_var_idx].set( # Setting varis' receiving messages\n",
    "        ftov_msgs.eta\n",
    "    )\n",
    "    varis.msgs.Lam = varis.msgs.Lam.at[facs.adj_var_id, facs.adj_var_idx].set(\n",
    "        ftov_msgs.Lam\n",
    "    )\n",
    "\n",
    "    mask = (varis.adj_factor_idx >= 0)[..., None]\n",
    "    varis.msgs.eta = varis.msgs.eta * mask\n",
    "    varis.msgs.Lam = varis.msgs.Lam * mask[..., None]\n",
    "\n",
    "    return facs, varis\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def huber(e, t):\n",
    "    x = jnp.sqrt(e)\n",
    "    return jnp.where(x <= t, 1.0, t / x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def l2(e, _):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2a05df-380e-43d7-aa11-ff716dc69aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pose_slam_graph(N, prior_meas, between_meas, prior_std=0.05, odom_std=0.05, Ni_v=10, D=2):\n",
    "    \"\"\"\n",
    "    Build a 2D pose-SLAM factor graph with:\n",
    "    - N variable nodes (each 2D position)\n",
    "    - Prior measurements (with strong precision)\n",
    "    - Between measurements (with moderate precision from noise_std)\n",
    "    \n",
    "    Parameters:\n",
    "    - N: number of variables\n",
    "    - prior_meas: list of (i, z) where z is the prior measurement at node i\n",
    "    - between_meas: list of (i, j, z) where z is relative measurement from i to j\n",
    "    - noise_std: standard deviation for between measurements (list or array of length D)\n",
    "    - Ni_v: number of factor connections per variable (default 5)\n",
    "    - D: dimension of each variable (default 2)\n",
    "    \n",
    "    Returns:\n",
    "    - varis: Variable object\n",
    "    - prior_facs: Factor object for priors\n",
    "    - between_facs: Factor object for between factors\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: Initialize Variable nodes ===\n",
    "    var_ids = jnp.arange(N, dtype=jnp.int32)\n",
    "    belief = Gaussian(jnp.zeros((N, D)), jnp.tile(jnp.eye(D), (N, 1, 1)))  # initial mean 0, covariance I\n",
    "    msgs = Gaussian(jnp.zeros((N, Ni_v, D)), jnp.zeros((N, Ni_v, D, D)))  # messages (eta, Lambda) to each factor port\n",
    "    adj_factor_idx = -jnp.ones((N, Ni_v), dtype=jnp.int32)  # -1 indicates no connected factor at this port\n",
    "\n",
    "    varis = Variable(var_ids, belief, msgs, adj_factor_idx)\n",
    "\n",
    "    # === Step 2: Build Prior Factors (strong precision for anchoring the graph) ===\n",
    "    prior_factor_id = []\n",
    "    prior_z = []\n",
    "    prior_z_Lam = []\n",
    "    prior_threshold = []\n",
    "    prior_adj_var_id = []\n",
    "    prior_adj_var_idx = []\n",
    "\n",
    "    fac_counter = 0  # global factor ID counter\n",
    "\n",
    "    for (i, z) in prior_meas:\n",
    "        prior_factor_id.append(fac_counter)\n",
    "        prior_z.append(jnp.array(z))\n",
    "\n",
    "        # Very weak prior: large noise variance -> small precision\n",
    "        prior_z_Lam.append(jnp.eye(D) / prior_std)  # shape (D, D)\n",
    "\n",
    "        prior_threshold.append(1.0)\n",
    "        prior_adj_var_id.append([i])     # only connected to variable i\n",
    "        prior_adj_var_idx.append([0])     # use port 0 for prior\n",
    "\n",
    "        varis.adj_factor_idx = varis.adj_factor_idx.at[i, 0].set(fac_counter)\n",
    "        fac_counter += 1\n",
    "\n",
    "    prior_facs = Factor(\n",
    "        factor_id=jnp.array(prior_factor_id),\n",
    "        z=jnp.stack(prior_z),\n",
    "        z_Lam=jnp.stack(prior_z_Lam),\n",
    "        threshold=jnp.array(prior_threshold),\n",
    "        potential=None,\n",
    "        adj_var_id=jnp.array(prior_adj_var_id),\n",
    "        adj_var_idx=jnp.array(prior_adj_var_idx),\n",
    "    )\n",
    "\n",
    "    # === Step 3: Build Between Factors (relative pose measurements) ===\n",
    "    between_factor_id = []\n",
    "    between_z = []\n",
    "    between_z_Lam = []\n",
    "    between_threshold = []\n",
    "    between_adj_var_id = []\n",
    "    between_adj_var_idx = []\n",
    "\n",
    "    for (i, j, z) in between_meas:\n",
    "        between_factor_id.append(fac_counter)\n",
    "        between_z.append(jnp.array(z))\n",
    "\n",
    "        # Between-factor noise: use provided noise_std to compute precision\n",
    "        between_z_Lam.append(jnp.diag(1.0 / (jnp.ones(D)*odom_std)   ))  # shape (D, D)\n",
    "\n",
    "        between_threshold.append(1.0)\n",
    "\n",
    "        # Assign first empty port >=1 to variable i and j\n",
    "        port_i = int(jnp.argmax(varis.adj_factor_idx[i, 1:] == -1)) + 1\n",
    "        port_j = int(jnp.argmax(varis.adj_factor_idx[j, 1:] == -1)) + 1\n",
    "        varis.adj_factor_idx = varis.adj_factor_idx.at[i, port_i].set(fac_counter)\n",
    "        varis.adj_factor_idx = varis.adj_factor_idx.at[j, port_j].set(fac_counter)\n",
    "\n",
    "        between_adj_var_id.append([i, j])\n",
    "        between_adj_var_idx.append([port_i, port_j])\n",
    "        fac_counter += 1\n",
    "\n",
    "    between_facs = Factor(\n",
    "        factor_id=jnp.array(between_factor_id),\n",
    "        z=jnp.stack(between_z),\n",
    "        z_Lam=jnp.stack(between_z_Lam),\n",
    "        threshold=jnp.array(between_threshold),\n",
    "        potential=None,\n",
    "        adj_var_id=jnp.array(between_adj_var_id),\n",
    "        adj_var_idx=jnp.array(between_adj_var_idx),\n",
    "    )\n",
    "\n",
    "    return varis, prior_facs, between_facs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f81753-7078-472e-93fa-7b8e232ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_slam_data(H=16, W=16, dx=1.0, dy=1.0, prior_noise_std=0.05, odom_noise_std=0.05, seed=0):\n",
    "    \"\"\"\n",
    "    Generate 2D SLAM data over a regular H x W grid.\n",
    "\n",
    "    Each variable is a node located at position (j*dx, i*dy), where i is row index and j is column index.\n",
    "    Relative pose measurements (between factors) are added between horizontal and vertical neighbors.\n",
    "    Each variable also receives a weak but accurate prior to ensure the global graph is well-constrained.\n",
    "\n",
    "    Args:\n",
    "        H: number of rows in the grid\n",
    "        W: number of columns in the grid\n",
    "        dx: horizontal spacing between grid points\n",
    "        dy: vertical spacing between grid points\n",
    "        odom_noise_std: standard deviation of noise added to relative measurements (between factors)\n",
    "        prior_std: standard deviation for prior factors (should be much larger than odom_noise_std to make priors weak)\n",
    "        seed: random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        positions: (N, 2) array of ground-truth positions (N = H * W)\n",
    "        prior_meas: list of (i, z) where i is variable index and z is its true position\n",
    "        between_meas: list of (i, j, z) where (i, j) is a measurement edge and z is the noisy relative pose from i to j\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = H * W  # total number of variables\n",
    "\n",
    "    # Step 1: Generate ground-truth positions on the grid\n",
    "    positions = []\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            x = j * dx\n",
    "            y = i * dy\n",
    "            positions.append([x, y])\n",
    "    positions = np.array(positions)  # shape (N, 2)\n",
    "\n",
    "    # Step 2: Add weak but accurate prior for each variable\n",
    "    prior_meas = []\n",
    "    for idx, pos in enumerate(positions):\n",
    "        noise = np.random.randn(2) * prior_noise_std\n",
    "        z = (pos + noise).tolist()\n",
    "        prior_meas.append((idx, z))  # accurate measurement with weak information will be set in z_Lam later\n",
    "\n",
    "    # Step 3: Add noisy relative pose measurements (between factors)\n",
    "    between_meas = []\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            idx = i * W + j  # flat index of (i, j)\n",
    "\n",
    "            # Horizontal neighbor (i, j) -> (i, j+1)\n",
    "            if j < W - 1:\n",
    "                nbr = i * W + (j + 1)\n",
    "                rel = positions[nbr] - positions[idx]  # ideal relative translation\n",
    "                noise = np.random.randn(2) * odom_noise_std\n",
    "                z = (rel + noise).tolist()\n",
    "                between_meas.append((idx, nbr, z))\n",
    "\n",
    "            # Vertical neighbor (i, j) -> (i+1, j)\n",
    "            if i < H - 1:\n",
    "                nbr = (i + 1) * W + j\n",
    "                rel = positions[nbr] - positions[idx]\n",
    "                noise = np.random.randn(2) * odom_noise_std\n",
    "                #z = (rel + noise).tolist()\n",
    "                z = noise.tolist()\n",
    "                between_meas.append((idx, nbr, z))\n",
    "\n",
    "    return positions, prior_meas, between_meas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043bbacd-2b85-4bf9-b5ed-ad0d6b27b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbp_solve(varis, prior_facs, between_facs, num_iters=50, visualize=False, prior_h=h_fn, between_h=h2_fn):\n",
    "    energy_log = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Step 1: Variable update\n",
    "        varis, vtof_msgs, linpoints = update_variable(varis)\n",
    "\n",
    "        # Step 2: Factor update\n",
    "        prior_facs, varis = update_factor(prior_facs, varis, vtof_msgs, linpoints, prior_h, l2)\n",
    "        between_facs, varis = update_factor(between_facs, varis, vtof_msgs, linpoints, between_h, l2)\n",
    "\n",
    "\n",
    "        if visualize:\n",
    "            # Step 3: Energy computation\n",
    "            prior_energy = jnp.sum(jax.vmap(factor_energy, in_axes=(0, 0, None))(\n",
    "                prior_facs, linpoints[prior_facs.adj_var_id[:, 0]], prior_h\n",
    "            ))\n",
    "    \n",
    "            between_energy = jnp.sum(jax.vmap(factor_energy, in_axes=(0, 0, None))(\n",
    "                between_facs, linpoints[between_facs.adj_var_id], between_h\n",
    "            ))\n",
    "    \n",
    "            energy = prior_energy + between_energy\n",
    "            energy_log.append(energy)\n",
    "        \n",
    "\n",
    "    # Step 4: Keep linpoints\n",
    "    linpoints = jax.vmap(lambda x: x.mu())(varis.belief)  \n",
    "        \n",
    "    return varis, prior_facs, between_facs, np.array(energy_log), linpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa86002-2750-4763-a8c8-cf01c89bbe4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22141.65  ,  4695.386 ,  2141.432 ,  1835.2019,  1699.0979,\n",
       "        1624.6194,  1578.5364,  1545.902 ,  1521.1299,  1501.608 ,\n",
       "        1485.6711,  1472.565 ,  1461.5751,  1452.3848,  1444.5889,\n",
       "        1438.0205,  1432.4224,  1427.6914,  1423.6506,  1420.2307,\n",
       "        1417.3069,  1414.8302,  1412.7115,  1410.9165,  1409.3801,\n",
       "        1408.0781,  1406.9634,  1406.0183,  1405.2095,  1404.5233,\n",
       "        1403.936 ,  1403.4377,  1403.0112,  1402.6493,  1402.3392,\n",
       "        1402.0764,  1401.8511,  1401.6602,  1401.4963,  1401.3574,\n",
       "        1401.2386,  1401.1376,  1401.0508,  1400.9775,  1400.9146,\n",
       "        1400.8611,  1400.8153,  1400.7762,  1400.7427,  1400.7146],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions, prior_meas, between_meas = generate_grid_slam_data(H=16, W=16, prior_noise_std=0.5, odom_noise_std=0.5)\n",
    "varis, prior_facs, between_facs = build_pose_slam_graph(N=256, prior_meas=prior_meas, between_meas=between_meas, \n",
    "                                                        prior_std=1, odom_std=0.1,\n",
    "                                                        Ni_v=10, D=2)\n",
    "\n",
    "cpu_device = jax.devices(\"cpu\")[0]\n",
    "varis= jax.device_put(varis, cpu_device)\n",
    "prior_facs = jax.device_put(prior_facs, cpu_device)\n",
    "between_facs = jax.device_put(between_facs, cpu_device)\n",
    "\n",
    "\"\"\"\n",
    "print(\"var_id:\", varis.var_id.shape)            # should be (N,)\n",
    "print(\"belief.eta:\", varis.belief.eta.shape)    # should be (N, D)\n",
    "print(\"belief.Lam:\", varis.belief.Lam.shape)    # should be (N, D, D)\n",
    "print(\"msgs.eta:\", varis.msgs.eta.shape)        # should be (N, Ni_v, D)\n",
    "print(\"msgs.Lam:\", varis.msgs.Lam.shape)        # should be (N, Ni_v, D, D)\n",
    "print(\"adj_factor_idx:\", varis.adj_factor_idx.shape)  # should be (N, Ni_v)\n",
    "\"\"\"\n",
    "\n",
    "varis, prior_facs, between_facs, energy_log, linpoints = gbp_solve(\n",
    "    varis, prior_facs, between_facs, num_iters=50, visualize=True\n",
    ")\n",
    "energy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7361946-b00f-4733-958d-e49ad6fbf58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22281c89-3777-4144-89db-b84657e3df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h3_fn(x):\n",
    "    \"\"\"\n",
    "    Predicts measurement h(x) for coarse prior.\n",
    "\n",
    "    Input:\n",
    "        x: (8,) → 4 stacked fine-level variables (each 2D)\n",
    "    Output:\n",
    "        z_hat: (16,) = [x0, x1, x2, x3, x1-x0, x2-x0, x3-x1, x3-x2]\n",
    "    \"\"\"\n",
    "    x = x.reshape(-1)\n",
    "    \n",
    "    x0 = x[0:2]\n",
    "    x1 = x[2:4]\n",
    "    x2 = x[4:6]\n",
    "    x3 = x[6:8]\n",
    "\n",
    "    # 4 priors (just xi)\n",
    "    z_hat_0 = x0\n",
    "    z_hat_1 = x1\n",
    "    z_hat_2 = x2\n",
    "    z_hat_3 = x3\n",
    "\n",
    "    # 4 internal between\n",
    "    z_hat_4 = x1 - x0\n",
    "    z_hat_5 = x2 - x0\n",
    "    z_hat_6 = x3 - x1\n",
    "    z_hat_7 = x3 - x2\n",
    "\n",
    "    return jnp.concatenate([\n",
    "        z_hat_0, z_hat_1, z_hat_2, z_hat_3,\n",
    "        z_hat_4, z_hat_5, z_hat_6, z_hat_7\n",
    "    ])\n",
    "\n",
    "\n",
    "def h4_fn(xs):\n",
    "    \"\"\"\n",
    "    Predicts coarse between measurement h(xs) where:\n",
    "      - xs[0] is coarse variable i (8D)\n",
    "      - xs[1] is coarse variable j (8D)\n",
    "    Fixed: uses v01→v02 and v11→v12 edges\n",
    "    \n",
    "    Returns:\n",
    "        z_hat: shape (4,) = two 2D relative positions\n",
    "    \"\"\"\n",
    "    jax.debug.print(\"xs = {}\", xs)\n",
    "    xi = xs[0]\n",
    "    xj = xs[1]\n",
    "\n",
    "    # First residual: xj[0:2] - xi[2:4]  (v02 - v01)\n",
    "    r1 = xj[0:2] - xi[2:4]\n",
    "\n",
    "    # Second residual: xj[4:6] - xi[6:8] (v12 - v11)\n",
    "    r2 = xj[4:6] - xi[6:8]\n",
    "\n",
    "    jax.debug.print(\"xs = {}\", xs)\n",
    "    return jnp.concatenate([r1, r2])  # shape (4,)\n",
    "\n",
    "\n",
    "def h5_fn(xs):\n",
    "    \"\"\"\n",
    "    Predicts coarse between measurement h(xs) where:\n",
    "      - xs[0] is coarse variable i (8D)\n",
    "      - xs[1] is coarse variable j (8D)\n",
    "    Fixed: uses v10→v20 and v11→v21 edges\n",
    "\n",
    "    Returns:\n",
    "        z_hat: shape (4,) = two 2D relative positions\n",
    "    \"\"\"\n",
    "    xi = xs[0]\n",
    "    xj = xs[1]\n",
    "\n",
    "    # First residual: xj[0:2] - xi[4:6]  (v20 - v10)\n",
    "    r1 = xj[0:2] - xi[4:6]\n",
    "\n",
    "    # Second residual: xj[2:4] - xi[6:8] (v21 - v11)\n",
    "    r2 = xj[2:4] - xi[6:8]\n",
    "\n",
    "    return jnp.concatenate([r1, r2])  # shape (4,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6faeffc-714c-4311-a901-826f63f457bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_coarse_slam_graph(\n",
    "    varis_fine: Variable,\n",
    "    prior_facs_fine: Factor,\n",
    "    between_facs_fine: Factor,\n",
    "    H: int, W: int,\n",
    "    stride: int = 2,\n",
    "    prior_std: float = 1.0,\n",
    "    between_std: float = 0.1,\n",
    ") -> Tuple[Variable, Factor, Factor]:\n",
    "    D = 2\n",
    "    patch_map: Dict[int, List[int]] = {}\n",
    "    fine_to_patch: Dict[int, Tuple[int, int]] = {}\n",
    "    coarse_var_id = 0\n",
    "\n",
    "    coarse_beliefs = []\n",
    "    coarse_msgs_eta = []\n",
    "    coarse_msgs_Lam = []\n",
    "    coarse_adj_factor_idx = []\n",
    "\n",
    "    # === 1. Build Coarse Variables ===\n",
    "    for i in range(0, H - 1, stride):\n",
    "        for j in range(0, W - 1, stride):\n",
    "            v00 = i * W + j\n",
    "            v01 = v00 + 1\n",
    "            v10 = v00 + W\n",
    "            v11 = v10 + 1\n",
    "            patch = [v00, v01, v10, v11]\n",
    "            patch_map[coarse_var_id] = patch\n",
    "            for k, vid in enumerate(patch):\n",
    "                fine_to_patch[vid] = (coarse_var_id, k)\n",
    "\n",
    "            eta = jnp.concatenate([varis_fine.belief.eta[v] for v in patch])\n",
    "            Lam = jax.scipy.linalg.block_diag(*[varis_fine.belief.Lam[v] for v in patch])\n",
    "            coarse_beliefs.append(Gaussian(eta, Lam))\n",
    "\n",
    "            Ni_v = 15\n",
    "            coarse_msgs_eta.append(jnp.zeros((Ni_v, 8)))\n",
    "            coarse_msgs_Lam.append(jnp.zeros((Ni_v, 8, 8)))\n",
    "            coarse_adj_factor_idx.append(-jnp.ones(Ni_v, dtype=jnp.int32))\n",
    "            coarse_var_id += 1\n",
    "\n",
    "    varis_coarse = Variable(\n",
    "        var_id=jnp.arange(len(patch_map)),\n",
    "        belief=tree_stack(coarse_beliefs, axis=0),\n",
    "        msgs=Gaussian(jnp.stack(coarse_msgs_eta), jnp.stack(coarse_msgs_Lam)),\n",
    "        adj_factor_idx=jnp.stack(coarse_adj_factor_idx),\n",
    "    )\n",
    "\n",
    "    # === 2. Build Coarse Priors ===\n",
    "    fine_between_dict = {\n",
    "        (int(i), int(j)): k for k, (i, j) in enumerate(between_facs_fine.adj_var_id)\n",
    "    }\n",
    "\n",
    "    fine_between_dict.update({(int(j), int(i)): k for k, (i, j) in enumerate(between_facs_fine.adj_var_id)})\n",
    "\n",
    "    prior_ids, prior_zs, prior_zLams = [], [], []\n",
    "    adj_var_ids, adj_var_idxs = [], []\n",
    "    factor_id_counter = 0\n",
    "\n",
    "    for patch_id, patch in patch_map.items():\n",
    "        residuals = []\n",
    "        precisions = []\n",
    "\n",
    "        for v in patch:\n",
    "            mask = (prior_facs_fine.adj_var_id[:, 0] == v)\n",
    "            if jnp.any(mask):\n",
    "                i = jnp.argmax(mask)\n",
    "                z_i = prior_facs_fine.z[i]\n",
    "                z_Lam_i = prior_facs_fine.z_Lam[i]\n",
    "            else:\n",
    "                z_i = jnp.zeros((D,))\n",
    "                z_Lam_i = (1. / (prior_std ** 2)) * jnp.eye(D)\n",
    "            residuals.append(z_i)\n",
    "            precisions.append(z_Lam_i)\n",
    "\n",
    "        edge_indices = [(0, 1), (0, 2), (1, 3), (2, 3)]\n",
    "        for i, j in edge_indices:\n",
    "            a, b = patch[i], patch[j]\n",
    "            key = (a, b)\n",
    "            if key in fine_between_dict:\n",
    "                k = fine_between_dict[key]\n",
    "                a_k, b_k = between_facs_fine.adj_var_id[k]\n",
    "                z = between_facs_fine.z[k]\n",
    "                z_Lam = between_facs_fine.z_Lam[k]\n",
    "                if a_k == b:\n",
    "                    z = -z\n",
    "                residuals.append(z)\n",
    "                precisions.append(z_Lam)\n",
    "            else:\n",
    "                residuals.append(jnp.zeros((D,)))\n",
    "                precisions.append((1. / (between_std ** 2)) * jnp.eye(D))\n",
    "\n",
    "        z = jnp.concatenate(residuals)\n",
    "        z_Lam = jax.scipy.linalg.block_diag(*precisions)\n",
    "\n",
    "        prior_ids.append(factor_id_counter)\n",
    "        prior_zs.append(z)\n",
    "        prior_zLams.append(z_Lam)\n",
    "        adj_var_ids.append(jnp.array([patch_id]))\n",
    "        adj_var_idxs.append(jnp.array([0]))\n",
    "        factor_id_counter += 1\n",
    "\n",
    "    prior_facs_coarse = Factor(\n",
    "        factor_id=jnp.array(prior_ids, dtype=jnp.int32),\n",
    "        z=jnp.stack(prior_zs),\n",
    "        z_Lam=jnp.stack(prior_zLams),\n",
    "        threshold=jnp.ones((len(prior_ids),)),\n",
    "        potential=None,\n",
    "        adj_var_id=jnp.stack(adj_var_ids),\n",
    "        adj_var_idx=jnp.stack(adj_var_idxs),\n",
    "    )\n",
    "\n",
    "    # === Build Horizontal & Vertical Between Factors Separately ===\n",
    "    horizontal_ids, horizontal_zs, horizontal_zLams = [], [], []\n",
    "    horizontal_adj_ids, horizontal_adj_idxs = [], []\n",
    "\n",
    "    vertical_ids, vertical_zs, vertical_zLams = [], [], []\n",
    "    vertical_adj_ids, vertical_adj_idxs = [], []\n",
    "\n",
    "    slot_counter = defaultdict(int)\n",
    "    factor_id_counter = len(prior_ids)\n",
    "\n",
    "    height = H // stride\n",
    "    width = W // stride\n",
    "\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            patch_i = row * width + col\n",
    "\n",
    "            # Horizontal neighbor\n",
    "            if col < width - 1:\n",
    "                patch_j = patch_i + 1\n",
    "                pi_patch, pj_patch = patch_map[patch_i], patch_map[patch_j]\n",
    "                fine_pairs = [(pi_patch[1], pj_patch[0]), (pi_patch[3], pj_patch[2])]\n",
    "\n",
    "                residuals = []\n",
    "                precisions = []\n",
    "                for a, b in fine_pairs:\n",
    "                    key = (a, b)\n",
    "                    if key in fine_between_dict:\n",
    "                        k = fine_between_dict[key]\n",
    "                        a_k, b_k = between_facs_fine.adj_var_id[k]\n",
    "                        z = between_facs_fine.z[k]\n",
    "                        z_Lam = between_facs_fine.z_Lam[k]\n",
    "                        if a_k == b:\n",
    "                            z = -z\n",
    "                    else:\n",
    "                        z = jnp.zeros((D,))\n",
    "                        z_Lam = (1. / (between_std ** 2)) * jnp.eye(D)\n",
    "                    residuals.append(z)\n",
    "                    precisions.append(z_Lam)\n",
    "\n",
    "                z = jnp.concatenate(residuals)\n",
    "                z_Lam = jax.scipy.linalg.block_diag(*precisions)\n",
    "\n",
    "                adj_id = jnp.array([patch_i, patch_j])\n",
    "                adj_idx = jnp.array([slot_counter[patch_i] + 1, slot_counter[patch_j] + 1])\n",
    "                slot_counter[patch_i] += 1\n",
    "                slot_counter[patch_j] += 1\n",
    "\n",
    "                horizontal_ids.append(factor_id_counter)\n",
    "                horizontal_zs.append(z)\n",
    "                horizontal_zLams.append(z_Lam)\n",
    "                horizontal_adj_ids.append(adj_id)\n",
    "                horizontal_adj_idxs.append(adj_idx)\n",
    "                factor_id_counter += 1\n",
    "\n",
    "            # Vertical neighbor\n",
    "            if row < height - 1:\n",
    "                patch_j = patch_i + width\n",
    "                pi_patch, pj_patch = patch_map[patch_i], patch_map[patch_j]\n",
    "                fine_pairs = [(pi_patch[2], pj_patch[0]), (pi_patch[3], pj_patch[1])]\n",
    "\n",
    "                residuals = []\n",
    "                precisions = []\n",
    "                for a, b in fine_pairs:\n",
    "                    key = (a, b)\n",
    "                    if key in fine_between_dict:\n",
    "                        k = fine_between_dict[key]\n",
    "                        a_k, b_k = between_facs_fine.adj_var_id[k]\n",
    "                        z = between_facs_fine.z[k]\n",
    "                        z_Lam = between_facs_fine.z_Lam[k]\n",
    "                        if a_k == b:\n",
    "                            z = -z\n",
    "                    else:\n",
    "                        z = jnp.zeros((D,))\n",
    "                        z_Lam = (1. / (between_std ** 2)) * jnp.eye(D)\n",
    "                    residuals.append(z)\n",
    "                    precisions.append(z_Lam)\n",
    "\n",
    "                z = jnp.concatenate(residuals)\n",
    "                z_Lam = jax.scipy.linalg.block_diag(*precisions)\n",
    "\n",
    "                adj_id = jnp.array([patch_i, patch_j])\n",
    "                adj_idx = jnp.array([slot_counter[patch_i] + 1, slot_counter[patch_j] + 1])\n",
    "                slot_counter[patch_i] += 1\n",
    "                slot_counter[patch_j] += 1\n",
    "\n",
    "                vertical_ids.append(factor_id_counter)\n",
    "                vertical_zs.append(z)\n",
    "                vertical_zLams.append(z_Lam)\n",
    "                vertical_adj_ids.append(adj_id)\n",
    "                vertical_adj_idxs.append(adj_idx)\n",
    "                factor_id_counter += 1\n",
    "\n",
    "    horizontal_between_facs = Factor(\n",
    "        factor_id=jnp.array(horizontal_ids, dtype=jnp.int32),\n",
    "        z=jnp.stack(horizontal_zs),\n",
    "        z_Lam=jnp.stack(horizontal_zLams),\n",
    "        threshold=jnp.ones((len(horizontal_ids),)),\n",
    "        potential=None,\n",
    "        adj_var_id=jnp.stack(horizontal_adj_ids),\n",
    "        adj_var_idx=jnp.stack(horizontal_adj_idxs),\n",
    "    )\n",
    "\n",
    "    vertical_between_facs = Factor(\n",
    "        factor_id=jnp.array(vertical_ids, dtype=jnp.int32),\n",
    "        z=jnp.stack(vertical_zs),\n",
    "        z_Lam=jnp.stack(vertical_zLams),\n",
    "        threshold=jnp.ones((len(vertical_ids),)),\n",
    "        potential=None,\n",
    "        adj_var_id=jnp.stack(vertical_adj_ids),\n",
    "        adj_var_idx=jnp.stack(vertical_adj_idxs),\n",
    "    )\n",
    "\n",
    "    return varis_coarse, prior_facs_coarse, horizontal_between_facs, vertical_between_facs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c27e46f-ed45-4b04-91b8-2c695e5566fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coarse Variables: 64\n",
      "Coarse Prior Factors: 64\n",
      "Coarse Horizontal Between Factors: 56\n",
      "Coarse vertical Between Factors: 56\n",
      "=== Prior Factor #0 ===\n",
      "factor_id: 0\n",
      "adj_var_id: [0]\n",
      "adj_var_idx: [0]\n",
      "z (residual target):\n",
      " [ 0.8820262   0.2000786   1.489369    1.1204466  -0.44389287  0.00960177\n",
      "  0.8260439   1.0781745   0.33389416 -0.98431236 -0.33002815  0.08790947\n",
      "  0.14213984  0.8713344   1.2949399  -0.18192941]\n",
      "z_Lam (precision matrix):\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.]]\n",
      "threshold: 1.0\n",
      "\n",
      "=== Coarse Between Factor #0 ===\n",
      "factor_id: 64\n",
      "adj_var_id: [0 1]\n",
      "adj_var_idx: [1 1]\n",
      "z (residual): [1.2493452  0.5239861  0.934473   0.56653994]\n",
      "z_Lam (precision):\n",
      " [[10.  0.  0.  0.]\n",
      " [ 0. 10.  0.  0.]\n",
      " [ 0.  0. 10.  0.]\n",
      " [ 0.  0.  0. 10.]]\n",
      "threshold: 1.0\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: construct coarse-level graph ===\n",
    "varis_coarse, prior_facs_coarse, horizontal_between_facs, vertical_between_facs  = build_coarse_slam_graph(\n",
    "    varis_fine=varis,\n",
    "    prior_facs_fine=prior_facs,\n",
    "    between_facs_fine=between_facs,\n",
    "    H=16, W=16,\n",
    "    stride = 2,\n",
    "    prior_std = 1.0,\n",
    "    between_std = 0.1,\n",
    ")\n",
    "\n",
    "# === Step 3: print coarse-level factor numbers to verify===\n",
    "print(\"Coarse Variables:\", len(varis_coarse.var_id))\n",
    "print(\"Coarse Prior Factors:\", len(prior_facs_coarse.factor_id))\n",
    "print(\"Coarse Horizontal Between Factors:\", len(horizontal_between_facs.factor_id))\n",
    "print(\"Coarse vertical Between Factors:\", len(vertical_between_facs.factor_id))\n",
    "\n",
    "k = 0  # factor index，e.g. 0~63\n",
    "\n",
    "print(\"=== Prior Factor #{} ===\".format(k))\n",
    "print(\"factor_id:\", prior_facs_coarse.factor_id[k])\n",
    "print(\"adj_var_id:\", prior_facs_coarse.adj_var_id[k])     # coarse variable index\n",
    "print(\"adj_var_idx:\", prior_facs_coarse.adj_var_idx[k])   # always [0] since 1 variable\n",
    "print(\"z (residual target):\\n\", prior_facs_coarse.z[k])   # shape: (16,)\n",
    "print(\"z_Lam (precision matrix):\\n\", prior_facs_coarse.z_Lam[k])  # shape: (16, 16)\n",
    "print(\"threshold:\", prior_facs_coarse.threshold[k])\n",
    "\n",
    "k = 0\n",
    "\n",
    "print(f\"\\n=== Coarse Between Factor #{k} ===\")\n",
    "print(\"factor_id:\", horizontal_between_facs.factor_id[k])\n",
    "\n",
    "print(\"adj_var_id:\", horizontal_between_facs.adj_var_id[k])     # coarse variable IDs\n",
    "print(\"adj_var_idx:\", horizontal_between_facs.adj_var_idx[k])   # slot idxs on both vars\n",
    "\n",
    "print(\"z (residual):\", horizontal_between_facs.z[k])            # shape: (4,)\n",
    "print(\"z_Lam (precision):\\n\", horizontal_between_facs.z_Lam[k])  # shape: (4,4)\n",
    "\n",
    "print(\"threshold:\", horizontal_between_facs.threshold[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f24db-958c-4260-b73c-9b63ea02a2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bd0b79a-099e-4718-81b3-3d1753baaa35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian(eta=[[[ 8.43366146e-01  9.16410732e+00  3.40691209e+00 -1.74360199e+01\n",
      "   -1.66935730e+01  2.70799065e+00  1.51968412e+01  7.97222424e+00]]\n",
      "\n",
      " [[ 2.45289898e+00  8.52161407e+00  7.67830324e+00 -1.17027130e+01\n",
      "   -1.00919342e+01 -6.76845407e+00  1.17910366e+01  1.18352747e+01]]\n",
      "\n",
      " [[ 1.69943447e+01 -6.12245750e+00  9.62253380e+00  7.97455406e+00\n",
      "   -2.04388561e+01  1.02271676e-01  1.04649782e+01  1.24344635e+00]]\n",
      "\n",
      " [[-1.29676986e+00  1.52147532e+00  1.71238022e+01 -8.33569241e+00\n",
      "   -6.18527651e-01  1.12449431e+00  1.05127211e+01  8.08710575e+00]]\n",
      "\n",
      " [[-4.72071981e+00  1.49125242e+01  1.64136639e+01 -3.12783527e+00\n",
      "    3.13648415e+00 -3.59879398e+00  1.88194637e+01 -6.62844086e+00]]\n",
      "\n",
      " [[-2.59870696e+00 -4.82444620e+00  3.28841858e+01  2.09881043e+00\n",
      "   -2.98598838e+00  4.87510395e+00  1.35867376e+01 -5.69891691e-01]]\n",
      "\n",
      " [[-7.89675474e+00  8.40360832e+00  3.13053741e+01 -3.40100193e+00\n",
      "    4.12990093e+00 -6.24532938e+00  2.33353329e+01  2.39181376e+00]]\n",
      "\n",
      " [[ 2.45434093e+00  7.24836016e+00  2.49355850e+01 -1.01540995e+00\n",
      "    9.36917400e+00 -7.74110556e+00  2.13419590e+01  3.38899803e+00]]\n",
      "\n",
      " [[-9.42287254e+00 -6.26311636e+00  4.22827244e-01  8.47493744e+00\n",
      "   -6.88297749e+00  5.28300238e+00  1.72253418e+01  3.62960672e+00]]\n",
      "\n",
      " [[-7.50863457e+00  3.91222572e+00  5.84033012e+00  1.21426449e+01\n",
      "   -6.59597540e+00  2.14016104e+00  1.84815083e+01 -8.29374981e+00]]\n",
      "\n",
      " [[-8.14414597e+00 -3.46617651e+00  1.86404934e+01  8.98587704e+00\n",
      "   -1.20974302e+01  6.72677851e+00  1.95785847e+01 -2.60821366e+00]]\n",
      "\n",
      " [[-2.03413939e+00  7.53643608e+00  1.22976351e+01 -9.68323326e+00\n",
      "   -1.69571066e+00  6.15718746e+00  1.87362919e+01  6.92154789e+00]]\n",
      "\n",
      " [[ 1.01670637e+01  9.98896027e+00  8.11420536e+00  1.15094638e+00\n",
      "   -2.63238144e+00 -1.38893986e+00  1.74367542e+01  1.28758502e+00]]\n",
      "\n",
      " [[-1.67588377e+00 -4.11759281e+00  1.31175032e+01  2.65810537e+00\n",
      "    8.11026192e+00  9.84293747e+00  2.45623016e+01  2.85813093e+00]]\n",
      "\n",
      " [[-5.83701324e+00  2.47543907e+01  1.78632107e+01 -1.63077831e+01\n",
      "    1.27204208e+01 -9.20426083e+00  2.48537502e+01  1.20095940e+01]]\n",
      "\n",
      " [[ 1.42796345e+01  5.92290592e+00  1.93974476e+01 -2.56186604e+00\n",
      "    1.06348991e+00  4.27190161e+00  2.29768887e+01  4.05848026e+00]]\n",
      "\n",
      " [[-1.10228405e+01  2.75223041e+00  8.86701393e+00  5.50166750e+00\n",
      "   -2.44512367e+00  9.04374218e+00  6.77783346e+00  2.18425679e+00]]\n",
      "\n",
      " [[-1.50900402e+01  4.97999239e+00  2.37553082e+01  1.22240219e+01\n",
      "   -1.27321491e+01  5.43347716e-01  1.22989645e+01  2.56566048e-01]]\n",
      "\n",
      " [[ 2.74334431e+00  5.81923723e+00  1.18454351e+01  8.57323742e+00\n",
      "   -1.59202671e+01  3.26556492e+00  2.08722305e+01  9.11569595e-03]]\n",
      "\n",
      " [[-9.77534676e+00  1.36430526e+00  2.32571831e+01  5.31018448e+00\n",
      "   -4.70506048e+00  9.48329353e+00  1.64722271e+01  2.39471364e+00]]\n",
      "\n",
      " [[ 7.97343874e+00  2.40459919e+00  1.61798992e+01  1.52706814e+01\n",
      "   -1.53335094e+00  2.34425592e+00  1.14857016e+01 -1.32482481e+00]]\n",
      "\n",
      " [[-7.42646599e+00  1.79627371e+00  2.55314789e+01  8.91665173e+00\n",
      "   -5.69655180e-01  1.85916686e+00  2.32737656e+01  4.25461006e+00]]\n",
      "\n",
      " [[ 1.27312222e+01  9.23834324e-01  1.21676979e+01 -5.57346392e+00\n",
      "   -5.75671434e+00  1.05811272e+01  3.01966152e+01  1.05485973e+01]]\n",
      "\n",
      " [[ 1.21648111e+01 -5.06437302e+00  2.11434307e+01  1.21332045e+01\n",
      "   -8.87483120e-01  1.48574524e+01  2.45526142e+01 -3.78012657e+00]]\n",
      "\n",
      " [[-1.08910618e+01  1.24830570e+01  1.06869726e+01 -8.85909319e-01\n",
      "   -7.46640825e+00 -2.77181911e+00  9.22034931e+00  1.66525726e+01]]\n",
      "\n",
      " [[-5.15654850e+00 -2.70144606e+00  5.76153755e+00  7.81184292e+00\n",
      "   -1.80529747e+01  1.56400805e+01  2.79048100e+01  5.96391773e+00]]\n",
      "\n",
      " [[-7.13651896e-01  8.34595108e+00  1.08490658e+01  8.83386612e+00\n",
      "   -5.91737175e+00  4.00965166e+00  1.32976761e+01  5.61709738e+00]]\n",
      "\n",
      " [[-4.39593506e+00  5.55294418e+00  2.79244118e+01  7.43906784e+00\n",
      "   -4.05059242e+00 -5.88614225e+00  7.79039097e+00  1.70200977e+01]]\n",
      "\n",
      " [[-3.34735799e+00  8.03448391e+00  1.25532551e+01  3.81225920e+00\n",
      "    1.63262010e-01  7.94761944e+00  2.48407326e+01  5.96764040e+00]]\n",
      "\n",
      " [[ 1.54965758e+00 -4.63344908e+00  1.27895927e+01  1.87859631e+01\n",
      "    4.93037939e-01  1.24227071e+00  2.75103111e+01  9.86878872e+00]]\n",
      "\n",
      " [[ 2.00971365e+00  1.72814445e+01  2.96327438e+01  1.74648380e+00\n",
      "   -1.41216493e+00  9.81807709e-04  2.07447014e+01  6.11843109e+00]]\n",
      "\n",
      " [[-7.12791681e-01 -1.44588928e+01  2.69707870e+01  1.14362850e+01\n",
      "    8.58812618e+00  2.01135406e+01  2.28608532e+01  9.16452694e+00]]\n",
      "\n",
      " [[-5.11452675e+00 -2.25677824e+00  1.39145250e+01  1.40368595e+01\n",
      "   -9.49933052e+00  2.51708069e+01  1.57592893e+00 -2.92697048e+00]]\n",
      "\n",
      " [[-1.07905188e+01  1.56539373e+01  1.59104919e+01  5.06775713e+00\n",
      "   -1.47190154e+00  7.31458187e+00  6.54332685e+00  5.72246838e+00]]\n",
      "\n",
      " [[-1.03020439e+01  1.74792442e+01  2.75919952e+01  3.39518714e+00\n",
      "   -5.96311283e+00  8.15258789e+00  5.81610584e+00  5.58808613e+00]]\n",
      "\n",
      " [[-6.89067745e+00  6.11260033e+00  1.41516714e+01  1.21026669e+01\n",
      "   -1.16311789e+01  1.56068621e+01  2.97019196e+01 -6.70748711e-01]]\n",
      "\n",
      " [[-1.24090796e+01  1.11028862e+00  2.48423195e+01  1.23105907e+01\n",
      "   -6.52035332e+00  1.83219547e+01  2.84049282e+01  1.35301685e+00]]\n",
      "\n",
      " [[ 3.68112564e+00  1.66806259e+01  4.29523277e+00  3.81394219e+00\n",
      "   -2.94255066e+00 -1.84349394e+00  3.64777603e+01  1.67822056e+01]]\n",
      "\n",
      " [[ 1.01907148e+01  5.48821259e+00  1.47995939e+01  1.55349827e+01\n",
      "   -6.68912601e+00  6.19262171e+00  3.05416698e+01  7.74429417e+00]]\n",
      "\n",
      " [[ 3.31382132e+00  6.31565094e+00  2.35476913e+01  8.43565178e+00\n",
      "   -3.02607745e-01  9.60108662e+00  3.10097408e+01  1.03605099e+01]]\n",
      "\n",
      " [[-1.38413792e+01  8.29646969e+00  8.23731709e+00  1.14782305e+01\n",
      "   -1.69215374e+01  4.10848904e+00  2.46591740e+01  1.66595383e+01]]\n",
      "\n",
      " [[-8.76624489e+00  2.20624466e+01  6.36372805e+00  5.43623495e+00\n",
      "   -9.25184155e+00  1.66537142e+00  1.98354836e+01  1.38793545e+01]]\n",
      "\n",
      " [[-1.22926788e+01  1.90898571e+01  2.02847919e+01  1.45183611e+00\n",
      "   -8.50035000e+00  1.32439041e+00  1.93446350e+01  2.01829414e+01]]\n",
      "\n",
      " [[-1.21317501e+01  1.56632433e+01  1.12656240e+01  8.40614700e+00\n",
      "    1.02096138e+01  5.83244228e+00  1.56103849e+01  1.16924267e+01]]\n",
      "\n",
      " [[-1.51991854e+01  6.70134068e+00  2.18569279e+01  1.82501926e+01\n",
      "    4.37145567e+00  1.30511627e+01  2.17572041e+01  3.99197912e+00]]\n",
      "\n",
      " [[-1.24286852e+01  1.08037577e+01  2.76372299e+01  3.78007841e+00\n",
      "    1.67632809e+01  1.84104462e+01  1.06640739e+01  8.29417515e+00]]\n",
      "\n",
      " [[-6.24338531e+00  1.17088518e+01  3.10879822e+01  4.73067617e+00\n",
      "   -1.85174203e+00  6.85011959e+00  2.68225441e+01  1.83033047e+01]]\n",
      "\n",
      " [[ 8.73137236e-01  6.63779688e+00  3.02854862e+01  9.74875641e+00\n",
      "    5.66835403e+00  1.54576035e+01  1.91033764e+01  9.29416275e+00]]\n",
      "\n",
      " [[ 6.85225582e+00  7.65529728e+00 -1.63064003e+00  6.90539360e+00\n",
      "   -1.23951340e+01  1.73156033e+01  9.64548874e+00  1.74902554e+01]]\n",
      "\n",
      " [[-1.79039383e+00 -9.32140350e-01  2.63858604e+01  6.65808058e+00\n",
      "   -2.19910660e+01  2.46381149e+01  7.92076206e+00  1.99596958e+01]]\n",
      "\n",
      " [[-1.63798885e+01  1.25764055e+01  2.95329380e+01  4.79216766e+00\n",
      "   -8.89023304e-01  1.98985214e+01  5.81980705e+00  1.02210608e+01]]\n",
      "\n",
      " [[-7.97930098e+00  1.28517160e+01  1.46914024e+01  1.22926874e+01\n",
      "   -2.56341219e+00  1.88312531e+01  2.24954815e+01  6.44766998e+00]]\n",
      "\n",
      " [[-5.30699158e+00  7.87189198e+00  1.18367453e+01  2.28675346e+01\n",
      "    5.54437590e+00 -4.38757753e+00  2.19588051e+01  2.35641937e+01]]\n",
      "\n",
      " [[-2.71687555e+00  2.04318237e+01  2.08464565e+01  1.42570219e+01\n",
      "    1.03571882e+01  6.68907833e+00  1.38472996e+01  8.87105846e+00]]\n",
      "\n",
      " [[ 8.70099831e+00  2.06806774e+01  1.44774570e+01  8.63127708e+00\n",
      "    2.08280087e+00  8.21885395e+00  2.36823692e+01  1.19829521e+01]]\n",
      "\n",
      " [[ 9.03706646e+00  1.04979048e+01  2.86545506e+01  1.32102261e+01\n",
      "    3.08071256e+00  1.21943989e+01  1.86060982e+01  1.37831268e+01]]\n",
      "\n",
      " [[-2.00176849e+01  5.75276184e+00  1.73059235e+01  1.52743921e+01\n",
      "   -9.58530903e+00  1.97064209e+01  1.33097639e+01  1.79239140e+01]]\n",
      "\n",
      " [[-7.22579479e+00  1.14608326e+01  1.07526951e+01  2.49700737e+01\n",
      "    9.47481918e+00  6.73133039e+00 -1.41274035e+00  1.49365911e+01]]\n",
      "\n",
      " [[-1.07378435e+01  2.49087448e+01  2.25850220e+01 -5.66362286e+00\n",
      "   -3.14264369e+00  9.52142334e+00  8.62286282e+00  2.80427151e+01]]\n",
      "\n",
      " [[-6.96367359e+00  1.25833626e+01  1.49812288e+01  1.88709946e+01\n",
      "    1.64891148e+00  2.30260105e+01  1.77534332e+01  3.73293304e+00]]\n",
      "\n",
      " [[ 1.65293789e+00  2.75343208e+01  3.08702507e+01  1.59384069e+01\n",
      "    2.29210138e+00  1.13606539e+01 -2.38524628e+00  3.68024063e+00]]\n",
      "\n",
      " [[-3.61663055e+00  2.05627079e+01  1.55425367e+01  3.08966541e+00\n",
      "    8.80802155e+00  1.62456455e+01  1.98530998e+01  1.78237648e+01]]\n",
      "\n",
      " [[ 4.07971668e+00  2.90555000e+01  2.07232933e+01  7.07906961e+00\n",
      "    4.81852722e+00 -1.85142708e+00  2.01200218e+01  2.27846756e+01]]\n",
      "\n",
      " [[-1.41961813e+00  1.35407658e+01  2.88730278e+01  1.31448603e+01\n",
      "    1.01124907e+00  2.30198631e+01  3.06172581e+01  9.43949890e+00]]], lam=[[[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]\n",
      "\n",
      "\n",
      " [[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]\n",
      "\n",
      "\n",
      " [[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]\n",
      "\n",
      "\n",
      " [[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]\n",
      "\n",
      "\n",
      " [[[ 21.   0. -10. ...   0.   0.   0.]\n",
      "   [  0.  21.   0. ... -10.   0.   0.]\n",
      "   [-10.   0.  21. ...   0. -10.   0.]\n",
      "   ...\n",
      "   [  0. -10.   0. ...  21.   0. -10.]\n",
      "   [  0.   0. -10. ...   0.  21.   0.]\n",
      "   [  0.   0.   0. ... -10.   0.  21.]]]])\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "xs = [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Gaussian(eta=[[[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan]\n",
      "  [nan nan nan nan nan nan nan nan]]], lam=[[[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]])\n"
     ]
    }
   ],
   "source": [
    "cpu_device = jax.devices(\"cpu\")[0]\n",
    "varis_coarse= jax.device_put(varis_coarse, cpu_device)\n",
    "prior_facs_coarse = jax.device_put(prior_facs_coarse, cpu_device)\n",
    "between_facs_coarse = jax.device_put(between_facs_coarse, cpu_device)\n",
    "\n",
    "\"\"\"\n",
    "print(\"var_id:\", varis_coarse.var_id.shape)            # should be (N,)\n",
    "print(\"belief.eta:\", varis_coarse.belief.eta.shape)    # should be (N, D)\n",
    "print(\"belief.Lam:\", varis_coarse.belief.Lam.shape)    # should be (N, D, D)\n",
    "print(\"msgs.eta:\", varis_coarse.msgs.eta.shape)        # should be (N, Ni_v, D)\n",
    "print(\"msgs.Lam:\", varis_coarse.msgs.Lam.shape)        # should be (N, Ni_v, D, D)\n",
    "print(\"adj_factor_idx:\", varis_coarse.adj_factor_idx.shape)  # should be (N, Ni_v)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "varis_coarse, prior_facs_coarse, between_facs_coarse, energy_log, linpoints = gbp_solve(\n",
    "    varis_coarse, prior_facs_coarse, between_facs_coarse, num_iters=50, visualize=False, prior_h=h3_fn, between_h=h4_fn\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "varis_coarse, vtof_msgs, linpoints = update_variable(varis_coarse)\n",
    "\n",
    "# Step 2: Factor update\n",
    "prior_facs_coarse, varis_coarse = update_factor(prior_facs_coarse, varis_coarse, vtof_msgs, linpoints, h3_fn, l2)\n",
    "horizontal_between_facs, varis_coarse = update_factor(horizontal_between_facs, varis_coarse, vtof_msgs, linpoints, h4_fn, l2)\n",
    "#vertical_between_facs, varis_coarse = update_factor(vertical_between_facs, varis_coarse, vtof_msgs, linpoints, h5_fn, l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f4193c8-b860-4c77-b7d8-ba9e402ada8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  0.,  0., -0., -0.,  0.,  0.,  0.],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varis_coarse.msgs.eta[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc4c66-94f5-4b5a-bae6-6d131768ca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe0a75-2d77-4136-8011-3c6662e6dee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
